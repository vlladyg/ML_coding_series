{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03e4cda7-71ca-4962-aa5b-9da9167356df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tools for handing the strided irreps layout.\"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from e3nn.o3 import Irreps\n",
    "\n",
    "\n",
    "class StridedLayout:\n",
    "    \"\"\"Utility class to represent a strided layout of a tensor whose irreps all have the same multiplicity.\"\"\"\n",
    "\n",
    "    irreps: Irreps\n",
    "    base_irreps: Irreps\n",
    "    pad_to_multiple: int\n",
    "    dim: int\n",
    "    base_dim: int\n",
    "    mul: int\n",
    "\n",
    "    def __init__(self, irreps: Irreps, pad_to_multiple: int = 1):\n",
    "        irreps = Irreps(irreps)\n",
    "        if not self.can_be_strided(irreps):\n",
    "            raise ValueError(f\"Irreps `{irreps}` cannot be strided.\")\n",
    "        self.irreps = irreps\n",
    "        self.base_irreps = Irreps([(1, ir) for _, ir in irreps])\n",
    "        self.mul = self.irreps[0].mul if len(irreps) > 0 else 0\n",
    "        assert self.irreps.dim == self.base_irreps.dim * self.mul\n",
    "        self.pad_to_multiple = pad_to_multiple\n",
    "        assert self.pad_to_multiple in (1, 2, 4, 8)\n",
    "\n",
    "        self.base_dim = int(\n",
    "            math.ceil(self.base_irreps.dim / self.pad_to_multiple)\n",
    "            * self.pad_to_multiple\n",
    "        )\n",
    "        pad_by = self.base_dim - self.base_irreps.dim\n",
    "        self.dim = self.base_dim * self.mul\n",
    "\n",
    "        # indexes to convert\n",
    "        self.indexes_to_strided = torch.zeros(self.dim, dtype=torch.long)\n",
    "        self.indexes_to_catted = torch.zeros(self.irreps.dim, dtype=torch.long)\n",
    "        i: int = 0\n",
    "        for mul_i in range(self.mul):\n",
    "            for irrep_i, (_, irrep) in enumerate(self.base_irreps):\n",
    "                strided_indexes = torch.arange(start=i, end=i + irrep.dim)\n",
    "                catted_indexes = (\n",
    "                    torch.arange(irrep.dim)\n",
    "                    + self.irreps[:irrep_i].dim\n",
    "                    + irrep.dim * mul_i\n",
    "                )\n",
    "                self.indexes_to_strided[strided_indexes] = catted_indexes\n",
    "                self.indexes_to_catted[catted_indexes] = strided_indexes\n",
    "                i += irrep.dim\n",
    "            # pad out this line of the [mul, k] shape\n",
    "            i += pad_by\n",
    "\n",
    "        # They should be inverses:\n",
    "        assert torch.all(\n",
    "            self.indexes_to_strided[self.indexes_to_catted]\n",
    "            == torch.arange(self.irreps.dim)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def can_be_strided(irreps: Irreps) -> bool:\n",
    "        \"\"\"Check whether ``irreps`` is compatible with strided layout.\"\"\"\n",
    "        irreps = Irreps(irreps)\n",
    "        if len(irreps) == 0:\n",
    "            return True\n",
    "        return all(irreps[0].mul == mul for mul, ir in irreps)\n",
    "\n",
    "    def to_strided(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Convert a tensor from default to strided layout.\"\"\"\n",
    "        return x[..., self.indexes_to_strided]\n",
    "\n",
    "    def to_catted(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Convert a tensor from strided to default layout.\"\"\"\n",
    "        return x[..., self.indexes_to_catted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73a953fb-936e-4421-91d7-440b29bf628e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.ceil(5.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64ea38d4-2cb3-4bcc-b558-725f7336cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_layout = StridedLayout(\"32x0e + 32x1o + 32x2e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27277dc7-343f-46cf-a634-090138e50a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([288])\n",
      "torch.Size([288])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  0,  32,  33,  34, 128, 129, 130, 131, 132,   1,  35,  36,  37, 133,\n",
       "        134, 135, 136, 137,   2,  38,  39,  40, 138, 139, 140, 141, 142,   3,\n",
       "         41,  42,  43, 143, 144, 145, 146, 147,   4,  44,  45,  46, 148, 149,\n",
       "        150, 151, 152,   5,  47,  48,  49, 153, 154, 155, 156, 157,   6,  50,\n",
       "         51,  52, 158, 159, 160, 161, 162,   7,  53,  54,  55, 163, 164, 165,\n",
       "        166, 167,   8,  56,  57,  58, 168, 169, 170, 171, 172,   9,  59,  60,\n",
       "         61, 173, 174, 175, 176, 177,  10,  62,  63,  64, 178, 179, 180, 181,\n",
       "        182,  11,  65,  66,  67, 183, 184, 185, 186, 187,  12,  68,  69,  70,\n",
       "        188, 189, 190, 191, 192,  13,  71,  72,  73, 193, 194, 195, 196, 197,\n",
       "         14,  74,  75,  76, 198, 199, 200, 201, 202,  15,  77,  78,  79, 203,\n",
       "        204, 205, 206, 207,  16,  80,  81,  82, 208, 209, 210, 211, 212,  17,\n",
       "         83,  84,  85, 213, 214, 215, 216, 217,  18,  86,  87,  88, 218, 219,\n",
       "        220, 221, 222,  19,  89,  90,  91, 223, 224, 225, 226, 227,  20,  92,\n",
       "         93,  94, 228, 229, 230, 231, 232,  21,  95,  96,  97, 233, 234, 235,\n",
       "        236, 237,  22,  98,  99, 100, 238, 239, 240, 241, 242,  23, 101, 102,\n",
       "        103, 243, 244, 245, 246, 247,  24, 104, 105, 106, 248, 249, 250, 251,\n",
       "        252,  25, 107, 108, 109, 253, 254, 255, 256, 257,  26, 110, 111, 112,\n",
       "        258, 259, 260, 261, 262,  27, 113, 114, 115, 263, 264, 265, 266, 267,\n",
       "         28, 116, 117, 118, 268, 269, 270, 271, 272,  29, 119, 120, 121, 273,\n",
       "        274, 275, 276, 277,  30, 122, 123, 124, 278, 279, 280, 281, 282,  31,\n",
       "        125, 126, 127, 283, 284, 285, 286, 287])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(out_layout.indexes_to_catted.shape)\n",
    "print(out_layout.indexes_to_strided.shape)\n",
    "out_layout.indexes_to_strided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae436836-b33a-4fa3-a91d-3a9e5b0999f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   9,  18,  27,  36,  45,  54,  63,  72,  81,  90,  99, 108, 117,\n",
       "        126, 135, 144, 153, 162, 171, 180, 189, 198, 207, 216, 225, 234, 243,\n",
       "        252, 261, 270, 279,   1,   2,   3,  10,  11,  12,  19,  20,  21,  28,\n",
       "         29,  30,  37,  38,  39,  46,  47,  48,  55,  56,  57,  64,  65,  66,\n",
       "         73,  74,  75,  82,  83,  84,  91,  92,  93, 100, 101, 102, 109, 110,\n",
       "        111, 118, 119, 120, 127, 128, 129, 136, 137, 138, 145, 146, 147, 154,\n",
       "        155, 156, 163, 164, 165, 172, 173, 174, 181, 182, 183, 190, 191, 192,\n",
       "        199, 200, 201, 208, 209, 210, 217, 218, 219, 226, 227, 228, 235, 236,\n",
       "        237, 244, 245, 246, 253, 254, 255, 262, 263, 264, 271, 272, 273, 280,\n",
       "        281, 282,   4,   5,   6,   7,   8,  13,  14,  15,  16,  17,  22,  23,\n",
       "         24,  25,  26,  31,  32,  33,  34,  35,  40,  41,  42,  43,  44,  49,\n",
       "         50,  51,  52,  53,  58,  59,  60,  61,  62,  67,  68,  69,  70,  71,\n",
       "         76,  77,  78,  79,  80,  85,  86,  87,  88,  89,  94,  95,  96,  97,\n",
       "         98, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 121, 122, 123,\n",
       "        124, 125, 130, 131, 132, 133, 134, 139, 140, 141, 142, 143, 148, 149,\n",
       "        150, 151, 152, 157, 158, 159, 160, 161, 166, 167, 168, 169, 170, 175,\n",
       "        176, 177, 178, 179, 184, 185, 186, 187, 188, 193, 194, 195, 196, 197,\n",
       "        202, 203, 204, 205, 206, 211, 212, 213, 214, 215, 220, 221, 222, 223,\n",
       "        224, 229, 230, 231, 232, 233, 238, 239, 240, 241, 242, 247, 248, 249,\n",
       "        250, 251, 256, 257, 258, 259, 260, 265, 266, 267, 268, 269, 274, 275,\n",
       "        276, 277, 278, 283, 284, 285, 286, 287])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_layout.indexes_to_catted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a569106-c51d-4444-8a30-294f93ab9ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse matrix multiplication\n",
    "from typing import Tuple\n",
    "from packaging import version\n",
    "\n",
    "import torch\n",
    "\n",
    "from e3nn.util.jit import compile_mode\n",
    "\n",
    "_USE_PYG_SPARSE: bool = False\n",
    "\n",
    "_TORCH_IS_GE_1_10: bool = version.parse(torch.__version__) >= version.parse(\"1.10.0\")\n",
    "\n",
    "if not _USE_PYG_SPARSE:\n",
    "\n",
    "    class _ExplicitGradSpmm(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, sparse, a):\n",
    "            ctx.save_for_backward(sparse)\n",
    "            return torch.mm(sparse, a)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            (sparse,) = ctx.saved_tensors\n",
    "            return None, torch.mm(sparse.t(), grad_output)\n",
    "\n",
    "    # TODO: support csr with similar method; wait for 1.10 probably\n",
    "    @torch.jit.script\n",
    "    def _remake_sparse_coo(i, v, shape: Tuple[int, int]):\n",
    "        out = torch.sparse_coo_tensor(\n",
    "            indices=i, values=v, size=shape, device=v.device, dtype=v.dtype\n",
    "        )\n",
    "        # mark it as coalesced, cause it is from when we build it in\n",
    "        # ExplicitGradSpmm's __init__\n",
    "        out._coalesced_(True)  # undocumented, AFAIK\n",
    "        assert out.is_coalesced()\n",
    "        return out\n",
    "\n",
    "    @compile_mode(\"trace\")\n",
    "    class ExplicitGradSpmmCOO(torch.nn.Module):\n",
    "        shape: Tuple[int, int]\n",
    "\n",
    "        def __init__(self, mat: torch.Tensor):\n",
    "            super().__init__()\n",
    "            assert mat.is_sparse\n",
    "            assert mat.ndim == 2\n",
    "            mat = mat.coalesce()\n",
    "            # To workaround https://github.com/pytorch/pytorch/issues/63987,\n",
    "            # save indices and values explicitly\n",
    "            self.register_buffer(\"_i\", mat.indices())\n",
    "            self.register_buffer(\"_v\", mat.values())\n",
    "            self.shape = tuple(mat.shape)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # TODO: support csr\n",
    "            sp = _remake_sparse_coo(self._i, self._v, self.shape)\n",
    "            if self.training:\n",
    "                # Use a custom autograd function for 2nd derivatives\n",
    "                # torch.mm doesn't do double derivatives with sparse w3j\n",
    "                tmp = _ExplicitGradSpmm.apply(sp, x)\n",
    "            else:\n",
    "                # For inference, assume only first derivatives necessary\n",
    "                tmp = torch.mm(sp, x)\n",
    "            return tmp\n",
    "\n",
    "        def _make_tracing_inputs(self, n: int):\n",
    "            return [\n",
    "                {\n",
    "                    \"forward\": (\n",
    "                        torch.randn(\n",
    "                            self.shape[-1],\n",
    "                            3,\n",
    "                            device=self._v.device,\n",
    "                            dtype=self._v.dtype,\n",
    "                        ),\n",
    "                    )\n",
    "                }\n",
    "                for _ in range(n)\n",
    "            ]\n",
    "\n",
    "    if _TORCH_IS_GE_1_10:\n",
    "\n",
    "        @torch.jit.script\n",
    "        def _remake_sparse_csr(crow, col, v, shape: Tuple[int, int]) -> torch.Tensor:\n",
    "            return torch.sparse_csr_tensor(\n",
    "                crow_indices=crow,\n",
    "                col_indices=col,\n",
    "                values=v,\n",
    "                size=shape,\n",
    "                layout=torch.sparse_csr,\n",
    "                device=v.device,\n",
    "                dtype=v.dtype,\n",
    "            )\n",
    "\n",
    "        @compile_mode(\"trace\")\n",
    "        class ExplicitGradSpmmCSR(torch.nn.Module):\n",
    "            shape: Tuple[int, int]\n",
    "\n",
    "            def __init__(self, mat: torch.Tensor):\n",
    "                super().__init__()\n",
    "                assert mat.is_sparse_csr\n",
    "                assert mat.ndim == 2\n",
    "                # To workaround https://github.com/pytorch/pytorch/issues/63987,\n",
    "                # save indices and values explicitly\n",
    "                self.register_buffer(\"_crow\", mat.crow_indices())\n",
    "                self.register_buffer(\"_col\", mat.col_indices())\n",
    "                self.register_buffer(\"_v\", mat.values())\n",
    "                self.shape = tuple(mat.shape)\n",
    "\n",
    "            def forward(self, x):\n",
    "                # TODO: support csr\n",
    "                sp = _remake_sparse_csr(self._crow, self._col, self._v, self.shape)\n",
    "                if self.training:\n",
    "                    # Use a custom autograd function for 2nd derivatives\n",
    "                    # torch.mm doesn't do double derivatives with sparse w3j\n",
    "                    tmp = _ExplicitGradSpmm.apply(sp, x)\n",
    "                else:\n",
    "                    # For inference, assume only first derivatives necessary\n",
    "                    tmp = torch.mm(sp, x)\n",
    "                return tmp\n",
    "\n",
    "            def _make_tracing_inputs(self, n: int):\n",
    "                return [\n",
    "                    {\n",
    "                        \"forward\": (\n",
    "                            torch.randn(\n",
    "                                self.shape[-1],\n",
    "                                3,\n",
    "                                device=self._v.device,\n",
    "                                dtype=self._v.dtype,\n",
    "                            ),\n",
    "                        )\n",
    "                    }\n",
    "                    for _ in range(n)\n",
    "                ]\n",
    "\n",
    "    def ExplicitGradSpmm(mat):\n",
    "        if mat.is_sparse:\n",
    "            return ExplicitGradSpmmCOO(mat)\n",
    "        elif _TORCH_IS_GE_1_10 and mat.is_sparse_csr:\n",
    "            return ExplicitGradSpmmCSR(mat)\n",
    "        else:\n",
    "            raise TypeError\n",
    "\n",
    "else:  # _USE_PYG_SPARSE\n",
    "\n",
    "    from torch_sparse import SparseTensor\n",
    "    from torch_sparse.matmul import spmm_add\n",
    "\n",
    "    class ExplicitGradSpmm(torch.nn.Module):\n",
    "        def __init__(self, mat):\n",
    "            super().__init__()\n",
    "            self.mat = SparseTensor.from_dense(mat.to_dense())\n",
    "\n",
    "        def forward(self, x):\n",
    "            return spmm_add(self.mat, x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gcnn",
   "language": "python",
   "name": "torch_gcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
