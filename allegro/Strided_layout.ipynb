{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03e4cda7-71ca-4962-aa5b-9da9167356df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tools for handing the strided irreps layout.\"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from e3nn.o3 import Irreps\n",
    "\n",
    "\n",
    "class StridedLayout:\n",
    "    \"\"\"Utility class to represent a strided layout of a tensor whose irreps all have the same multiplicity.\"\"\"\n",
    "\n",
    "    irreps: Irreps\n",
    "    base_irreps: Irreps\n",
    "    pad_to_multiple: int\n",
    "    dim: int\n",
    "    base_dim: int\n",
    "    mul: int\n",
    "\n",
    "    def __init__(self, irreps: Irreps, pad_to_multiple: int = 1):\n",
    "        irreps = Irreps(irreps)\n",
    "        if not self.can_be_strided(irreps):\n",
    "            raise ValueError(f\"Irreps `{irreps}` cannot be strided.\")\n",
    "        self.irreps = irreps\n",
    "        self.base_irreps = Irreps([(1, ir) for _, ir in irreps])\n",
    "        self.mul = self.irreps[0].mul if len(irreps) > 0 else 0\n",
    "        assert self.irreps.dim == self.base_irreps.dim * self.mul\n",
    "        self.pad_to_multiple = pad_to_multiple\n",
    "        assert self.pad_to_multiple in (1, 2, 4, 8)\n",
    "\n",
    "        self.base_dim = int(\n",
    "            math.ceil(self.base_irreps.dim / self.pad_to_multiple)\n",
    "            * self.pad_to_multiple\n",
    "        )\n",
    "        pad_by = self.base_dim - self.base_irreps.dim\n",
    "        self.dim = self.base_dim * self.mul\n",
    "\n",
    "        # indexes to convert\n",
    "        self.indexes_to_strided = torch.zeros(self.dim, dtype=torch.long)\n",
    "        self.indexes_to_catted = torch.zeros(self.irreps.dim, dtype=torch.long)\n",
    "        i: int = 0\n",
    "        for mul_i in range(self.mul):\n",
    "            for irrep_i, (_, irrep) in enumerate(self.base_irreps):\n",
    "                strided_indexes = torch.arange(start=i, end=i + irrep.dim)\n",
    "                catted_indexes = (\n",
    "                    torch.arange(irrep.dim)\n",
    "                    + self.irreps[:irrep_i].dim\n",
    "                    + irrep.dim * mul_i\n",
    "                )\n",
    "                self.indexes_to_strided[strided_indexes] = catted_indexes\n",
    "                self.indexes_to_catted[catted_indexes] = strided_indexes\n",
    "                i += irrep.dim\n",
    "            # pad out this line of the [mul, k] shape\n",
    "            i += pad_by\n",
    "\n",
    "        # They should be inverses:\n",
    "        assert torch.all(\n",
    "            self.indexes_to_strided[self.indexes_to_catted]\n",
    "            == torch.arange(self.irreps.dim)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def can_be_strided(irreps: Irreps) -> bool:\n",
    "        \"\"\"Check whether ``irreps`` is compatible with strided layout.\"\"\"\n",
    "        irreps = Irreps(irreps)\n",
    "        if len(irreps) == 0:\n",
    "            return True\n",
    "        return all(irreps[0].mul == mul for mul, ir in irreps)\n",
    "\n",
    "    def to_strided(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Convert a tensor from default to strided layout.\"\"\"\n",
    "        return x[..., self.indexes_to_strided]\n",
    "\n",
    "    def to_catted(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Convert a tensor from strided to default layout.\"\"\"\n",
    "        return x[..., self.indexes_to_catted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73a953fb-936e-4421-91d7-440b29bf628e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.ceil(5.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64ea38d4-2cb3-4bcc-b558-725f7336cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_layout = StridedLayout(\"32x0e + 32x1o + 32x2e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27277dc7-343f-46cf-a634-090138e50a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([288])\n",
      "torch.Size([288])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  0,  32,  33,  34, 128, 129, 130, 131, 132,   1,  35,  36,  37, 133,\n",
       "        134, 135, 136, 137,   2,  38,  39,  40, 138, 139, 140, 141, 142,   3,\n",
       "         41,  42,  43, 143, 144, 145, 146, 147,   4,  44,  45,  46, 148, 149,\n",
       "        150, 151, 152,   5,  47,  48,  49, 153, 154, 155, 156, 157,   6,  50,\n",
       "         51,  52, 158, 159, 160, 161, 162,   7,  53,  54,  55, 163, 164, 165,\n",
       "        166, 167,   8,  56,  57,  58, 168, 169, 170, 171, 172,   9,  59,  60,\n",
       "         61, 173, 174, 175, 176, 177,  10,  62,  63,  64, 178, 179, 180, 181,\n",
       "        182,  11,  65,  66,  67, 183, 184, 185, 186, 187,  12,  68,  69,  70,\n",
       "        188, 189, 190, 191, 192,  13,  71,  72,  73, 193, 194, 195, 196, 197,\n",
       "         14,  74,  75,  76, 198, 199, 200, 201, 202,  15,  77,  78,  79, 203,\n",
       "        204, 205, 206, 207,  16,  80,  81,  82, 208, 209, 210, 211, 212,  17,\n",
       "         83,  84,  85, 213, 214, 215, 216, 217,  18,  86,  87,  88, 218, 219,\n",
       "        220, 221, 222,  19,  89,  90,  91, 223, 224, 225, 226, 227,  20,  92,\n",
       "         93,  94, 228, 229, 230, 231, 232,  21,  95,  96,  97, 233, 234, 235,\n",
       "        236, 237,  22,  98,  99, 100, 238, 239, 240, 241, 242,  23, 101, 102,\n",
       "        103, 243, 244, 245, 246, 247,  24, 104, 105, 106, 248, 249, 250, 251,\n",
       "        252,  25, 107, 108, 109, 253, 254, 255, 256, 257,  26, 110, 111, 112,\n",
       "        258, 259, 260, 261, 262,  27, 113, 114, 115, 263, 264, 265, 266, 267,\n",
       "         28, 116, 117, 118, 268, 269, 270, 271, 272,  29, 119, 120, 121, 273,\n",
       "        274, 275, 276, 277,  30, 122, 123, 124, 278, 279, 280, 281, 282,  31,\n",
       "        125, 126, 127, 283, 284, 285, 286, 287])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(out_layout.indexes_to_catted.shape)\n",
    "print(out_layout.indexes_to_strided.shape)\n",
    "out_layout.indexes_to_strided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae436836-b33a-4fa3-a91d-3a9e5b0999f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   9,  18,  27,  36,  45,  54,  63,  72,  81,  90,  99, 108, 117,\n",
       "        126, 135, 144, 153, 162, 171, 180, 189, 198, 207, 216, 225, 234, 243,\n",
       "        252, 261, 270, 279,   1,   2,   3,  10,  11,  12,  19,  20,  21,  28,\n",
       "         29,  30,  37,  38,  39,  46,  47,  48,  55,  56,  57,  64,  65,  66,\n",
       "         73,  74,  75,  82,  83,  84,  91,  92,  93, 100, 101, 102, 109, 110,\n",
       "        111, 118, 119, 120, 127, 128, 129, 136, 137, 138, 145, 146, 147, 154,\n",
       "        155, 156, 163, 164, 165, 172, 173, 174, 181, 182, 183, 190, 191, 192,\n",
       "        199, 200, 201, 208, 209, 210, 217, 218, 219, 226, 227, 228, 235, 236,\n",
       "        237, 244, 245, 246, 253, 254, 255, 262, 263, 264, 271, 272, 273, 280,\n",
       "        281, 282,   4,   5,   6,   7,   8,  13,  14,  15,  16,  17,  22,  23,\n",
       "         24,  25,  26,  31,  32,  33,  34,  35,  40,  41,  42,  43,  44,  49,\n",
       "         50,  51,  52,  53,  58,  59,  60,  61,  62,  67,  68,  69,  70,  71,\n",
       "         76,  77,  78,  79,  80,  85,  86,  87,  88,  89,  94,  95,  96,  97,\n",
       "         98, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 121, 122, 123,\n",
       "        124, 125, 130, 131, 132, 133, 134, 139, 140, 141, 142, 143, 148, 149,\n",
       "        150, 151, 152, 157, 158, 159, 160, 161, 166, 167, 168, 169, 170, 175,\n",
       "        176, 177, 178, 179, 184, 185, 186, 187, 188, 193, 194, 195, 196, 197,\n",
       "        202, 203, 204, 205, 206, 211, 212, 213, 214, 215, 220, 221, 222, 223,\n",
       "        224, 229, 230, 231, 232, 233, 238, 239, 240, 241, 242, 247, 248, 249,\n",
       "        250, 251, 256, 257, 258, 259, 260, 265, 266, 267, 268, 269, 274, 275,\n",
       "        276, 277, 278, 283, 284, 285, 286, 287])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_layout.indexes_to_catted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a569106-c51d-4444-8a30-294f93ab9ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse matrix multiplication\n",
    "from typing import Tuple\n",
    "from packaging import version\n",
    "\n",
    "import torch\n",
    "\n",
    "from e3nn.util.jit import compile_mode\n",
    "\n",
    "_USE_PYG_SPARSE: bool = False\n",
    "\n",
    "_TORCH_IS_GE_1_10: bool = version.parse(torch.__version__) >= version.parse(\"1.10.0\")\n",
    "\n",
    "if not _USE_PYG_SPARSE:\n",
    "\n",
    "    class _ExplicitGradSpmm(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, sparse, a):\n",
    "            ctx.save_for_backward(sparse)\n",
    "            return torch.mm(sparse, a)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            (sparse,) = ctx.saved_tensors\n",
    "            return None, torch.mm(sparse.t(), grad_output)\n",
    "\n",
    "    # TODO: support csr with similar method; wait for 1.10 probably\n",
    "    @torch.jit.script\n",
    "    def _remake_sparse_coo(i, v, shape: Tuple[int, int]):\n",
    "        out = torch.sparse_coo_tensor(\n",
    "            indices=i, values=v, size=shape, device=v.device, dtype=v.dtype\n",
    "        )\n",
    "        # mark it as coalesced, cause it is from when we build it in\n",
    "        # ExplicitGradSpmm's __init__\n",
    "        out._coalesced_(True)  # undocumented, AFAIK\n",
    "        assert out.is_coalesced()\n",
    "        return out\n",
    "\n",
    "    @compile_mode(\"trace\")\n",
    "    class ExplicitGradSpmmCOO(torch.nn.Module):\n",
    "        shape: Tuple[int, int]\n",
    "\n",
    "        def __init__(self, mat: torch.Tensor):\n",
    "            super().__init__()\n",
    "            assert mat.is_sparse\n",
    "            assert mat.ndim == 2\n",
    "            mat = mat.coalesce()\n",
    "            # To workaround https://github.com/pytorch/pytorch/issues/63987,\n",
    "            # save indices and values explicitly\n",
    "            self.register_buffer(\"_i\", mat.indices())\n",
    "            self.register_buffer(\"_v\", mat.values())\n",
    "            self.shape = tuple(mat.shape)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # TODO: support csr\n",
    "            sp = _remake_sparse_coo(self._i, self._v, self.shape)\n",
    "            if self.training:\n",
    "                # Use a custom autograd function for 2nd derivatives\n",
    "                # torch.mm doesn't do double derivatives with sparse w3j\n",
    "                tmp = _ExplicitGradSpmm.apply(sp, x)\n",
    "            else:\n",
    "                # For inference, assume only first derivatives necessary\n",
    "                tmp = torch.mm(sp, x)\n",
    "            return tmp\n",
    "\n",
    "        def _make_tracing_inputs(self, n: int):\n",
    "            return [\n",
    "                {\n",
    "                    \"forward\": (\n",
    "                        torch.randn(\n",
    "                            self.shape[-1],\n",
    "                            3,\n",
    "                            device=self._v.device,\n",
    "                            dtype=self._v.dtype,\n",
    "                        ),\n",
    "                    )\n",
    "                }\n",
    "                for _ in range(n)\n",
    "            ]\n",
    "\n",
    "    if _TORCH_IS_GE_1_10:\n",
    "\n",
    "        @torch.jit.script\n",
    "        def _remake_sparse_csr(crow, col, v, shape: Tuple[int, int]) -> torch.Tensor:\n",
    "            return torch.sparse_csr_tensor(\n",
    "                crow_indices=crow,\n",
    "                col_indices=col,\n",
    "                values=v,\n",
    "                size=shape,\n",
    "                layout=torch.sparse_csr,\n",
    "                device=v.device,\n",
    "                dtype=v.dtype,\n",
    "            )\n",
    "\n",
    "        @compile_mode(\"trace\")\n",
    "        class ExplicitGradSpmmCSR(torch.nn.Module):\n",
    "            shape: Tuple[int, int]\n",
    "\n",
    "            def __init__(self, mat: torch.Tensor):\n",
    "                super().__init__()\n",
    "                assert mat.is_sparse_csr\n",
    "                assert mat.ndim == 2\n",
    "                # To workaround https://github.com/pytorch/pytorch/issues/63987,\n",
    "                # save indices and values explicitly\n",
    "                self.register_buffer(\"_crow\", mat.crow_indices())\n",
    "                self.register_buffer(\"_col\", mat.col_indices())\n",
    "                self.register_buffer(\"_v\", mat.values())\n",
    "                self.shape = tuple(mat.shape)\n",
    "\n",
    "            def forward(self, x):\n",
    "                # TODO: support csr\n",
    "                sp = _remake_sparse_csr(self._crow, self._col, self._v, self.shape)\n",
    "                if self.training:\n",
    "                    # Use a custom autograd function for 2nd derivatives\n",
    "                    # torch.mm doesn't do double derivatives with sparse w3j\n",
    "                    tmp = _ExplicitGradSpmm.apply(sp, x)\n",
    "                else:\n",
    "                    # For inference, assume only first derivatives necessary\n",
    "                    tmp = torch.mm(sp, x)\n",
    "                return tmp\n",
    "\n",
    "            def _make_tracing_inputs(self, n: int):\n",
    "                return [\n",
    "                    {\n",
    "                        \"forward\": (\n",
    "                            torch.randn(\n",
    "                                self.shape[-1],\n",
    "                                3,\n",
    "                                device=self._v.device,\n",
    "                                dtype=self._v.dtype,\n",
    "                            ),\n",
    "                        )\n",
    "                    }\n",
    "                    for _ in range(n)\n",
    "                ]\n",
    "\n",
    "    def ExplicitGradSpmm(mat):\n",
    "        if mat.is_sparse:\n",
    "            return ExplicitGradSpmmCOO(mat)\n",
    "        elif _TORCH_IS_GE_1_10 and mat.is_sparse_csr:\n",
    "            return ExplicitGradSpmmCSR(mat)\n",
    "        else:\n",
    "            raise TypeError\n",
    "\n",
    "else:  # _USE_PYG_SPARSE\n",
    "\n",
    "    from torch_sparse import SparseTensor\n",
    "    from torch_sparse.matmul import spmm_add\n",
    "\n",
    "    class ExplicitGradSpmm(torch.nn.Module):\n",
    "        def __init__(self, mat):\n",
    "            super().__init__()\n",
    "            self.mat = SparseTensor.from_dense(mat.to_dense())\n",
    "\n",
    "        def forward(self, x):\n",
    "            return spmm_add(self.mat, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0bf3ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some code to optimize tensor product as defined in o3.tensor products\n",
    "from typing import List, Optional, Tuple\n",
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "from torch import fx\n",
    "\n",
    "from e3nn import o3\n",
    "from e3nn.util.jit import compile\n",
    "from e3nn.util import prod\n",
    "from e3nn.o3 import Instruction\n",
    "\n",
    "from opt_einsum_fx import jitable, optimize_einsums_full\n",
    "\n",
    "#from ._layout import StridedLayout\n",
    "#from ._spmm import ExplicitGradSpmm\n",
    "\n",
    "\n",
    "def codegen_strided_tensor_product_forward(\n",
    "    irreps_in1: o3.Irreps,\n",
    "    in1_var: List[float],\n",
    "    irreps_in2: o3.Irreps,\n",
    "    in2_var: List[float],\n",
    "    irreps_out: o3.Irreps,\n",
    "    out_var: List[float],\n",
    "    instructions: List[Instruction],\n",
    "    normalization: str = \"component\",\n",
    "    shared_weights: bool = False,\n",
    "    specialized_code: bool = True,\n",
    "    sparse_mode: Optional[str] = None,\n",
    "    pad_to_alignment: int = 1,\n",
    ") -> Optional[fx.GraphModule]:\n",
    "    \"\"\"Returns None if strided doesn't make sense for this TP.\"\"\"\n",
    "    # TODO padding\n",
    "    # Check if irreps can be strided\n",
    "    try:\n",
    "        layout_in1 = StridedLayout(irreps_in1, pad_to_multiple=pad_to_alignment)\n",
    "        layout_in2 = StridedLayout(irreps_in2, pad_to_multiple=pad_to_alignment)\n",
    "        layout_out = StridedLayout(irreps_out, pad_to_multiple=pad_to_alignment)\n",
    "    except ValueError:\n",
    "        # one cannot be strided\n",
    "        return None\n",
    "\n",
    "    # check the instructions\n",
    "    assert specialized_code\n",
    "\n",
    "    connection_mode = instructions[0].connection_mode\n",
    "    if not all(ins.connection_mode == connection_mode for ins in instructions):\n",
    "        return None\n",
    "\n",
    "    has_weight = instructions[0].has_weight\n",
    "    if not all(ins.has_weight == has_weight for ins in instructions):\n",
    "        return None\n",
    "    if not has_weight:\n",
    "        assert connection_mode == \"uuu\"  # for now\n",
    "\n",
    "    # TODO: sort insturctions?\n",
    "\n",
    "    # Make the big w3j\n",
    "    w3j_index = []\n",
    "    w3j_values = []\n",
    "\n",
    "    for ins_i, ins in enumerate(instructions):\n",
    "        mul_ir_in1 = layout_in1.base_irreps[ins.i_in1]\n",
    "        mul_ir_in2 = layout_in2.base_irreps[ins.i_in2]\n",
    "        mul_ir_out = layout_out.base_irreps[ins.i_out]\n",
    "\n",
    "        assert mul_ir_in1.ir.p * mul_ir_in2.ir.p == mul_ir_out.ir.p\n",
    "        assert (\n",
    "            abs(mul_ir_in1.ir.l - mul_ir_in2.ir.l)\n",
    "            <= mul_ir_out.ir.l\n",
    "            <= mul_ir_in1.ir.l + mul_ir_in2.ir.l\n",
    "        )\n",
    "\n",
    "        if mul_ir_in1.dim == 0 or mul_ir_in2.dim == 0 or mul_ir_out.dim == 0:\n",
    "            raise ValueError\n",
    "\n",
    "        this_w3j = o3.wigner_3j(mul_ir_in1.ir.l, mul_ir_in2.ir.l, mul_ir_out.ir.l)\n",
    "        this_w3j_index = this_w3j.nonzero()\n",
    "        w3j_values.append(\n",
    "            this_w3j[this_w3j_index[:, 0], this_w3j_index[:, 1], this_w3j_index[:, 2]]\n",
    "        )\n",
    "\n",
    "        # Normalize the path through its w3j entries\n",
    "        # TODO: path_weight\n",
    "        # TODO: in and out var\n",
    "        if normalization == \"component\":\n",
    "            w3j_norm_term = 2 * mul_ir_out.ir.l + 1\n",
    "        if normalization == \"norm\":\n",
    "            w3j_norm_term = (2 * mul_ir_in1.ir.l + 1) * (2 * mul_ir_in2.ir.l + 1)\n",
    "        alpha = sqrt(\n",
    "            ins.path_weight  # per-path weight\n",
    "            * out_var[ins.i_out]  # enforce output variance\n",
    "            * w3j_norm_term\n",
    "            / sum(\n",
    "                in1_var[i.i_in1]\n",
    "                * in2_var[i.i_in2]\n",
    "                * {\n",
    "                    \"uvw\": (layout_in1.mul * layout_in2.mul),\n",
    "                    \"uvu\": layout_in2.mul,\n",
    "                    \"uvv\": layout_in1.mul,\n",
    "                    \"uuw\": layout_in1.mul,\n",
    "                    \"uuu\": 1,\n",
    "                    \"uvuv\": 1,\n",
    "                }[connection_mode]\n",
    "                for i in instructions\n",
    "                if i.i_out == ins.i_out\n",
    "            )\n",
    "        )\n",
    "        w3j_values[-1].mul_(alpha)\n",
    "\n",
    "        this_w3j_index[:, 0] += layout_in1.base_irreps[: ins.i_in1].dim\n",
    "        this_w3j_index[:, 1] += layout_in2.base_irreps[: ins.i_in2].dim\n",
    "        this_w3j_index[:, 2] += layout_out.base_irreps[: ins.i_out].dim\n",
    "        # Now need to flatten the index to be for [pk][ij]\n",
    "        w3j_index.append(\n",
    "            torch.cat(\n",
    "                (\n",
    "                    (ins_i if ins.has_weight else 0)  # unweighted all go in first path\n",
    "                    * layout_out.base_dim\n",
    "                    + this_w3j_index[:, 2].unsqueeze(-1),\n",
    "                    this_w3j_index[:, 0].unsqueeze(-1) * layout_in2.base_dim\n",
    "                    + this_w3j_index[:, 1].unsqueeze(-1),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    num_paths: int = len(instructions) if has_weight else 1\n",
    "\n",
    "    w3j = torch.sparse_coo_tensor(\n",
    "        indices=torch.cat(w3j_index, dim=0).t(),\n",
    "        values=torch.cat(w3j_values, dim=0),\n",
    "        size=(\n",
    "            num_paths * layout_out.base_dim,\n",
    "            layout_in1.base_dim * layout_in2.base_dim,\n",
    "        ),\n",
    "    ).coalesce()\n",
    "\n",
    "    # w3j is k,i,j, so this is whether, for nonzero entries,\n",
    "    # the i index is always equal to the j index. If so, then\n",
    "    # it is diagonal and we can eliminate the j dimension\n",
    "    # in this case we are only taking diagonal (i == j)\n",
    "    # entries from the outer product; but those values are just\n",
    "    # the direct multiplication of the two tensors, eliminating\n",
    "    # the need for the outer product.\n",
    "    # obviously this only makes sense if they have the same size as well\n",
    "    # this is more or less a test of whether this TP is an inner product\n",
    "    w3j_i_indexes = torch.div(\n",
    "        w3j.indices()[1], layout_in1.base_dim, rounding_mode=\"floor\"\n",
    "    )\n",
    "    w3j_j_indexes = w3j.indices()[1] % layout_in1.base_dim\n",
    "    w3j_is_ij_diagonal = (layout_in1.base_dim == layout_in2.base_dim) and torch.all(\n",
    "        w3j_i_indexes == w3j_j_indexes\n",
    "    )\n",
    "    if w3j_is_ij_diagonal:\n",
    "        # change the w3j to eliminate the dimension\n",
    "        # now its just k,i\n",
    "        w3j = torch.sparse_coo_tensor(\n",
    "            indices=torch.stack((w3j.indices()[0], w3j_i_indexes)),\n",
    "            values=w3j.values(),\n",
    "            size=(\n",
    "                num_paths * layout_out.base_dim,\n",
    "                layout_in1.base_dim,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # TODO: support use of sparse w3j\n",
    "    if sparse_mode is None:\n",
    "        # in dense, must shape it for einsum:\n",
    "        if w3j_is_ij_diagonal:\n",
    "            kij_shape = (\n",
    "                layout_out.base_dim,\n",
    "                layout_in1.base_dim,\n",
    "            )\n",
    "        else:\n",
    "            kij_shape = (\n",
    "                layout_out.base_dim,\n",
    "                layout_in1.base_dim,\n",
    "                layout_in2.base_dim,\n",
    "            )\n",
    "        w3j = (\n",
    "            w3j.to_dense()\n",
    "            .reshape(((num_paths,) if num_paths > 1 else tuple()) + kij_shape)\n",
    "            .contiguous()\n",
    "        )\n",
    "        del kij_shape\n",
    "    elif sparse_mode == \"coo\":\n",
    "        w3j = w3j.coalesce()\n",
    "    elif sparse_mode == \"csr\":\n",
    "        w3j = w3j.coalesce().to_sparse_csr()\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    # Generate the mixer\n",
    "    u, v, w = connection_mode\n",
    "    uv = {\"uv\": \"uv\", \"uu\": \"u\"}[connection_mode[:2]]\n",
    "    if has_weight:\n",
    "        weight_label = {\"uvw\": \"uvw\", \"uuu\": \"u\", \"uvv\": \"uv\"}[connection_mode]\n",
    "\n",
    "        z = \"\" if shared_weights else \"z\"\n",
    "\n",
    "        weight_shape = {\n",
    "            \"uvw\": (layout_in1.mul, layout_in2.mul, layout_out.mul),\n",
    "            \"uuu\": (layout_in1.mul,),\n",
    "            \"uvv\": (layout_in1.mul, layout_in2.mul),\n",
    "        }[connection_mode]\n",
    "        if num_paths > 1:\n",
    "            # ^ if there's only one weighted path, the einsum simplifies without the p dimension\n",
    "            weight_label = weight_label + \"p\"\n",
    "            weight_shape = weight_shape + (num_paths,)\n",
    "        if not shared_weights:\n",
    "            weight_shape = (-1,) + weight_shape\n",
    "    else:\n",
    "        weight_shape = tuple()\n",
    "\n",
    "    # generate actual code\n",
    "    graph_out = fx.Graph()\n",
    "    tracer = fx.proxy.GraphAppendingTracer(graph_out)\n",
    "\n",
    "    def Proxy(n):\n",
    "        return fx.Proxy(n, tracer=tracer)\n",
    "\n",
    "    # = Function definitions =\n",
    "    x1s_out = Proxy(graph_out.placeholder(\"x1\", torch.Tensor))\n",
    "    x2s_out = Proxy(graph_out.placeholder(\"x2\", torch.Tensor))\n",
    "    if has_weight:\n",
    "        ws_out = Proxy(graph_out.placeholder(\"w\", torch.Tensor))\n",
    "        ws_out = ws_out.reshape(weight_shape)\n",
    "\n",
    "    if sparse_mode is None:\n",
    "        w3j_proxy = Proxy(graph_out.get_attr(\"_big_w3j\"))\n",
    "\n",
    "    # convert to strided\n",
    "    x1s_out = x1s_out.reshape(-1, layout_in1.mul, layout_in1.base_dim)\n",
    "    x2s_out = x2s_out.reshape(-1, layout_in2.mul, layout_in2.base_dim)\n",
    "\n",
    "    # do the einsum\n",
    "    # has shape zwk\n",
    "    j = \"i\" if w3j_is_ij_diagonal else \"j\"\n",
    "    ij = \"i\" if w3j_is_ij_diagonal else \"ij\"\n",
    "    if has_weight:\n",
    "        if sparse_mode is None:\n",
    "            # use einsum for the full contract\n",
    "            einstr = f\"{z}{weight_label},z{u}i,z{v}{j},{'p' if num_paths > 1 else ''}k{ij}->z{w}k\"\n",
    "            out = torch.einsum(einstr, ws_out, x1s_out, x2s_out, w3j_proxy)\n",
    "        else:\n",
    "            outer = torch.einsum(f\"z{u}i,z{v}{j}->z{uv}{ij}\", x1s_out, x2s_out)\n",
    "            # \\/ has shape [pk][ij] * [ij][zuv] = [pk][zuv]\n",
    "            contracted = Proxy(\n",
    "                graph_out.call_module(\n",
    "                    \"_w3j_mm\",\n",
    "                    (\n",
    "                        outer.reshape(\n",
    "                            -1,\n",
    "                            (\n",
    "                                layout_in1.base_dim\n",
    "                                if w3j_is_ij_diagonal\n",
    "                                else layout_in1.base_dim * layout_in2.base_dim\n",
    "                            ),\n",
    "                        ).T.node,\n",
    "                    ),\n",
    "                )\n",
    "            ).T.reshape(\n",
    "                (-1,)\n",
    "                + {\"uv\": (layout_in1.mul, layout_in2.mul), \"uu\": (layout_in1.mul,)}[\n",
    "                    connection_mode[:2]\n",
    "                ]\n",
    "                + (num_paths, layout_out.base_dim)\n",
    "            )\n",
    "            out = torch.einsum(f\"z{uv}pk,{z}{weight_label}->z{w}k\", contracted, ws_out)\n",
    "    else:\n",
    "        if sparse_mode is None:\n",
    "            # use einsum for the full contract\n",
    "            einstr = f\"z{u}i,z{v}{j},{'p' if num_paths > 1 else ''}k{ij}->z{w}k\"\n",
    "            out = torch.einsum(einstr, x1s_out, x2s_out, w3j_proxy)\n",
    "        else:\n",
    "            outer = torch.einsum(f\"z{u}i,z{v}{j}->z{uv}{ij}\", x1s_out, x2s_out)\n",
    "            # \\/ has shape [k][ij] * [ij][zuv] = [pk][zuv]\n",
    "            out = Proxy(\n",
    "                graph_out.call_module(\n",
    "                    \"_w3j_mm\",\n",
    "                    (\n",
    "                        outer.reshape(\n",
    "                            -1,\n",
    "                            (\n",
    "                                layout_in1.base_dim\n",
    "                                if w3j_is_ij_diagonal\n",
    "                                else layout_in1.base_dim * layout_in2.base_dim\n",
    "                            ),\n",
    "                        ).T.node,\n",
    "                    ),\n",
    "                )\n",
    "            ).T.reshape(\n",
    "                (\n",
    "                    -1,\n",
    "                    layout_in1.mul,  # its only uuu for now\n",
    "                    layout_out.base_dim,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    graph_out.output(out.node)\n",
    "\n",
    "    # check graphs\n",
    "    graph_out.lint()\n",
    "\n",
    "    # Make GraphModules\n",
    "    # By putting the constants in a Module rather than a dict,\n",
    "    # we force FX to copy them as buffers instead of as attributes.\n",
    "    #\n",
    "    # FX seems to have resolved this issue for dicts in 1.9, but we support all the way back to 1.8.0.\n",
    "    constants_root = torch.nn.Module()\n",
    "    constants_root.register_buffer(\"_big_w3j\", w3j)\n",
    "    if sparse_mode is not None:\n",
    "        constants_root._w3j_mm = ExplicitGradSpmm(w3j)\n",
    "    graphmod_out = fx.GraphModule(constants_root, graph_out, class_name=\"tp_forward\")\n",
    "\n",
    "    if True:  # optimize_einsums\n",
    "        # Note that for our einsums, we can optimize _once_ for _any_ batch dimension\n",
    "        # and still get the right path for _all_ batch dimensions.\n",
    "        # This is because our einsums are essentially of the form:\n",
    "        #    zuvw,ijk,zuvij->zwk    OR     uvw,ijk,zuvij->zwk\n",
    "        # In the first case, all but one operands have the batch dimension\n",
    "        #    => The first contraction gains the batch dimension\n",
    "        #    => All following contractions have batch dimension\n",
    "        #    => All possible contraction paths have cost that scales linearly in batch size\n",
    "        #    => The optimal path is the same for all batch sizes\n",
    "        # For the second case, this logic follows as long as the first contraction is not between the first two operands. Since those two operands do not share any indexes, contracting them first is a rare pathological case. See\n",
    "        # https://github.com/dgasmith/opt_einsum/issues/158\n",
    "        # for more details.\n",
    "        #\n",
    "        # TODO: consider the impact maximum intermediate result size on this logic\n",
    "        #         \\- this is the `memory_limit` option in opt_einsum\n",
    "        # TODO: allow user to choose opt_einsum parameters?\n",
    "        #\n",
    "        # We use float32 and zeros to save memory and time, since opt_einsum_fx looks only at traced shapes, not values or dtypes.\n",
    "        batchdim = 4\n",
    "        example_inputs = (\n",
    "            torch.zeros((batchdim, layout_in1.dim)),\n",
    "            torch.zeros((batchdim, layout_in2.dim)),\n",
    "            torch.zeros(\n",
    "                1 if shared_weights else batchdim,\n",
    "                sum(prod(ins.path_shape) for ins in instructions if ins.has_weight),\n",
    "            ),\n",
    "        )\n",
    "        graphmod_out = jitable(optimize_einsums_full(graphmod_out, example_inputs))\n",
    "\n",
    "    graphmod_out.weight_shape = weight_shape\n",
    "    graphmod_out._dim_in1 = layout_in1.base_dim\n",
    "    graphmod_out._dim_in2 = layout_in2.base_dim\n",
    "    graphmod_out._dim_out = layout_out.base_dim\n",
    "    graphmod_out._mul_out = layout_out.mul\n",
    "    graphmod_out.weight_numel = abs(prod(weight_shape))\n",
    "\n",
    "    return graphmod_out\n",
    "\n",
    "\n",
    "def Contracter(\n",
    "    irreps_in1,\n",
    "    irreps_in2,\n",
    "    irreps_out,\n",
    "    instructions: List[Tuple[int, int, int]],\n",
    "    has_weight: bool,\n",
    "    connection_mode: str,\n",
    "    pad_to_alignment: int = 1,\n",
    "    shared_weights: bool = False,\n",
    "    sparse_mode: Optional[str] = None,\n",
    "):\n",
    "    irreps_in1 = o3.Irreps(irreps_in1)\n",
    "    assert all(mul == irreps_in1[0].mul for mul, ir in irreps_in1)\n",
    "    irreps_in2 = o3.Irreps(irreps_in2)\n",
    "    assert all(mul == irreps_in2[0].mul for mul, ir in irreps_in2)\n",
    "    irreps_out = o3.Irreps(irreps_out)\n",
    "    assert all(mul == irreps_out[0].mul for mul, ir in irreps_out)\n",
    "\n",
    "    mod = codegen_strided_tensor_product_forward(\n",
    "        irreps_in1,\n",
    "        [1.0 for _ in irreps_in1],\n",
    "        irreps_in2,\n",
    "        [1.0 for _ in irreps_in2],\n",
    "        irreps_out,\n",
    "        [1.0 for _ in irreps_out],\n",
    "        instructions=[\n",
    "            Instruction(\n",
    "                i_in1,\n",
    "                i_in2,\n",
    "                i_out,\n",
    "                connection_mode,\n",
    "                has_weight,\n",
    "                1.0,\n",
    "                {\n",
    "                    \"uvw\": (\n",
    "                        irreps_in1[i_in1].mul,\n",
    "                        irreps_in2[i_in2].mul,\n",
    "                        irreps_out[i_out].mul,\n",
    "                    ),\n",
    "                    \"uvu\": (irreps_in1[i_in1].mul, irreps_in2[i_in2].mul),\n",
    "                    \"uvv\": (irreps_in1[i_in1].mul, irreps_in2[i_in2].mul),\n",
    "                    \"uuw\": (irreps_in1[i_in1].mul, irreps_out[i_out].mul),\n",
    "                    \"uuu\": (irreps_in1[i_in1].mul,),\n",
    "                    \"uvuv\": (\n",
    "                        irreps_in1[i_in1].mul,\n",
    "                        irreps_in2[i_in2].mul,\n",
    "                    ),\n",
    "                }[connection_mode],\n",
    "            )\n",
    "            for i_in1, i_in2, i_out in instructions\n",
    "        ],\n",
    "        shared_weights=shared_weights,\n",
    "        sparse_mode=sparse_mode,\n",
    "        pad_to_alignment=pad_to_alignment,\n",
    "    )\n",
    "    if mod is None:\n",
    "        raise ValueError(\"Couldn't use strided for given layout\")\n",
    "    if sparse_mode is None:\n",
    "        mod = compile(mod)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c26533c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.,  5.,  5.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([-5., 10., 5.]).clamp_(-5., 5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e133267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.5000, 0.5000])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([0., 0., 0.]).sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cba529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_mkl",
   "language": "python",
   "name": "torch_mkl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
