{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad2821d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/temporary/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.0.0 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# reference example\n",
    "from nequip.data import dataset_from_config\n",
    "from nequip.utils import Config\n",
    "from nequip.utils.misc import get_default_device_name\n",
    "from nequip.utils.config import _GLOBAL_ALL_ASKED_FOR_KEYS\n",
    "\n",
    "from nequip.model import model_from_config\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "default_config = dict(\n",
    "    root=\"./\",\n",
    "    tensorboard=False,\n",
    "    wandb=False,\n",
    "    model_builders=[\n",
    "        \"SimpleIrrepsConfig\",\n",
    "        \"EnergyModel\",\n",
    "        \"PerSpeciesRescale\",\n",
    "        \"StressForceOutput\",\n",
    "        \"RescaleEnergyEtc\",\n",
    "    ],\n",
    "    dataset_statistics_stride=1,\n",
    "    device=get_default_device_name(),\n",
    "    default_dtype=\"float64\",\n",
    "    model_dtype=\"float32\",\n",
    "    allow_tf32=True,\n",
    "    verbose=\"INFO\",\n",
    "    model_debug_mode=False,\n",
    "    equivariance_test=False,\n",
    "    grad_anomaly_mode=False,\n",
    "    gpu_oom_offload=False,\n",
    "    append=False,\n",
    "    warn_unused=False,\n",
    "    _jit_bailout_depth=2,  # avoid 20 iters of pain, see https://github.com/pytorch/pytorch/issues/52286\n",
    "    # Quote from eelison in PyTorch slack:\n",
    "    # https://pytorch.slack.com/archives/CDZD1FANA/p1644259272007529?thread_ts=1644064449.039479&cid=CDZD1FANA\n",
    "    # > Right now the default behavior is to specialize twice on static shapes and then on dynamic shapes.\n",
    "    # > To reduce warmup time you can do something like setFusionStrartegy({{FusionBehavior::DYNAMIC, 3}})\n",
    "    # > ... Although we would wouldn't really expect to recompile a dynamic shape fusion in a model,\n",
    "    # > provided broadcasting patterns remain fixed\n",
    "    # We default to DYNAMIC alone because the number of edges is always dynamic,\n",
    "    # even if the number of atoms is fixed:\n",
    "    _jit_fusion_strategy=[(\"DYNAMIC\", 3)],\n",
    "    # Due to what appear to be ongoing bugs with nvFuser, we default to NNC (fuser1) for now:\n",
    "    # TODO: still default to NNC on CPU regardless even if change this for GPU\n",
    "    # TODO: default for ROCm?\n",
    "    _jit_fuser=\"fuser1\",\n",
    ")\n",
    "\n",
    "# All default_config keys are valid / requested\n",
    "_GLOBAL_ALL_ASKED_FOR_KEYS.update(default_config.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d80fac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtomicData(atom_types=[21, 1], cell=[3, 3], edge_cell_shift=[364, 3], edge_index=[2, 364], forces=[21, 3], pbc=[3], pos=[21, 3], total_energy=[1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config.from_file('./configs/example.yaml', defaults=default_config)\n",
    "    \n",
    "\n",
    "dataset = dataset_from_config(config, prefix=\"dataset\")\n",
    "\n",
    "validation_dataset = None\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87c25eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:* Initialize Output\n",
      "  ...generate file name results/aspirin/example/log\n",
      "  ...open log file results/aspirin/example/log\n",
      "  ...generate file name results/aspirin/example/metrics_epoch.csv\n",
      "  ...open log file results/aspirin/example/metrics_epoch.csv\n",
      "  ...generate file name results/aspirin/example/metrics_initialization.csv\n",
      "  ...open log file results/aspirin/example/metrics_initialization.csv\n",
      "  ...generate file name results/aspirin/example/metrics_batch_train.csv\n",
      "  ...open log file results/aspirin/example/metrics_batch_train.csv\n",
      "  ...generate file name results/aspirin/example/metrics_batch_val.csv\n",
      "  ...open log file results/aspirin/example/metrics_batch_val.csv\n",
      "  ...generate file name results/aspirin/example/best_model.pth\n",
      "  ...generate file name results/aspirin/example/last_model.pth\n",
      "  ...generate file name results/aspirin/example/trainer.pth\n",
      "  ...generate file name results/aspirin/example/config.yaml\n",
      "Torch device: cpu\n",
      "instantiate Loss\n",
      "...Loss_param = dict(\n",
      "...   optional_args = {},\n",
      "...   positional_args = {'coeffs': {'forces': 1.0, 'total_energy': [1.0, 'PerAtomMSELoss']}})\n",
      "instantiate MSELoss\n",
      "...MSELoss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      " parsing forces 1.0\n",
      " parsing 1.0 MSELoss\n",
      "instantiate MSELoss\n",
      "...MSELoss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      " parsing total_energy [1.0, 'PerAtomMSELoss']\n",
      " parsing 1.0 PerAtomMSELoss\n",
      "create loss instance <class 'nequip.train._loss.PerAtomLoss'>\n",
      "instantiate MSELoss\n",
      "...MSELoss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      "Building Allegro model...\n",
      "instantiate OneHotAtomEncoding\n",
      "        all_args :                                           num_types\n",
      "...OneHotAtomEncoding_param = dict(\n",
      "...   optional_args = {'set_features': True, 'num_types': 3},\n",
      "...   positional_args = {'irreps_in': None})\n",
      "instantiate RadialBasisEdgeEncoding\n",
      "        all_args :                                  basis_kwargs.r_max <-                                              r_max\n",
      "        all_args :            basis_kwargs.original_basis_kwargs.r_max <-                                              r_max\n",
      "        all_args :        basis_kwargs.original_basis_kwargs.trainable <-                              BesselBasis_trainable\n",
      "        all_args :                                 cutoff_kwargs.r_max <-                                              r_max\n",
      "        all_args :                                     cutoff_kwargs.p <-                                 PolynomialCutoff_p\n",
      "   optional_args :                                               basis\n",
      "   optional_args :                                           out_field\n",
      "...RadialBasisEdgeEncoding_param = dict(\n",
      "...   optional_args = {'basis': <class 'allegro.nn._norm_basis.NormalizedBasis'>, 'cutoff': <class 'nequip.nn.cutoffs.PolynomialCutoff'>, 'basis_kwargs': {'r_min': 0.0, 'original_basis': <class 'nequip.nn.radial_basis.BesselBasis'>, 'original_basis_kwargs': {'num_basis': 8, 'trainable': True, 'r_max': 6.0}, 'n': 4000, 'norm_basis_mean_shift': True, 'r_max': 6.0}, 'cutoff_kwargs': {'p': 6, 'r_max': 6.0}, 'out_field': 'edge_embedding'},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1o, 'edge_index': None, 'node_attrs': 3x0e, 'node_features': 3x0e}})\n",
      "instantiate SphericalHarmonicEdgeAttrs\n",
      "        all_args :                                      irreps_edge_sh\n",
      "...SphericalHarmonicEdgeAttrs_param = dict(\n",
      "...   optional_args = {'edge_sh_normalization': 'component', 'edge_sh_normalize': True, 'out_field': 'edge_attrs', 'irreps_edge_sh': '1x0e+1x1o+1x2e'},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1o, 'edge_index': None, 'node_attrs': 3x0e, 'node_features': 3x0e, 'edge_embedding': 8x0e, 'edge_cutoff': 1x0e}})\n",
      "instantiate Allegro_Module\n",
      "        all_args :                                  PolynomialCutoff_p\n",
      "        all_args :                                               r_max\n",
      "        all_args :                              env_embed_multiplicity\n",
      "        all_args :                                       latent_resnet\n",
      "        all_args :                                          num_layers\n",
      "        all_args :                                   avg_num_neighbors\n",
      "        all_args :                                           num_types\n",
      "        all_args :                           nonscalars_include_parity\n",
      "        all_args :                                  embed_initial_edge\n",
      "        all_args :        two_body_latent_kwargs.mlp_latent_dimensions <-              two_body_latent_mlp_latent_dimensions\n",
      "        all_args :             two_body_latent_kwargs.mlp_nonlinearity <-                   two_body_latent_mlp_nonlinearity\n",
      "        all_args :           two_body_latent_kwargs.mlp_initialization <-                 two_body_latent_mlp_initialization\n",
      "        all_args :              env_embed_kwargs.mlp_latent_dimensions <-                    env_embed_mlp_latent_dimensions\n",
      "        all_args :                   env_embed_kwargs.mlp_nonlinearity <-                         env_embed_mlp_nonlinearity\n",
      "        all_args :                 env_embed_kwargs.mlp_initialization <-                       env_embed_mlp_initialization\n",
      "        all_args :                 latent_kwargs.mlp_latent_dimensions <-                       latent_mlp_latent_dimensions\n",
      "        all_args :                      latent_kwargs.mlp_nonlinearity <-                            latent_mlp_nonlinearity\n",
      "        all_args :                    latent_kwargs.mlp_initialization <-                          latent_mlp_initialization\n",
      "   optional_args :                                               field\n",
      "   optional_args :                                node_invariant_field\n",
      "   optional_args :                                edge_invariant_field\n",
      "...Allegro_Module_param = dict(\n",
      "...   optional_args = {'avg_num_neighbors': 17.211328506469727, 'r_start_cos_ratio': 0.8, 'PolynomialCutoff_p': 6, 'per_layer_cutoffs': None, 'cutoff_type': 'polynomial', 'field': 'edge_attrs', 'edge_invariant_field': 'edge_embedding', 'node_invariant_field': 'node_attrs', 'env_embed_multiplicity': 64, 'embed_initial_edge': True, 'linear_after_env_embed': False, 'nonscalars_include_parity': True, 'two_body_latent': <class 'allegro.nn._fc.ScalarMLPFunction'>, 'two_body_latent_kwargs': {'mlp_nonlinearity': 'silu', 'mlp_initialization': 'uniform', 'mlp_dropout_p': 0.0, 'mlp_batchnorm': False, 'mlp_latent_dimensions': [128, 256, 512, 1024]}, 'env_embed': <class 'allegro.nn._fc.ScalarMLPFunction'>, 'env_embed_kwargs': {'mlp_nonlinearity': None, 'mlp_initialization': 'uniform', 'mlp_dropout_p': 0.0, 'mlp_batchnorm': False, 'mlp_latent_dimensions': []}, 'latent': <class 'allegro.nn._fc.ScalarMLPFunction'>, 'latent_kwargs': {'mlp_nonlinearity': 'silu', 'mlp_initialization': 'uniform', 'mlp_dropout_p': 0.0, 'mlp_batchnorm': False, 'mlp_latent_dimensions': [1024, 1024, 1024]}, 'latent_resnet': True, 'latent_resnet_update_ratios': None, 'latent_resnet_update_ratios_learnable': False, 'latent_out_field': 'edge_features', 'pad_to_alignment': 1, 'sparse_mode': None, 'r_max': 6.0, 'num_layers': 2, 'num_types': 3},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1o, 'edge_index': None, 'node_attrs': 3x0e, 'node_features': 3x0e, 'edge_embedding': 8x0e, 'edge_cutoff': 1x0e, 'edge_attrs': 1x0e+1x1o+1x2e}})\n",
      "/Users/temporary/Documents/GitHub/pytorch-intel-mps/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n",
      "instantiate ScalarMLP\n",
      "        all_args :                               mlp_latent_dimensions <-                     edge_eng_mlp_latent_dimensions\n",
      "        all_args :                                    mlp_nonlinearity <-                          edge_eng_mlp_nonlinearity\n",
      "        all_args :                                  mlp_initialization <-                        edge_eng_mlp_initialization\n",
      "   optional_args :                                               field\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   optional_args :                                           out_field\n",
      "   optional_args :                                mlp_output_dimension\n",
      "...ScalarMLP_param = dict(\n",
      "...   optional_args = {'mlp_nonlinearity': None, 'mlp_initialization': 'uniform', 'mlp_dropout_p': 0.0, 'mlp_batchnorm': False, 'field': 'edge_features', 'out_field': 'edge_energy', 'mlp_latent_dimensions': [128], 'mlp_output_dimension': 1},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1o, 'edge_index': None, 'node_attrs': 3x0e, 'node_features': 3x0e, 'edge_embedding': 8x0e, 'edge_cutoff': 1x0e, 'edge_attrs': 1x0e+1x1o+1x2e, 'edge_features': 1024x0e}})\n",
      "instantiate EdgewiseEnergySum\n",
      "        all_args :                                   avg_num_neighbors\n",
      "        all_args :                                           num_types\n",
      "...EdgewiseEnergySum_param = dict(\n",
      "...   optional_args = {'avg_num_neighbors': 17.211328506469727, 'normalize_edge_energy_sum': True, 'per_edge_species_scale': False, 'num_types': 3},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1o, 'edge_index': None, 'node_attrs': 3x0e, 'node_features': 3x0e, 'edge_embedding': 8x0e, 'edge_cutoff': 1x0e, 'edge_attrs': 1x0e+1x1o+1x2e, 'edge_features': 1024x0e, 'edge_energy': 1x0e}})\n",
      "instantiate AtomwiseReduce\n",
      "   optional_args :                                              reduce\n",
      "   optional_args :                                               field\n",
      "   optional_args :                                           out_field\n",
      "...AtomwiseReduce_param = dict(\n",
      "...   optional_args = {'out_field': 'total_energy', 'reduce': 'sum', 'avg_num_atoms': None, 'field': 'atomic_energy'},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1o, 'edge_index': None, 'node_attrs': 3x0e, 'node_features': 3x0e, 'edge_embedding': 8x0e, 'edge_cutoff': 1x0e, 'edge_attrs': 1x0e+1x1o+1x2e, 'edge_features': 1024x0e, 'edge_energy': 1x0e, 'atomic_energy': 1x0e}})\n",
      "Replace string dataset_per_atom_total_energy_mean to -19318.35546875\n",
      "Atomic outputs are scaled by: [H, C, O: None], shifted by [H, C, O: -19318.355469].\n",
      "instantiate PerSpeciesScaleShift\n",
      "        all_args :                                           num_types\n",
      "        all_args :                                       default_dtype\n",
      "        all_args :                                          type_names\n",
      "   optional_args :                                              shifts\n",
      "   optional_args :                                               field\n",
      "   optional_args :                                           out_field\n",
      "   optional_args :                                              scales\n",
      "   optional_args :                          arguments_in_dataset_units\n",
      "...PerSpeciesScaleShift_param = dict(\n",
      "...   optional_args = {'out_field': 'atomic_energy', 'scales_trainable': False, 'shifts_trainable': False, 'default_dtype': 'float32', 'num_types': 3, 'type_names': ['H', 'C', 'O'], 'field': 'atomic_energy', 'shifts': tensor(-19318.3555), 'scales': None, 'arguments_in_dataset_units': True},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1o, 'edge_index': None, 'node_attrs': 3x0e, 'node_features': 3x0e, 'edge_embedding': 8x0e, 'edge_cutoff': 1x0e, 'edge_attrs': 1x0e+1x1o+1x2e, 'edge_features': 1024x0e, 'edge_energy': 1x0e, 'atomic_energy': 1x0e}})\n",
      "Replace string dataset_forces_rms to 31.252248764038086\n",
      "Initially outputs are globally scaled by: 31.252248764038086, total_energy are globally shifted by None.\n",
      "PerSpeciesScaleShift's arguments were in dataset units; rescaling:\n",
      "  Original scales: n/a shifts: [H: -19318.355469, C: -19318.355469, O: -19318.355469]\n",
      "  New scales: n/a shifts: [H: -618.142883, C: -618.142883, O: -618.142883]\n"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "from nequip.train.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(model=None, **Config.as_dict(config))\n",
    "\n",
    "# what is this\n",
    "# to update wandb data?\n",
    "config.update(trainer.params)\n",
    "\n",
    "# = Train/test split =\n",
    "trainer.set_dataset(dataset, validation_dataset)\n",
    "\n",
    "# = Build model =\n",
    "final_model = model_from_config(\n",
    "    config=config, initialize=True, dataset=trainer.dataset_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ea3507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model = final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e61479ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of weights: 8152968\n",
      "Number of trainable weights: 8152968\n",
      "instantiate Adam\n",
      "...Adam_param = dict(\n",
      "...   optional_args = {'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'foreach': None, 'maximize': False, 'capturable': False, 'differentiable': False, 'fused': None},\n",
      "...   positional_args = {'params': <generator object Module.parameters at 0x7fbc740e33e0>, 'lr': 0.001})\n",
      "instantiate ReduceLROnPlateau\n",
      "        all_args :                                              factor <-                                lr_scheduler_factor\n",
      "        all_args :                                            patience <-                              lr_scheduler_patience\n",
      "...ReduceLROnPlateau_param = dict(\n",
      "...   optional_args = {'mode': 'min', 'factor': 0.5, 'patience': 50, 'threshold': 0.0001, 'threshold_mode': 'rel', 'cooldown': 0, 'min_lr': 0, 'eps': 1e-08, 'verbose': False},\n",
      "...   positional_args = {'optimizer': Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")})\n",
      "get args for EarlyStopping\n",
      "        all_args :                                        upper_bounds <-                        early_stopping_upper_bounds\n",
      "        all_args :                                        lower_bounds <-                        early_stopping_lower_bounds\n",
      "        all_args :                                           patiences <-                           early_stopping_patiences\n",
      "...EarlyStopping_param = dict(\n",
      "...   optional_args = {'lower_bounds': {'LR': 1e-05}, 'upper_bounds': {'cumulative_wall': 604800.0}, 'patiences': {'validation_loss': 100}, 'delta': {}, 'cumulative_delta': False},\n",
      "...   positional_args = {})\n",
      "! Starting training ...\n",
      "Saved trainer to results/aspirin/example/trainer.pth\n",
      "Saved last model to to results/aspirin/example/last_model.pth\n",
      "instantiate Metrics\n",
      "...Metrics_param = dict(\n",
      "...   optional_args = {},\n",
      "...   positional_args = {'components': [('forces', 'mae', {'PerSpecies': False}), ('forces', 'rmse', {'PerSpecies': False}), ('total_energy', 'mae', {'PerSpecies': False}), ('total_energy', 'rmse', {'PerSpecies': False})]})\n",
      "instantiate L1Loss\n",
      "...L1Loss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      "instantiate L1Loss\n",
      "...L1Loss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      "instantiate L1Loss\n",
      "...L1Loss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      "instantiate L1Loss\n",
      "...L1Loss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11a4fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e3nn import o3\n",
    "\n",
    "l_max = 2\n",
    "\n",
    "irreps_edge_sh = repr(\n",
    "            o3.Irreps.spherical_harmonics(\n",
    "                l_max, p=(-1)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4bb8134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1x0e+1x1o+1x2e'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irreps_edge_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96a9dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "from nequip.data import AtomicData, AtomicDataDict\n",
    "from torch.nn.functional import one_hot\n",
    "from e3nn.nn import FullyConnectedNet\n",
    "    \n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "\n",
    "data = AtomicData.to_AtomicDataDict(dataset[0])\n",
    "\n",
    "\n",
    "\n",
    "# edge length embedding\n",
    "torch.manual_seed(32)\n",
    "\n",
    "num_basis = 8\n",
    "r_max = 5\n",
    "\n",
    "\n",
    "data_my = {key: torch.clone(data[key]) for key in data}\n",
    "data_my = AtomicDataDict.with_edge_vectors(data_my, with_lengths=True)\n",
    "\n",
    "edge_length = data_my['edge_lengths']\n",
    "\n",
    "bessel_weights = (torch.linspace(start=1.0, end=num_basis, steps=num_basis) * math.pi)\n",
    "bessel_weights = nn.Parameter(bessel_weights)\n",
    "\n",
    "edge_length_embedding = 2/r_max*torch.sin(bessel_weights * edge_length.unsqueeze(-1) / r_max)/edge_length.unsqueeze(-1)\n",
    "\n",
    "# cutoff\n",
    "factor = 1/r_max\n",
    "p = 6\n",
    "    \n",
    "x = edge_length * factor\n",
    "\n",
    "cutoff = 1.0\n",
    "cutoff = cutoff - (((p + 1.0) * (p + 2.0) / 2.0) * torch.pow(x, p))\n",
    "cutoff = cutoff + (p * (p + 2.0) * torch.pow(x, p + 1.0))\n",
    "cutoff = cutoff - ((p * (p + 1.0) / 2) * torch.pow(x, p + 2.0))\n",
    "cutoff *= (x < 1.0)\n",
    "\n",
    "cutoff = cutoff.unsqueeze(-1)\n",
    "\n",
    "data_my['edge_embedding'] = edge_length_embedding * cutoff\n",
    "\n",
    "# types embedding\n",
    "num_classes = 3\n",
    "\n",
    "\n",
    "edge_ind = data_my['edge_index']\n",
    "\n",
    "types_embed = one_hot(dataset[0]['atom_types'], num_classes)\n",
    "types_src = types_embed[edge_ind[0]].squeeze(1)\n",
    "types_dst = types_embed[edge_ind[1]].squeeze(1)\n",
    "\n",
    "\n",
    "\n",
    "# latent vector\n",
    "latent_vector = torch.concatenate([types_src, types_dst, edge_length_embedding], dim = 1)\n",
    "\n",
    "# MLP\n",
    "invariant_layers = 2\n",
    "invariant_neurons = 64\n",
    "out_neurons = 32\n",
    "\n",
    "fc = FullyConnectedNet(\n",
    "    [latent_vector.shape[1]]\n",
    "    + invariant_layers * [invariant_neurons]\n",
    "    + [out_neurons],\n",
    "    torch.nn.functional.silu)\n",
    "\n",
    "latent_vector_out = fc(latent_vector)\n",
    "\n",
    "\n",
    "data_my['scalar'] = latent_vector_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c708419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([364, 14])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89c32180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([364, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_vector_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "018f85fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([364, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nequip.nn import AtomwiseLinear\n",
    "from e3nn.o3 import Irreps\n",
    "\n",
    "\n",
    "#data2_my = {key: torch.clone(data_my[key]) for key in data_my}\n",
    "\n",
    "linear1 = o3.Linear('32x0e', '32x0e')\n",
    "\n",
    "weight1 = linear1(latent_vector_out)\n",
    "weight1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fae20a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "\n",
    "\n",
    "torch.manual_seed(32)\n",
    "\n",
    "\n",
    "l_max = 2\n",
    "irreps_edge_sh = o3.Irreps.spherical_harmonics(2)\n",
    "\n",
    "data2_my = {key: torch.clone(data_my[key]) for key in data_my}\n",
    "data2_my = AtomicDataDict.with_edge_vectors(data2_my, with_lengths=False)\n",
    "\n",
    "\n",
    "harm_gen = o3.SphericalHarmonics(irreps_edge_sh, True, 'component')\n",
    "\n",
    "edge_vec = data_my['edge_vectors']\n",
    "\n",
    "harm_edge = harm_gen(edge_vec)\n",
    "harm_edge.shape\n",
    "\n",
    "\n",
    "data2_my['edge_features'] = harm_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70f558c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([364, 32, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from e3nn.o3 import TensorProduct, Linear, FullyConnectedTensorProduct\n",
    "from torch_runstats.scatter import scatter\n",
    "\n",
    "x = data2_my['edge_features']\n",
    "edge_src = data2_my['edge_index'][1]\n",
    "edge_dst = data2_my['edge_index'][0]\n",
    "\n",
    "\n",
    "term_1 = harm_edge\n",
    "edge_features = torch.einsum('ij,ib->ijb', weight1[edge_src], harm_edge[edge_src])\n",
    "\n",
    "# TODO: Check if it really right result\n",
    "edge_features = scatter(edge_features, edge_dst, dim=0, dim_size=len(x))\n",
    "edge_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "468f7063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullyConnectedTensorProduct(1x0e+1x1o+1x2e x 32x0e+32x1o+32x2e -> 32x0e+32x0o+32x1e+32x1o+32x2e+32x2o | 15360 paths | 15360 weights)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from e3nn.o3 import TensorProduct, Linear, FullyConnectedTensorProduct\n",
    "\n",
    "hidden_layer_irrep = o3.Irreps(\"32x0e + 32x0o + 32x1e + 32x1o + 32x2e + 32x2o\")\n",
    "\n",
    "irreps_in = o3.Irreps(\"1x0e + 1x1o + 1x2e\")\n",
    "irreps_edge = o3.Irreps(\"32x0e + 32x1o + 32x2e\")\n",
    "\n",
    "\n",
    "irreps_mid = []\n",
    "instructions = []\n",
    "\n",
    "# instructions means stuff for multiplicities\n",
    "for i, (_, ir_in) in enumerate(irreps_in):\n",
    "    for j, (mul, ir_edge) in enumerate(irreps_edge):\n",
    "        for ir_out in ir_in * ir_edge:\n",
    "            if ir_out in hidden_layer_irrep:\n",
    "                k = len(irreps_mid)\n",
    "                irreps_mid.append((mul, ir_out))\n",
    "                instructions.append((i, j, k, \"uvu\", True))\n",
    "\n",
    "# We sort the output irreps of the tensor product so that we can simplify them\n",
    "# when they are provided to the second o3.Linear\n",
    "irreps_mid = o3.Irreps(irreps_mid)\n",
    "irreps_mid, p, _ = irreps_mid.sort()\n",
    "\n",
    "fctp = FullyConnectedTensorProduct(\n",
    "            irreps_in,\n",
    "            irreps_edge,\n",
    "            hidden_layer_irrep\n",
    "        )\n",
    "\n",
    "fctp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2b1d2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2e\n"
     ]
    }
   ],
   "source": [
    "irreps_scalar = o3.Irreps('32x0e')\n",
    "\n",
    "for el in irreps_scalar[0][1] * irreps_in[2][1]:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f8c94ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32x0e+1x0e+1x1o+1x2e"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irreps_scalar + irreps_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f6217f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32x0e+32x1o+32x2e"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irreps_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fdd9e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/temporary/Documents/GitHub/pytorch-intel-mps/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from e3nn.o3 import FullTensorProduct\n",
    "\n",
    "#fctp(irrep_in, o3.Irreps('32x0e'))\n",
    "irreps_scalar = o3.Irreps('32x0e')\n",
    "\n",
    "\n",
    "fctp_simple = FullyConnectedTensorProduct(\n",
    "            irreps_scalar,\n",
    "            irreps_in,\n",
    "            irreps_edge\n",
    ")\n",
    "\n",
    "fctp_simple.visualize();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee46a3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullyConnectedTensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 3072 paths | 3072 weights)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fctp_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a01e1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Figure size 640x480 with 1 Axes>, <Axes: >)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fctp.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62eeca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_features_simple = fctp_simple(weight1, harm_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d54f9d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([364]), torch.Size([364, 288]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_dst.shape, edge_features_simple.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72ec3bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([364, 288])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Check if it really right result\n",
    "edge_features_simple = scatter(edge_features_simple, edge_dst, dim=0, dim_size=len(x))\n",
    "edge_features_simple.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57e4b9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32x0e+32x1o+32x2e"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fctp_simple.irreps_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5fb994a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([364, 288])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_features_simple.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e79b4b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "288/9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6a43c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irreps_edge.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4aba2455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7318,  1.5868,  3.1454,  ...,  1.4503, -1.1855,  0.3548],\n",
       "        [ 0.1975,  1.8518,  4.1484,  ...,  0.5697, -0.9912, -0.1461],\n",
       "        [ 0.7615,  0.9864,  3.7604,  ...,  1.7781, -0.9809,  0.9555],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(edge_features.transpose(2, 1), -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c750875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -4.4395,  -8.6668,  -7.1808,  ...,   0.7276,  -0.4207,   0.7423],\n",
       "        [ -4.9382, -10.0506,  -8.1547,  ...,   1.0427,   0.1516,   0.0430],\n",
       "        [ -3.9019,  -7.6113,  -6.3113,  ...,   0.3851,  -0.2934,   0.1124],\n",
       "        ...,\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
       "       grad_fn=<ScatterAddBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_features_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c485448",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = fctp(harm_edge, edge_features_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d55c973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([364, 576])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "788810b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([364, 9, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harm_edge.unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ccac2e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([364, 9, 32])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(edge_features.transpose(2, 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6eb25f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([364, 32, 9])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0717d198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32x0e+32x0o+32x1e+32x1o+32x2e+32x2o"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer_irrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "81cc6e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([364, 576])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8a797ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32x0e, 32x0o+32x1e+32x1o+32x2e+32x2o)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from e3nn.nn import Extract\n",
    "\n",
    "irreps_scalar = []\n",
    "irreps_features = []\n",
    "for ir in hidden_layer_irrep:\n",
    "    if ir.ir[1] == 1 and ir.ir[0] == 0:\n",
    "        irreps_scalar.append(ir)\n",
    "    else:\n",
    "        irreps_features.append(ir)\n",
    "        \n",
    "irreps_scalar = o3.Irreps(irreps_scalar)\n",
    "irreps_features = o3.Irreps(irreps_features)\n",
    "\n",
    "Extract_new = Extract(hidden_layer_irrep,\n",
    "        [irreps_scalar, irreps_features],\n",
    "        instructions=[(0,), (1, 2, 3, 4, 5)])\n",
    "out2 = Extract_new(out)\n",
    "\n",
    "Extract_new.irreps_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4115a7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([364, 32]), torch.Size([364, 544]), torch.Size([364, 576]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2[0].shape, out2[1].shape, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b6ab7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_1 = o3.Irreps('32x0e')\n",
    "in_2 = o3.Irreps('32x0e')\n",
    "\n",
    "ir_out2 = in_1 + in_2\n",
    "\n",
    "scalar_concat = torch.concatenate([latent_vector_out, out2[0]], dim = -1)\n",
    "\n",
    "linear_scalar = o3.Linear(ir_out2, '32x0e')\n",
    "linear_features = o3.Linear(hidden_layer_irrep, hidden_layer_irrep)\n",
    "\n",
    "\n",
    "\n",
    "data2_my['scalar'] = linear_scalar(scalar_concat) + latent_vector_out\n",
    "data2_my['edge_attrs'] = linear_features(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a32df3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([364, 64])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e830704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5633, -2.9845,  3.7044,  ...,  2.2098,  2.6116,  2.2809],\n",
       "        [-3.7988, -0.1964,  3.8040,  ..., -3.7400, -1.1233, -1.0725],\n",
       "        [-2.3400,  1.8147,  2.9686,  ..., -3.1784,  1.0734,  1.9833],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "062c277a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[slice(0, 32, None),\n",
       " slice(32, 128, None),\n",
       " slice(128, 224, None),\n",
       " slice(224, 384, None),\n",
       " slice(384, 544, None)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Extract_new.irreps_outs[1].slices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25e3ac21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.]), tensor([2.]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Extract('1e + 0e + 0e', ['0e', '0e'], [(1,), (2,)])\n",
    "c(torch.tensor([0.0, 0.0, 0.0, 1.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bcde216c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1, 5)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hidden_layer_irrep), len(irreps_scalar), len(irreps_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "30593da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 96, 96, 160, 160)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(el.dim for el in hidden_layer_irrep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b431ecd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 96, 96, 160, 160)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(el.dim for el in irreps_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f68958ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32x0e+32x0o+32x1e+32x1o+32x2e+32x2o"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer_irrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "067d58a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(irreps_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d041d704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32x0o+32x1e+32x1o+32x2e+32x2o"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irreps_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7382a8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "irreps_features.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "857ce025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32x0o+32x1e+32x1o+32x2e+32x2o"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irreps_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f1b1e51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_attrs\n",
      "edge_attrs\n",
      "edge_embedding\n"
     ]
    }
   ],
   "source": [
    "print(AtomicDataDict.NODE_ATTRS_KEY)\n",
    "print(AtomicDataDict.EDGE_ATTRS_KEY)\n",
    "print(AtomicDataDict.EDGE_EMBEDDING_KEY)\n",
    "#data_my['node_attrs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eb7d36",
   "metadata": {},
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "04a0068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "from nequip.data import AtomicData, AtomicDataDict\n",
    "from torch.nn.functional import one_hot\n",
    "from e3nn.nn import FullyConnectedNet\n",
    "    \n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "\n",
    "data = AtomicData.to_AtomicDataDict(dataset[0])\n",
    "\n",
    "\n",
    "\n",
    "# edge length embedding\n",
    "torch.manual_seed(32)\n",
    "\n",
    "num_basis = 8\n",
    "r_max = 5\n",
    "\n",
    "\n",
    "data_my = {key: torch.clone(data[key]) for key in data}\n",
    "data_my = AtomicDataDict.with_edge_vectors(data_my, with_lengths=True)\n",
    "\n",
    "edge_length = data_my['edge_lengths']\n",
    "\n",
    "bessel_weights = (torch.linspace(start=1.0, end=num_basis, steps=num_basis) * math.pi)\n",
    "bessel_weights = nn.Parameter(bessel_weights)\n",
    "\n",
    "edge_length_embedding = 2/r_max*torch.sin(bessel_weights * edge_length.unsqueeze(-1) / r_max)/edge_length.unsqueeze(-1)\n",
    "\n",
    "# cutoff\n",
    "factor = 1/r_max\n",
    "p = 6\n",
    "    \n",
    "x = edge_length * factor\n",
    "\n",
    "cutoff = 1.0\n",
    "cutoff = cutoff - (((p + 1.0) * (p + 2.0) / 2.0) * torch.pow(x, p))\n",
    "cutoff = cutoff + (p * (p + 2.0) * torch.pow(x, p + 1.0))\n",
    "cutoff = cutoff - ((p * (p + 1.0) / 2) * torch.pow(x, p + 2.0))\n",
    "cutoff *= (x < 1.0)\n",
    "\n",
    "cutoff = cutoff.unsqueeze(-1)\n",
    "\n",
    "data_my['edge_embedding'] = edge_length_embedding * cutoff\n",
    "\n",
    "# types embedding\n",
    "num_classes = 3\n",
    "\n",
    "\n",
    "edge_ind = data_my['edge_index']\n",
    "\n",
    "types_embed = one_hot(dataset[0]['atom_types'], num_classes)\n",
    "types_src = types_embed[edge_ind[0]].squeeze(1)\n",
    "types_dst = types_embed[edge_ind[1]].squeeze(1)\n",
    "\n",
    "\n",
    "\n",
    "# latent vector\n",
    "latent_vector = torch.concatenate([types_src, types_dst, edge_length_embedding], dim = 1)\n",
    "\n",
    "# MLP\n",
    "invariant_layers = 2\n",
    "invariant_neurons = 64\n",
    "out_neurons = 32\n",
    "\n",
    "fc = FullyConnectedNet(\n",
    "    [latent_vector.shape[1]]\n",
    "    + invariant_layers * [invariant_neurons]\n",
    "    + [out_neurons],\n",
    "    torch.nn.functional.silu)\n",
    "\n",
    "latent_vector_out = fc(latent_vector)\n",
    "\n",
    "\n",
    "data_my['scalar'] = latent_vector_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7cc52dc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnequip\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GraphModuleMixin\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnequip\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtp_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tp_path_exists\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ScalarMLPFunction\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _keys\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_strided\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Contracter, MakeWeightedChannels, Linear\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from typing import Optional, List\n",
    "import math\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "from torch_runstats.scatter import scatter\n",
    "\n",
    "from e3nn import o3\n",
    "from e3nn.util.jit import compile_mode\n",
    "\n",
    "from nequip.data import AtomicDataDict\n",
    "from nequip.nn import GraphModuleMixin\n",
    "from nequip.utils.tp_utils import tp_path_exists\n",
    "\n",
    "from ._fc import ScalarMLPFunction\n",
    "from .. import _keys\n",
    "from ._strided import Contracter, MakeWeightedChannels, Linear\n",
    "from .cutoffs import cosine_cutoff, polynomial_cutoff\n",
    "\n",
    "\n",
    "@compile_mode(\"script\")\n",
    "class Allegro_Module(GraphModuleMixin, torch.nn.Module):\n",
    "    # saved params\n",
    "    num_layers: int\n",
    "    field: str\n",
    "    out_field: str\n",
    "    num_types: int\n",
    "    env_embed_mul: int\n",
    "    weight_numel: int\n",
    "    latent_resnet: bool\n",
    "    embed_initial_edge: bool\n",
    "\n",
    "    # internal values\n",
    "    _env_builder_w_index: List[int]\n",
    "    _env_builder_n_irreps: int\n",
    "    _input_pad: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # required params\n",
    "        num_layers: int,\n",
    "        num_types: int,\n",
    "        r_max: float,\n",
    "        avg_num_neighbors: Optional[float] = None,\n",
    "        # cutoffs\n",
    "        r_start_cos_ratio: float = 0.8,\n",
    "        PolynomialCutoff_p: float = 6,\n",
    "        per_layer_cutoffs: Optional[List[float]] = None,\n",
    "        cutoff_type: str = \"polynomial\",\n",
    "        # general hyperparameters:\n",
    "        field: str = AtomicDataDict.EDGE_ATTRS_KEY,\n",
    "        edge_invariant_field: str = AtomicDataDict.EDGE_EMBEDDING_KEY,\n",
    "        node_invariant_field: str = AtomicDataDict.NODE_ATTRS_KEY,\n",
    "        env_embed_multiplicity: int = 32,\n",
    "        embed_initial_edge: bool = True,\n",
    "        linear_after_env_embed: bool = False,\n",
    "        nonscalars_include_parity: bool = True,\n",
    "        # MLP parameters:\n",
    "        two_body_latent=ScalarMLPFunction,\n",
    "        two_body_latent_kwargs={},\n",
    "        env_embed=ScalarMLPFunction,\n",
    "        env_embed_kwargs={},\n",
    "        latent=ScalarMLPFunction,\n",
    "        latent_kwargs={},\n",
    "        latent_resnet: bool = True,\n",
    "        latent_resnet_update_ratios: Optional[List[float]] = None,\n",
    "        latent_resnet_update_ratios_learnable: bool = False,\n",
    "        latent_out_field: Optional[str] = _keys.EDGE_FEATURES,\n",
    "        # Performance parameters:\n",
    "        pad_to_alignment: int = 1,\n",
    "        sparse_mode: Optional[str] = None,\n",
    "        # Other:\n",
    "        irreps_in=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        SCALAR = o3.Irrep(\"0e\")  # define for convinience\n",
    "\n",
    "        # save parameters\n",
    "        assert (\n",
    "            num_layers >= 1\n",
    "        )  # zero layers is \"two body\", but we don't need to support that fallback case\n",
    "        self.num_layers = num_layers\n",
    "        self.nonscalars_include_parity = nonscalars_include_parity\n",
    "        self.field = field\n",
    "        self.latent_out_field = latent_out_field\n",
    "        self.edge_invariant_field = edge_invariant_field\n",
    "        self.node_invariant_field = node_invariant_field\n",
    "        self.latent_resnet = latent_resnet\n",
    "        self.env_embed_mul = env_embed_multiplicity\n",
    "        self.r_start_cos_ratio = r_start_cos_ratio\n",
    "        self.polynomial_cutoff_p = float(PolynomialCutoff_p)\n",
    "        self.cutoff_type = cutoff_type\n",
    "        assert cutoff_type in (\"cosine\", \"polynomial\")\n",
    "        self.embed_initial_edge = embed_initial_edge\n",
    "        self.avg_num_neighbors = avg_num_neighbors\n",
    "        self.linear_after_env_embed = linear_after_env_embed\n",
    "        self.num_types = num_types\n",
    "\n",
    "        # set up irreps\n",
    "        self._init_irreps(\n",
    "            irreps_in=irreps_in,\n",
    "            required_irreps_in=[\n",
    "                self.field,\n",
    "                self.edge_invariant_field,\n",
    "                self.node_invariant_field,\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # for normalization of env embed sums\n",
    "        # one per layer\n",
    "        self.register_buffer(\n",
    "            \"env_sum_normalizations\",\n",
    "            # dividing by sqrt(N)\n",
    "            torch.as_tensor([avg_num_neighbors] * num_layers).rsqrt(),\n",
    "        )\n",
    "\n",
    "        latent = functools.partial(latent, **latent_kwargs)\n",
    "        env_embed = functools.partial(env_embed, **env_embed_kwargs)\n",
    "\n",
    "        self.latents = torch.nn.ModuleList([])\n",
    "        self.env_embed_mlps = torch.nn.ModuleList([])\n",
    "        self.tps = torch.nn.ModuleList([])\n",
    "        self.linears = torch.nn.ModuleList([])\n",
    "        self.env_linears = torch.nn.ModuleList([])\n",
    "\n",
    "        # Embed to the spharm * it as mul\n",
    "        input_irreps = self.irreps_in[self.field]\n",
    "        # this is not inherant, but no reason to fix right now:\n",
    "        assert all(mul == 1 for mul, ir in input_irreps)\n",
    "        env_embed_irreps = o3.Irreps([(1, ir) for _, ir in input_irreps])\n",
    "        assert (\n",
    "            env_embed_irreps[0].ir == SCALAR\n",
    "        ), \"env_embed_irreps must start with scalars\"\n",
    "        self._input_pad = (\n",
    "            int(math.ceil(env_embed_irreps.dim / pad_to_alignment)) * pad_to_alignment\n",
    "        ) - env_embed_irreps.dim\n",
    "        self.register_buffer(\"_zero\", torch.zeros(1, 1))\n",
    "\n",
    "        # Initially, we have the B(r)Y(\\vec{r})-projection of the edges\n",
    "        # (possibly embedded)\n",
    "        if self.embed_initial_edge:\n",
    "            arg_irreps = env_embed_irreps\n",
    "        else:\n",
    "            arg_irreps = input_irreps\n",
    "\n",
    "        # - begin irreps -\n",
    "        # start to build up the irreps for the iterated TPs\n",
    "        tps_irreps = [arg_irreps]\n",
    "\n",
    "        for layer_idx in range(num_layers):\n",
    "            # Create higher order terms cause there are more TPs coming\n",
    "            if layer_idx == 0:\n",
    "                # Add parity irreps\n",
    "                ir_out = []\n",
    "                for (mul, ir) in env_embed_irreps:\n",
    "                    if self.nonscalars_include_parity:\n",
    "                        # add both parity options\n",
    "                        ir_out.append((1, (ir.l, 1)))\n",
    "                        ir_out.append((1, (ir.l, -1)))\n",
    "                    else:\n",
    "                        # add only the parity option seen in the inputs\n",
    "                        ir_out.append((1, ir))\n",
    "\n",
    "                ir_out = o3.Irreps(ir_out)\n",
    "\n",
    "            if layer_idx == self.num_layers - 1:\n",
    "                # ^ means we're doing the last layer\n",
    "                # No more TPs follow this, so only need scalars\n",
    "                ir_out = o3.Irreps([(1, (0, 1))])\n",
    "\n",
    "            # Prune impossible paths\n",
    "            ir_out = o3.Irreps(\n",
    "                [\n",
    "                    (mul, ir)\n",
    "                    for mul, ir in ir_out\n",
    "                    if tp_path_exists(arg_irreps, env_embed_irreps, ir)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # the argument to the next tensor product is the output of this one\n",
    "            arg_irreps = ir_out\n",
    "            tps_irreps.append(ir_out)\n",
    "        # - end build irreps -\n",
    "\n",
    "        # == Remove unneeded paths ==\n",
    "        out_irreps = tps_irreps[-1]\n",
    "        new_tps_irreps = [out_irreps]\n",
    "        for arg_irreps in reversed(tps_irreps[:-1]):\n",
    "            new_arg_irreps = []\n",
    "            for mul, arg_ir in arg_irreps:\n",
    "                for _, env_ir in env_embed_irreps:\n",
    "                    if any(i in out_irreps for i in arg_ir * env_ir):\n",
    "                        # arg_ir is useful: arg_ir * env_ir has a path to something we want\n",
    "                        new_arg_irreps.append((mul, arg_ir))\n",
    "                        # once its useful once, we keep it no matter what\n",
    "                        break\n",
    "            new_arg_irreps = o3.Irreps(new_arg_irreps)\n",
    "            new_tps_irreps.append(new_arg_irreps)\n",
    "            out_irreps = new_arg_irreps\n",
    "\n",
    "        assert len(new_tps_irreps) == len(tps_irreps)\n",
    "        tps_irreps = list(reversed(new_tps_irreps))\n",
    "        del new_tps_irreps\n",
    "\n",
    "        assert tps_irreps[-1].lmax == 0\n",
    "\n",
    "        tps_irreps_in = tps_irreps[:-1]\n",
    "        tps_irreps_out = tps_irreps[1:]\n",
    "        del tps_irreps\n",
    "\n",
    "        # Environment builder:\n",
    "        self._env_weighter = MakeWeightedChannels(\n",
    "            irreps_in=input_irreps,\n",
    "            multiplicity_out=env_embed_multiplicity,\n",
    "            pad_to_alignment=pad_to_alignment,\n",
    "        )\n",
    "\n",
    "        self._n_scalar_outs = []\n",
    "\n",
    "        # == Build TPs ==\n",
    "        for layer_idx, (arg_irreps, out_irreps) in enumerate(\n",
    "            zip(tps_irreps_in, tps_irreps_out)\n",
    "        ):\n",
    "            # Make the env embed linear\n",
    "            if self.linear_after_env_embed:\n",
    "                self.env_linears.append(\n",
    "                    Linear(\n",
    "                        [(env_embed_multiplicity, ir) for _, ir in env_embed_irreps],\n",
    "                        [(env_embed_multiplicity, ir) for _, ir in env_embed_irreps],\n",
    "                        shared_weights=True,\n",
    "                        internal_weights=True,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                self.env_linears.append(torch.nn.Identity())\n",
    "            # Make TP\n",
    "            tmp_i_out: int = 0\n",
    "            instr = []\n",
    "            n_scalar_outs: int = 0\n",
    "            full_out_irreps = []\n",
    "            for i_out, (_, ir_out) in enumerate(out_irreps):\n",
    "                for i_1, (_, ir_1) in enumerate(arg_irreps):\n",
    "                    for i_2, (_, ir_2) in enumerate(env_embed_irreps):\n",
    "                        if ir_out in ir_1 * ir_2:\n",
    "                            if ir_out == SCALAR:\n",
    "                                n_scalar_outs += 1\n",
    "                            instr.append((i_1, i_2, tmp_i_out))\n",
    "                            full_out_irreps.append((env_embed_multiplicity, ir_out))\n",
    "                            tmp_i_out += 1\n",
    "            full_out_irreps = o3.Irreps(full_out_irreps)\n",
    "            self._n_scalar_outs.append(n_scalar_outs)\n",
    "            assert all(ir == SCALAR for _, ir in full_out_irreps[:n_scalar_outs])\n",
    "            tp = Contracter(\n",
    "                irreps_in1=o3.Irreps(\n",
    "                    [\n",
    "                        (\n",
    "                            (\n",
    "                                env_embed_multiplicity\n",
    "                                if layer_idx > 0 or self.embed_initial_edge\n",
    "                                else 1\n",
    "                            ),\n",
    "                            ir,\n",
    "                        )\n",
    "                        for _, ir in arg_irreps\n",
    "                    ]\n",
    "                ),\n",
    "                irreps_in2=o3.Irreps(\n",
    "                    [(env_embed_multiplicity, ir) for _, ir in env_embed_irreps]\n",
    "                ),\n",
    "                irreps_out=o3.Irreps(\n",
    "                    [(env_embed_multiplicity, ir) for _, ir in full_out_irreps]\n",
    "                ),\n",
    "                instructions=instr,\n",
    "                # For the first layer, we have the unprocessed edges\n",
    "                # coming in from the input if `not self.embed_initial_edge`.\n",
    "                # These don't match the embedding in mul, so we have\n",
    "                # to use uvv --- since the input edges should be mul\n",
    "                # of one in normal circumstances, this is still plenty fast.\n",
    "                # For this reason it also doesn't increase the number of weights.\n",
    "                connection_mode=(\n",
    "                    \"uuu\" if layer_idx > 0 or self.embed_initial_edge else \"uvv\"\n",
    "                ),\n",
    "                shared_weights=False,\n",
    "                has_weight=False,\n",
    "                pad_to_alignment=pad_to_alignment,\n",
    "                sparse_mode=sparse_mode,\n",
    "            )\n",
    "            self.tps.append(tp)\n",
    "            # we extract the scalars from the first irrep of the tp\n",
    "            assert out_irreps[0].ir == SCALAR\n",
    "\n",
    "            # Make env embed mlp\n",
    "            generate_n_weights = (\n",
    "                self._env_weighter.weight_numel\n",
    "            )  # the weight for the edge embedding\n",
    "            if layer_idx == 0 and self.embed_initial_edge:\n",
    "                # also need weights to embed the edge itself\n",
    "                # this is because the 2 body latent is mixed in with the first layer\n",
    "                # in terms of code\n",
    "                generate_n_weights += self._env_weighter.weight_numel\n",
    "\n",
    "            # the linear acts after the extractor\n",
    "            self.linears.append(\n",
    "                Linear(\n",
    "                    full_out_irreps,\n",
    "                    [(env_embed_multiplicity, ir) for _, ir in out_irreps],\n",
    "                    shared_weights=True,\n",
    "                    internal_weights=True,\n",
    "                    pad_to_alignment=pad_to_alignment,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if layer_idx == 0:\n",
    "                # at the first layer, we have no invariants from previous TPs\n",
    "                self.latents.append(\n",
    "                    two_body_latent(\n",
    "                        mlp_input_dimension=(\n",
    "                            (\n",
    "                                # Node invariants for center and neighbor (chemistry)\n",
    "                                2 * self.irreps_in[self.node_invariant_field].num_irreps\n",
    "                                # Plus edge invariants for the edge (radius).\n",
    "                                + self.irreps_in[self.edge_invariant_field].num_irreps\n",
    "                            )\n",
    "                        ),\n",
    "                        mlp_output_dimension=None,\n",
    "                        **two_body_latent_kwargs,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                self.latents.append(\n",
    "                    latent(\n",
    "                        mlp_input_dimension=(\n",
    "                            (\n",
    "                                # the embedded latent invariants from the previous layer(s)\n",
    "                                self.latents[-1].out_features\n",
    "                                # and the invariants extracted from the last layer's TP:\n",
    "                                + env_embed_multiplicity * n_scalar_outs\n",
    "                            )\n",
    "                        ),\n",
    "                        mlp_output_dimension=None,\n",
    "                    )\n",
    "                )\n",
    "            # the env embed MLP takes the last latent's output as input\n",
    "            # and outputs enough weights for the env embedder\n",
    "            self.env_embed_mlps.append(\n",
    "                env_embed(\n",
    "                    mlp_input_dimension=self.latents[-1].out_features,\n",
    "                    mlp_output_dimension=generate_n_weights,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # For the final layer, we specialize:\n",
    "        # we don't need to propagate nonscalars, so there is no TP\n",
    "        # thus we only need the latent:\n",
    "        self.final_latent = latent(\n",
    "            mlp_input_dimension=self.latents[-1].out_features\n",
    "            + env_embed_multiplicity * n_scalar_outs,\n",
    "            mlp_output_dimension=None,\n",
    "        )\n",
    "        # - end build modules -\n",
    "\n",
    "        # - layer resnet update weights -\n",
    "        if latent_resnet_update_ratios is None:\n",
    "            # We initialize to zeros, which under the sigmoid() become 0.5\n",
    "            # so 1/2 * layer_1 + 1/4 * layer_2 + ...\n",
    "            # note that the sigmoid of these are the factor _between_ layers\n",
    "            # so the first entry is the ratio for the latent resnet of the first and second layers, etc.\n",
    "            # e.g. if there are 3 layers, there are 2 ratios: l1:l2, l2:l3\n",
    "            latent_resnet_update_params = torch.zeros(self.num_layers)\n",
    "        else:\n",
    "            latent_resnet_update_ratios = torch.as_tensor(\n",
    "                latent_resnet_update_ratios, dtype=torch.get_default_dtype()\n",
    "            )\n",
    "            assert latent_resnet_update_ratios.min() > 0.0\n",
    "            assert latent_resnet_update_ratios.min() < 1.0\n",
    "            latent_resnet_update_params = torch.special.logit(\n",
    "                latent_resnet_update_ratios\n",
    "            )\n",
    "            # The sigmoid is mostly saturated at 6, keep it in a reasonable range\n",
    "            latent_resnet_update_params.clamp_(-6.0, 6.0)\n",
    "        assert latent_resnet_update_params.shape == (\n",
    "            num_layers,\n",
    "        ), f\"There must be {num_layers} layer resnet update ratios (layer0:layer1, layer1:layer2)\"\n",
    "        if latent_resnet_update_ratios_learnable:\n",
    "            self._latent_resnet_update_params = torch.nn.Parameter(\n",
    "                latent_resnet_update_params\n",
    "            )\n",
    "        else:\n",
    "            self.register_buffer(\n",
    "                \"_latent_resnet_update_params\", latent_resnet_update_params\n",
    "            )\n",
    "\n",
    "        # - Per-layer cutoffs -\n",
    "        if per_layer_cutoffs is None:\n",
    "            per_layer_cutoffs = torch.full((num_layers + 1,), r_max)\n",
    "        self.register_buffer(\"per_layer_cutoffs\", torch.as_tensor(per_layer_cutoffs))\n",
    "        assert torch.all(self.per_layer_cutoffs <= r_max)\n",
    "        assert self.per_layer_cutoffs.shape == (\n",
    "            num_layers + 1,\n",
    "        ), \"Must be one per-layer cutoff for layer 0 and every layer for a total of {num_layers} cutoffs (the first applies to the two body latent, which is 'layer 0')\"\n",
    "        assert (\n",
    "            self.per_layer_cutoffs[1:] <= self.per_layer_cutoffs[:-1]\n",
    "        ).all(), \"Per-layer cutoffs must be equal or decreasing\"\n",
    "        assert (\n",
    "            self.per_layer_cutoffs.min() > 0\n",
    "        ), \"Per-layer cutoffs must be >0. To remove higher layers entirely, lower `num_layers`.\"\n",
    "        self._latent_dim = self.final_latent.out_features\n",
    "        self.register_buffer(\"_zero\", torch.as_tensor(0.0))\n",
    "\n",
    "        self.irreps_out.update(\n",
    "            {\n",
    "                self.latent_out_field: o3.Irreps(\n",
    "                    [(self.final_latent.out_features, (0, 1))]\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def forward(self, data: AtomicDataDict.Type) -> AtomicDataDict.Type:\n",
    "        \"\"\"Evaluate.\n",
    "\n",
    "        :param data: AtomicDataDict.Type\n",
    "        :return: AtomicDataDict.Type\n",
    "        \"\"\"\n",
    "        edge_center = data[AtomicDataDict.EDGE_INDEX_KEY][0]\n",
    "        edge_neighbor = data[AtomicDataDict.EDGE_INDEX_KEY][1]\n",
    "\n",
    "        edge_attr = data[self.field]\n",
    "        # pad edge_attr\n",
    "        if self._input_pad > 0:\n",
    "            edge_attr = torch.cat(\n",
    "                (\n",
    "                    edge_attr,\n",
    "                    self._zero.expand(len(edge_attr), self._input_pad),\n",
    "                ),\n",
    "                dim=-1,\n",
    "            )\n",
    "\n",
    "        edge_length = data[AtomicDataDict.EDGE_LENGTH_KEY]\n",
    "        num_edges: int = len(edge_attr)\n",
    "        edge_invariants = data[self.edge_invariant_field]\n",
    "        node_invariants = data[self.node_invariant_field]\n",
    "        # pre-declare variables as Tensors for TorchScript\n",
    "        scalars = self._zero\n",
    "        coefficient_old = scalars\n",
    "        coefficient_new = scalars\n",
    "        # Initialize state\n",
    "        latents = torch.zeros(\n",
    "            (num_edges, self._latent_dim),\n",
    "            dtype=edge_attr.dtype,\n",
    "            device=edge_attr.device,\n",
    "        )\n",
    "        active_edges = torch.arange(\n",
    "            num_edges,\n",
    "            device=edge_attr.device,\n",
    "        )\n",
    "\n",
    "        # For the first layer, we use the input invariants:\n",
    "        # The center and neighbor invariants and edge invariants\n",
    "        latent_inputs_to_cat = [\n",
    "            node_invariants[edge_center],\n",
    "            node_invariants[edge_neighbor],\n",
    "            edge_invariants,\n",
    "        ]\n",
    "        # The nonscalar features. Initially, the edge data.\n",
    "        features = edge_attr\n",
    "\n",
    "        layer_index: int = 0\n",
    "        # compute the sigmoids vectorized instead of each loop\n",
    "        layer_update_coefficients = self._latent_resnet_update_params.sigmoid()\n",
    "\n",
    "        # Vectorized precompute per layer cutoffs\n",
    "        if self.cutoff_type == \"cosine\":\n",
    "            cutoff_coeffs_all = cosine_cutoff(\n",
    "                edge_length,\n",
    "                self.per_layer_cutoffs,\n",
    "                r_start_cos_ratio=self.r_start_cos_ratio,\n",
    "            )\n",
    "        elif self.cutoff_type == \"polynomial\":\n",
    "            cutoff_coeffs_all = polynomial_cutoff(\n",
    "                edge_length, self.per_layer_cutoffs, p=self.polynomial_cutoff_p\n",
    "            )\n",
    "        else:\n",
    "            # This branch is unreachable (cutoff type is checked in __init__)\n",
    "            # But TorchScript doesn't know that, so we need to make it explicitly\n",
    "            # impossible to make it past so it doesn't throw\n",
    "            # \"cutoff_coeffs_all is not defined in the false branch\"\n",
    "            assert False, \"Invalid cutoff type\"\n",
    "\n",
    "        # !!!! REMEMBER !!!! update final layer if update the code in main loop!!!\n",
    "        # This goes through layer0, layer1, ..., layer_max-1\n",
    "        for latent, env_embed_mlp, env_linear, tp, linear in zip(\n",
    "            self.latents, self.env_embed_mlps, self.env_linears, self.tps, self.linears\n",
    "        ):\n",
    "            # Determine which edges are still in play\n",
    "            cutoff_coeffs = cutoff_coeffs_all[layer_index]\n",
    "            prev_mask = cutoff_coeffs[active_edges] > 0\n",
    "            active_edges = (cutoff_coeffs > 0).nonzero().squeeze(-1)\n",
    "\n",
    "            # Compute latents\n",
    "            new_latents = latent(torch.cat(latent_inputs_to_cat, dim=-1)[prev_mask])\n",
    "            # Apply cutoff, which propagates through to everything else\n",
    "            new_latents = cutoff_coeffs[active_edges].unsqueeze(-1) * new_latents\n",
    "\n",
    "            if self.latent_resnet and layer_index > 0:\n",
    "                this_layer_update_coeff = layer_update_coefficients[layer_index - 1]\n",
    "                # At init, we assume new and old to be approximately uncorrelated\n",
    "                # Thus their variances add\n",
    "                # we always want the latent space to be normalized to variance = 1.0,\n",
    "                # because it is critical for learnability. Still, we want to preserve\n",
    "                # the _relative_ magnitudes of the current latent and the residual update\n",
    "                # to be controled by `this_layer_update_coeff`\n",
    "                # Solving the simple system for the two coefficients:\n",
    "                #   a^2 + b^2 = 1  (variances add)   &    a * this_layer_update_coeff = b\n",
    "                # gives\n",
    "                #   a = 1 / sqrt(1 + this_layer_update_coeff^2)  &  b = this_layer_update_coeff / sqrt(1 + this_layer_update_coeff^2)\n",
    "                # rsqrt is reciprocal sqrt\n",
    "                coefficient_old = torch.rsqrt(this_layer_update_coeff.square() + 1)\n",
    "                coefficient_new = this_layer_update_coeff * coefficient_old\n",
    "                # Residual update\n",
    "                # Note that it only runs when there are latents to resnet with, so not at the first layer\n",
    "                # index_add adds only to the edges for which we have something to contribute\n",
    "                latents = torch.index_add(\n",
    "                    coefficient_old * latents,\n",
    "                    0,\n",
    "                    active_edges,\n",
    "                    coefficient_new * new_latents,\n",
    "                )\n",
    "            else:\n",
    "                # Normal (non-residual) update\n",
    "                # index_copy replaces, unlike index_add\n",
    "                latents = torch.index_copy(latents, 0, active_edges, new_latents)\n",
    "\n",
    "            # From the latents, compute the weights for active edges:\n",
    "            weights = env_embed_mlp(latents[active_edges])\n",
    "            w_index: int = 0\n",
    "\n",
    "            if self.embed_initial_edge and layer_index == 0:\n",
    "                # embed initial edge\n",
    "                env_w = weights.narrow(-1, w_index, self._env_weighter.weight_numel)\n",
    "                w_index += self._env_weighter.weight_numel\n",
    "                features = self._env_weighter(\n",
    "                    features[prev_mask], env_w\n",
    "                )  # features is edge_attr\n",
    "            else:\n",
    "                # just take the previous features that we still need\n",
    "                features = features[prev_mask]\n",
    "\n",
    "            # Extract weights for the environment builder\n",
    "            env_w = weights.narrow(-1, w_index, self._env_weighter.weight_numel)\n",
    "            w_index += self._env_weighter.weight_numel\n",
    "\n",
    "            # Build the local environments\n",
    "            # This local environment should only be a sum over neighbors\n",
    "            # who are within the cutoff of the _current_ layer\n",
    "            # Those are the active edges, which are the only ones we\n",
    "            # have weights for (env_w) anyway.\n",
    "            # So we mask out the edges in the sum:\n",
    "            local_env_per_edge = scatter(\n",
    "                self._env_weighter(edge_attr[active_edges], env_w),\n",
    "                edge_center[active_edges],\n",
    "                dim=0,\n",
    "            )\n",
    "            if self.env_sum_normalizations.ndim < 2:\n",
    "                # it's a scalar per layer\n",
    "                norm_const = self.env_sum_normalizations[layer_index]\n",
    "            else:\n",
    "                # it's per type\n",
    "                # get shape [N_atom, 1] for broadcasting\n",
    "                norm_const = self.env_sum_normalizations[\n",
    "                    layer_index, data[AtomicDataDict.ATOM_TYPE_KEY]\n",
    "                ].unsqueeze(-1)\n",
    "            local_env_per_edge = local_env_per_edge * norm_const\n",
    "            local_env_per_edge = env_linear(local_env_per_edge)\n",
    "            # Copy to get per-edge\n",
    "            # Large allocation, but no better way to do this:\n",
    "            local_env_per_edge = local_env_per_edge[edge_center[active_edges]]\n",
    "\n",
    "            # Now do the TP\n",
    "            # recursively tp current features with the environment embeddings\n",
    "            features = tp(features, local_env_per_edge)\n",
    "\n",
    "            # Get invariants\n",
    "            # features has shape [z][mul][k]\n",
    "            # we know scalars are first\n",
    "            scalars = features[:, :, : self._n_scalar_outs[layer_index]].reshape(\n",
    "                features.shape[0], -1\n",
    "            )\n",
    "\n",
    "            # do the linear\n",
    "            features = linear(features)\n",
    "\n",
    "            # For layer2+, use the previous latents and scalars\n",
    "            # This makes it deep\n",
    "            latent_inputs_to_cat = [\n",
    "                latents[active_edges],\n",
    "                scalars,\n",
    "            ]\n",
    "\n",
    "            # increment co_unter\n",
    "            layer_index += 1\n",
    "\n",
    "        # - final layer -\n",
    "        # due to TorchScript limitations, we have to\n",
    "        # copy and repeat the code here --- no way to\n",
    "        # escape the final iteration of the loop early\n",
    "        cutoff_coeffs = cutoff_coeffs_all[layer_index]\n",
    "        prev_mask = cutoff_coeffs[active_edges] > 0\n",
    "        active_edges = (cutoff_coeffs > 0).nonzero().squeeze(-1)\n",
    "        new_latents = self.final_latent(\n",
    "            torch.cat(latent_inputs_to_cat, dim=-1)[prev_mask]\n",
    "        )\n",
    "        new_latents = cutoff_coeffs[active_edges].unsqueeze(-1) * new_latents\n",
    "        if self.latent_resnet:\n",
    "            this_layer_update_coeff = layer_update_coefficients[layer_index - 1]\n",
    "            coefficient_old = torch.rsqrt(this_layer_update_coeff.square() + 1)\n",
    "            coefficient_new = this_layer_update_coeff * coefficient_old\n",
    "            latents = torch.index_add(\n",
    "                coefficient_old * latents,\n",
    "                0,\n",
    "                active_edges,\n",
    "                coefficient_new * new_latents,\n",
    "            )\n",
    "        else:\n",
    "            latents = torch.index_copy(latents, 0, active_edges, new_latents)\n",
    "        # - end final layer -\n",
    "\n",
    "        # final latents\n",
    "        data[self.latent_out_field] = latents\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bc0c3a",
   "metadata": {},
   "source": [
    "### Edge embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "53f2d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "from nequip.data import AtomicData, AtomicDataDict\n",
    "from torch.nn.functional import one_hot\n",
    "from e3nn.nn import FullyConnectedNet\n",
    "    \n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "irreps_in = {}\n",
    "\n",
    "data = AtomicData.to_AtomicDataDict(dataset[0])\n",
    "\n",
    "\n",
    "\n",
    "# edge length embedding\n",
    "torch.manual_seed(32)\n",
    "\n",
    "num_basis = 8\n",
    "r_max = 5\n",
    "\n",
    "\n",
    "data_in = {key: torch.clone(data[key]) for key in data}\n",
    "data_in = AtomicDataDict.with_edge_vectors(data_in, with_lengths=True)\n",
    "\n",
    "edge_length = data_my['edge_lengths']\n",
    "\n",
    "bessel_weights = (torch.linspace(start=1.0, end=num_basis, steps=num_basis) * math.pi)\n",
    "bessel_weights = nn.Parameter(bessel_weights)\n",
    "\n",
    "edge_length_embedding = 2/r_max*torch.sin(bessel_weights * edge_length.unsqueeze(-1) / r_max)/edge_length.unsqueeze(-1)\n",
    "\n",
    "# cutoff\n",
    "factor = 1/r_max\n",
    "p = 6\n",
    "    \n",
    "x = edge_length * factor\n",
    "\n",
    "cutoff = 1.0\n",
    "cutoff = cutoff - (((p + 1.0) * (p + 2.0) / 2.0) * torch.pow(x, p))\n",
    "cutoff = cutoff + (p * (p + 2.0) * torch.pow(x, p + 1.0))\n",
    "cutoff = cutoff - ((p * (p + 1.0) / 2) * torch.pow(x, p + 2.0))\n",
    "cutoff *= (x < 1.0)\n",
    "\n",
    "cutoff = cutoff.unsqueeze(-1)\n",
    "\n",
    "data_in['edge_embedding'] = edge_length_embedding * cutoff\n",
    "\n",
    "irreps_in['edge_embedding'] = o3.Irreps(f'{num_basis}x0e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "853efb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([364, 8])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in['edge_embedding'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b2f5e7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8x0e+2x0e"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irreps_in['edge_embedding'] + '2x0e'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dabdca",
   "metadata": {},
   "source": [
    "### Node attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7fecd372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "from nequip.data import AtomicData, AtomicDataDict\n",
    "from torch.nn.functional import one_hot\n",
    "from e3nn.nn import FullyConnectedNet\n",
    "    \n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "# types embedding\n",
    "num_classes = 3\n",
    "\n",
    "\n",
    "types_embed = one_hot(data_in['atom_types'], num_classes).squeeze(1)\n",
    "\n",
    "data_in['node_attrs'] = types_embed\n",
    "\n",
    "irreps_in['node_attrs'] = o3.Irreps(f'{num_classes}x0e')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3ade01",
   "metadata": {},
   "source": [
    "### Edge attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "95f6c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "\n",
    "\n",
    "torch.manual_seed(32)\n",
    "\n",
    "\n",
    "l_max = 2\n",
    "irreps_edge_sh = o3.Irreps.spherical_harmonics(2)\n",
    "\n",
    "#data_in = {key: torch.clone(data_in[key]) for key in data_my}\n",
    "data_in = AtomicDataDict.with_edge_vectors(data_in, with_lengths=False)\n",
    "\n",
    "\n",
    "harm_gen = o3.SphericalHarmonics(irreps_edge_sh, True, 'component')\n",
    "\n",
    "edge_vec = data_in['edge_vectors']\n",
    "\n",
    "harm_edge = harm_gen(edge_vec)\n",
    "harm_edge.shape\n",
    "\n",
    "\n",
    "data_in['edge_attrs'] = harm_edge\n",
    "\n",
    "irreps_in['edge_attrs'] = irreps_edge_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7be743d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allegro.nn import Allegro_Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "05698ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Allegro_Module(\n",
       "  (latents): ModuleList(\n",
       "    (0-1): 2 x ScalarMLPFunction(\n",
       "      (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
       "    )\n",
       "  )\n",
       "  (env_embed_mlps): ModuleList(\n",
       "    (0-1): 2 x ScalarMLPFunction(\n",
       "      (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
       "    )\n",
       "  )\n",
       "  (tps): ModuleList(\n",
       "    (0-1): 2 x RecursiveScriptModule(original_name=GraphModule)\n",
       "  )\n",
       "  (linears): ModuleList(\n",
       "    (0-1): 2 x RecursiveScriptModule(original_name=GraphModule)\n",
       "  )\n",
       "  (env_linears): ModuleList(\n",
       "    (0-1): 2 x Identity()\n",
       "  )\n",
       "  (_env_weighter): MakeWeightedChannels()\n",
       "  (final_latent): ScalarMLPFunction(\n",
       "    (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#self, # required params\n",
    "# num_layers: int, num_types: int,\n",
    "# r_max: float, avg_num_neighbors: Optional[float] = None,\n",
    "# cutoffs\n",
    "# r_start_cos_ratio: float = 0.8, PolynomialCutoff_p: float = 6,\n",
    "# per_layer_cutoffs: Optional[List[float]] = None, cutoff_type: str = \"polynomial\",\n",
    "# general hyperparameters:\n",
    "# field: str = AtomicDataDict.EDGE_ATTRS_KEY,\n",
    "# edge_invariant_field: str = AtomicDataDict.EDGE_EMBEDDING_KEY,\n",
    "# node_invariant_field: str = AtomicDataDict.NODE_ATTRS_KEY,\n",
    "# env_embed_multiplicity: int = 32,\n",
    "# embed_initial_edge: bool = True,\n",
    "# linear_after_env_embed: bool = False,\n",
    "# nonscalars_include_parity: bool = True,\n",
    "# MLP parameters:\n",
    "# two_body_latent=ScalarMLPFunction,\n",
    "# two_body_latent_kwargs={},\n",
    "# env_embed=ScalarMLPFunction,\n",
    "# env_embed_kwargs={},\n",
    "# latent=ScalarMLPFunction,\n",
    "# latent_kwargs={}, latent_resnet: bool = True,\n",
    "# latent_resnet_update_ratios: Optional[List[float]] = None,\n",
    "# latent_resnet_update_ratios_learnable: bool = False,\n",
    "# latent_out_field: Optional[str] = _keys.EDGE_FEATURES,\n",
    "# Performance parameters:\n",
    "# pad_to_alignment: int = 1,\n",
    "# sparse_mode: Optional[str] = None,\n",
    "# Other:\n",
    "# irreps_in=None,\n",
    "\n",
    "Main_Module = Allegro_Module(r_max = 5,\n",
    "               num_layers = 2,\n",
    "               num_types = 3,\n",
    "               avg_num_neighbors=10., \n",
    "               two_body_latent_kwargs={'mlp_latent_dimensions': [32]},\n",
    "               latent_kwargs={'mlp_latent_dimensions': [32]},\n",
    "               env_embed_kwargs={'mlp_latent_dimensions': [32]},\n",
    "               irreps_in=irreps_in)\n",
    "\n",
    "Main_Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cf7454aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "Main_Module(data_in)\n",
    "\n",
    "print('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "0d214f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing(GraphModuleMixin, torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, irreps_in = None):\n",
    "        super().__init__()\n",
    "        self._init_irreps(\n",
    "            irreps_in=irreps_in,\n",
    "            irreps_out = {'edge_embedding': '8x0e', 'node_attrs': '3x0e', 'edge_attrs': '1x0e+1x1o+1x2e'}\n",
    "        )\n",
    "        \n",
    "        #self.irreps_in = irreps_in\n",
    "        #self.irreps_out = {'edge_embedding': '8x0e', 'node_attrs': '3x0e', 'edge_attrs': '1x0e+1x1o+1x2e'}\n",
    "    \n",
    "    def forward(self, data_in):\n",
    "        \n",
    "        \n",
    "        \n",
    "        import torch\n",
    "        from torch.nn.functional import one_hot\n",
    "        from nequip.data import AtomicData, AtomicDataDict\n",
    "        from torch.nn.functional import one_hot\n",
    "        from e3nn.nn import FullyConnectedNet\n",
    "\n",
    "        from torch import nn\n",
    "        import math\n",
    "\n",
    "        irreps_in = {}\n",
    "\n",
    "\n",
    "\n",
    "        # edge length embedding\n",
    "        torch.manual_seed(32)\n",
    "\n",
    "        num_basis = 8\n",
    "        r_max = 5\n",
    "\n",
    "\n",
    "        #data_in = {key: torch.clone(data_in[key]) for key in data}\n",
    "        data_in = AtomicDataDict.with_edge_vectors(data_in, with_lengths=True)\n",
    "\n",
    "        edge_length = data_in['edge_lengths']\n",
    "\n",
    "        bessel_weights = (torch.linspace(start=1.0, end=num_basis, steps=num_basis) * math.pi)\n",
    "        bessel_weights = nn.Parameter(bessel_weights)\n",
    "\n",
    "        edge_length_embedding = 2/r_max*torch.sin(bessel_weights * edge_length.unsqueeze(-1) / r_max)/edge_length.unsqueeze(-1)\n",
    "\n",
    "        # cutoff\n",
    "        factor = 1/r_max\n",
    "        p = 6\n",
    "\n",
    "        x = edge_length * factor\n",
    "\n",
    "        cutoff = 1.0\n",
    "        cutoff = cutoff - (((p + 1.0) * (p + 2.0) / 2.0) * torch.pow(x, p))\n",
    "        cutoff = cutoff + (p * (p + 2.0) * torch.pow(x, p + 1.0))\n",
    "        cutoff = cutoff - ((p * (p + 1.0) / 2) * torch.pow(x, p + 2.0))\n",
    "        cutoff *= (x < 1.0)\n",
    "\n",
    "        cutoff = cutoff.unsqueeze(-1)\n",
    "\n",
    "        data_in['edge_embedding'] = edge_length_embedding * cutoff\n",
    "\n",
    "        irreps_in['edge_embedding'] = o3.Irreps(f'{num_basis}x0e')\n",
    "        \n",
    "        import torch\n",
    "        from torch.nn.functional import one_hot\n",
    "        from nequip.data import AtomicData, AtomicDataDict\n",
    "        from torch.nn.functional import one_hot\n",
    "        from e3nn.nn import FullyConnectedNet\n",
    "\n",
    "        from torch import nn\n",
    "        import math\n",
    "\n",
    "        # types embedding\n",
    "        num_classes = 3\n",
    "\n",
    "\n",
    "        types_embed = one_hot(data_in['atom_types'], num_classes).squeeze(1)\n",
    "\n",
    "        data_in['node_attrs'] = types_embed\n",
    "\n",
    "        irreps_in['node_attrs'] = o3.Irreps(f'{num_classes}x0e')\n",
    "        \n",
    "        # Edge attrs\n",
    "        from torch import nn\n",
    "        import math\n",
    "\n",
    "\n",
    "        torch.manual_seed(32)\n",
    "\n",
    "\n",
    "        l_max = 2\n",
    "        irreps_edge_sh = o3.Irreps.spherical_harmonics(2)\n",
    "\n",
    "        #data_in = {key: torch.clone(data_in[key]) for key in data_my}\n",
    "        data_in = AtomicDataDict.with_edge_vectors(data_in, with_lengths=False)\n",
    "\n",
    "\n",
    "        harm_gen = o3.SphericalHarmonics(irreps_edge_sh, True, 'component')\n",
    "\n",
    "        edge_vec = data_in['edge_vectors']\n",
    "\n",
    "        harm_edge = harm_gen(edge_vec)\n",
    "        harm_edge.shape\n",
    "\n",
    "\n",
    "        data_in['edge_attrs'] = harm_edge\n",
    "\n",
    "        irreps_in['edge_attrs'] = irreps_edge_sh\n",
    "        \n",
    "        return data_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "52c03ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([364, 576]) torch.Size([364, 288])\n",
      "torch.Size([364, 576]) torch.Size([364, 288])\n",
      "torch.Size([364, 576]) torch.Size([364, 288])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'edge_index': tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "           1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "           4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,  5,\n",
       "           5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "           6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "           6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "           7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "           8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "           9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "          10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "          11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "          12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
       "          13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "          14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "          15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "          16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18,\n",
       "          18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "          20, 20, 20, 20],\n",
       "         [ 9,  1,  2,  3,  4,  5,  6,  7,  8, 20, 11, 12, 13, 14, 15, 16, 17, 10,\n",
       "           7,  6,  5,  3,  2, 10,  4, 19, 15, 12, 13, 14, 16, 17, 18,  0,  9, 11,\n",
       "          20,  8,  8,  9,  1,  3,  5,  6, 17, 10, 11, 12, 13, 14,  7, 15,  0, 16,\n",
       "          16, 15, 14, 13, 12, 11,  9,  7,  6,  5,  4,  2,  1,  8, 10, 18, 17,  0,\n",
       "           0,  9,  1,  5,  6,  7,  8, 10, 11, 12, 13, 15, 18, 19, 20,  3,  0,  9,\n",
       "           1,  2,  3,  4,  6,  7,  8, 10, 19, 11, 12, 13, 14, 15, 16, 20, 17, 18,\n",
       "           1,  0, 18, 17, 16, 15, 13, 12, 11, 19, 10,  8,  7,  5,  4,  3,  2,  9,\n",
       "          20, 14,  0,  9,  1,  2,  3,  4,  5,  6, 20,  8, 11, 12, 13, 14, 15, 16,\n",
       "          17, 10,  9, 17,  1,  2,  3,  4,  5,  6,  7, 10, 19, 11, 12, 13, 14, 20,\n",
       "          15, 18,  0, 18, 20, 16, 15, 14,  0, 12, 11, 19, 10, 13,  7,  6,  5,  4,\n",
       "           3,  2,  1,  8,  6,  5,  4,  3, 20,  1,  9,  7,  2,  8, 19, 18, 17, 16,\n",
       "          15,  0, 13, 12, 11, 14,  8,  1,  2,  3,  4,  5,  6,  7, 10, 15, 12, 13,\n",
       "           9, 17, 18,  0, 19, 20, 14,  8, 20,  9,  1,  2,  3,  4,  5, 18,  6, 17,\n",
       "          10, 19, 11, 13, 14,  7, 15,  0, 16, 14, 12, 11, 19, 20,  9,  7,  6,  5,\n",
       "           4,  3,  2,  1,  8, 10, 18,  0,  0,  9,  1,  2,  3,  5,  6,  7, 10, 11,\n",
       "          12, 13, 15, 16, 17,  8,  0,  9,  1,  2,  3,  4,  5,  6,  7,  8, 10, 19,\n",
       "          11, 12, 14, 16, 20, 17, 18,  1, 17, 15, 14, 12, 10,  7,  6,  5,  3,  2,\n",
       "           9,  0,  0,  1,  2,  3,  5,  6,  7,  8, 11, 12, 14, 15, 16, 10,  9,  1,\n",
       "           3,  4,  5,  6,  8, 10, 19, 11, 12, 13, 15, 20, 18, 20, 15, 13, 11, 10,\n",
       "          12,  9,  8,  1,  4,  5,  6,  8,  4,  5,  6,  7,  1, 10,  0, 11, 12, 13,\n",
       "          15, 18,  9, 19]]),\n",
       " 'pos': tensor([[ 2.1345, -0.9844, -0.1952],\n",
       "         [ 0.7626,  0.9594, -1.6799],\n",
       "         [ 2.6603, -0.4079, -1.3073],\n",
       "         [ 1.9103,  0.3940, -2.1470],\n",
       "         [-3.0302,  1.4954,  0.7197],\n",
       "         [ 0.8494, -0.5509,  0.2844],\n",
       "         [ 0.2384,  0.4735, -0.4044],\n",
       "         [ 0.8979, -2.2764,  1.7301],\n",
       "         [-2.3835,  0.4178, -1.4629],\n",
       "         [-0.4762, -0.5291,  2.3393],\n",
       "         [ 0.3930, -1.1902,  1.5380],\n",
       "         [-2.1230,  0.9518, -0.3977],\n",
       "         [-0.8047,  1.2862,  0.1105],\n",
       "         [-0.4938, -1.1868,  3.0960],\n",
       "         [ 2.5547, -1.8025,  0.3921],\n",
       "         [ 0.3307,  1.8557, -2.3453],\n",
       "         [ 3.8038, -0.4937, -1.4562],\n",
       "         [ 2.2311,  0.5572, -3.1241],\n",
       "         [-2.7089,  2.4847,  0.9269],\n",
       "         [-4.1305,  1.4822,  0.4313],\n",
       "         [-2.8741,  1.0032,  1.6995]]),\n",
       " 'forces': tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " 'edge_cell_shift': tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " 'pbc': tensor([False, False, False]),\n",
       " 'total_energy': tensor([-405681.5938]),\n",
       " 'cell': tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " 'atom_types': tensor([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [1],\n",
       "         [1],\n",
       "         [2],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]),\n",
       " 'edge_vectors': tensor([[-2.6107,  0.4553,  2.5345],\n",
       "         [-1.3718,  1.9438, -1.4847],\n",
       "         [ 0.5259,  0.5764, -1.1121],\n",
       "         ...,\n",
       "         [ 0.1652,  1.4814, -0.7726],\n",
       "         [ 2.3979, -1.5323,  0.6398],\n",
       "         [-1.2563,  0.4790, -1.2682]]),\n",
       " 'edge_lengths': tensor([3.6670, 2.8044, 1.3585, 2.3999, 5.8017, 1.4385, 2.4009, 2.6278, 4.8974,\n",
       "         5.7120, 4.6814, 3.7266, 4.2167, 1.0913, 3.9928, 2.1488, 3.3112, 2.4656,\n",
       "         4.7029, 1.4621, 2.4793, 1.3620, 2.3685, 3.8875, 4.5201, 5.3547, 1.1969,\n",
       "         2.4019, 5.3846, 3.8901, 3.3779, 2.0986, 4.6015, 2.8044, 4.4614, 3.1577,\n",
       "         4.9647, 3.1998, 5.1133, 4.8114, 2.3685, 1.3823, 2.4152, 2.7309, 2.1016,\n",
       "         3.7214, 5.0553, 4.1093, 5.4721, 2.2009, 3.9778, 3.4101, 1.3585, 1.1563,\n",
       "         2.2024, 2.1613, 3.4186, 5.9806, 3.6419, 4.4316, 5.1647, 4.8154, 2.4162,\n",
       "         2.8160, 5.8172, 1.3823, 1.3620, 4.3480, 4.2885, 5.9294, 1.0413, 2.3999,\n",
       "         5.8017, 3.6393, 4.5201, 4.4077, 3.6044, 5.5387, 2.5185, 4.4272, 1.5385,\n",
       "         2.3168, 4.3903, 4.5628, 1.0606, 1.1375, 1.1075, 5.8172, 1.4385, 2.4455,\n",
       "         2.4793, 2.4152, 2.8160, 4.4077, 1.3773, 2.2516, 3.8003, 1.4794, 5.3809,\n",
       "         3.3998, 2.4782, 3.1802, 2.1181, 3.6022, 3.4295, 4.2758, 3.8412, 4.7211,\n",
       "         1.4621, 2.4009, 3.8084, 3.3726, 3.8410, 2.3845, 3.9428, 1.4191, 2.4094,\n",
       "         4.5611, 2.5622, 2.8280, 3.5430, 1.3773, 3.6044, 2.4162, 2.7309, 3.0073,\n",
       "         3.7941, 3.3436, 2.6278, 2.3049, 4.7029, 3.9778, 4.8154, 5.5387, 2.2516,\n",
       "         3.5430, 4.9985, 5.3123, 4.9066, 4.2678, 2.2338, 2.1817, 5.8313, 4.6663,\n",
       "         5.7767, 1.2131, 4.3578, 4.9065, 3.1998, 5.1133, 4.3480, 2.5185, 3.8003,\n",
       "         2.8280, 5.3123, 4.3931, 2.7880, 1.2196, 2.3921, 5.1893, 5.7233, 3.2533,\n",
       "         3.1958, 3.1763, 4.8974, 4.0078, 2.9167, 5.7206, 5.3182, 3.8209, 3.6670,\n",
       "         2.8932, 3.5208, 4.5869, 1.3545, 1.0027, 2.3049, 3.0073, 2.4455, 3.6393,\n",
       "         5.1647, 4.8114, 4.4614, 4.3578, 2.5622, 1.4794, 4.4272, 4.2885, 3.9385,\n",
       "         3.8875, 1.3545, 1.2131, 3.7214, 4.3931, 5.3692, 4.8477, 5.3073, 4.5917,\n",
       "         4.9357, 2.4656, 1.7927, 3.0992, 3.8295, 2.5221, 1.2196, 3.1577, 5.0553,\n",
       "         4.4316, 1.5385, 3.3998, 2.4094, 4.9066, 3.8295, 3.2605, 1.4519, 4.4083,\n",
       "         3.5208, 5.1524, 2.1090, 4.6814, 2.2357, 2.2283, 5.4855, 2.3921, 2.6244,\n",
       "         2.8932, 2.4019, 4.1093, 3.6419, 2.3168, 2.4782, 2.3935, 1.4191, 4.4956,\n",
       "         3.0992, 3.3470, 1.4519, 3.8892, 4.5722, 4.2678, 2.7648, 3.7266, 5.1827,\n",
       "         4.1211, 3.8892, 4.4083, 5.2392, 3.5231, 1.0027, 2.2338, 3.9428, 3.1802,\n",
       "         4.3903, 5.9806, 5.4721, 5.3846, 5.1893, 1.7927, 4.8053, 4.2167, 1.0913,\n",
       "         3.8209, 3.8901, 2.2009, 3.4186, 2.1181, 3.3436, 2.1817, 2.5221, 5.4855,\n",
       "         4.5722, 4.1211, 5.0816, 2.5864, 4.2470, 5.7233, 3.9928, 5.3182, 1.1969,\n",
       "         3.4101, 2.1613, 4.5628, 3.6022, 2.3845, 5.8313, 3.1958, 4.9357, 5.2679,\n",
       "         3.2605, 2.7648, 5.0816, 4.2863, 5.2305, 2.4299, 4.5102, 3.3779, 2.5218,\n",
       "         4.2863, 2.5864, 5.1827, 4.5917, 4.6663, 3.8410, 3.4295, 2.2024, 1.1563,\n",
       "         5.7206, 2.1488, 3.3112, 2.0986, 2.1016, 1.0413, 3.8412, 3.3726, 5.7767,\n",
       "         4.9065, 5.1524, 4.4956, 4.2470, 2.4299, 2.5218, 5.3073, 4.0078, 4.6015,\n",
       "         5.9294, 1.0606, 4.7211, 3.8084, 3.1763, 4.8477, 1.8087, 2.1090, 2.3935,\n",
       "         4.8053, 4.5102, 1.6789, 1.8087, 1.8483, 5.2679, 5.2392, 2.2357, 5.3692,\n",
       "         3.3470, 4.5869, 2.7880, 5.3547, 1.1375, 5.3809, 4.5611, 3.2533, 1.1075,\n",
       "         4.2758, 3.7941, 4.9985, 4.9647, 3.9385, 5.7120, 2.2283, 2.6244, 3.5231,\n",
       "         5.2305, 1.6789, 2.9167, 1.8483]),\n",
       " 'edge_embedding': tensor([[ 0.0294, -0.0394,  0.0233,  ...,  0.0377, -0.0162, -0.0161],\n",
       "         [ 0.1065, -0.0405, -0.0911,  ..., -0.0989, -0.0250,  0.1084],\n",
       "         [ 0.2204,  0.2898,  0.1605,  ..., -0.2684, -0.0887,  0.1517],\n",
       "         ...,\n",
       "         [ 0.2030,  0.2003, -0.0054,  ...,  0.0108,  0.2081,  0.1945],\n",
       "         [ 0.0952, -0.0493, -0.0697,  ..., -0.0985,  0.0255,  0.0853],\n",
       "         [ 0.1919,  0.1528, -0.0702,  ...,  0.1323,  0.2013,  0.0280]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " 'node_attrs': tensor([[0, 1, 0],\n",
       "         [0, 1, 0],\n",
       "         [0, 1, 0],\n",
       "         [0, 1, 0],\n",
       "         [0, 1, 0],\n",
       "         [0, 1, 0],\n",
       "         [0, 1, 0],\n",
       "         [0, 0, 1],\n",
       "         [0, 0, 1],\n",
       "         [0, 0, 1],\n",
       "         [0, 1, 0],\n",
       "         [0, 1, 0],\n",
       "         [0, 0, 1],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0]]),\n",
       " 'edge_attrs': tensor([[ 1.0000, -1.2331,  0.2150,  ..., -1.0663,  0.3324, -0.0565],\n",
       "         [ 1.0000, -0.8473,  1.2005,  ...,  0.4933, -1.4212,  0.0794],\n",
       "         [ 1.0000,  0.6704,  0.7349,  ..., -0.5141, -1.3453,  1.0075],\n",
       "         ...,\n",
       "         [ 1.0000,  0.1704,  1.5283,  ...,  1.4934, -1.5725,  0.3913],\n",
       "         [ 1.0000,  1.4240, -0.9099,  ..., -0.1923, -0.4463, -1.2157],\n",
       "         [ 1.0000, -1.1773,  0.4488,  ..., -0.8928, -0.6887,  0.0170]]),\n",
       " 'edge_features': tensor([[ 2.6292e+03,  1.6206e+02, -3.1910e+03,  ..., -3.6218e+03,\n",
       "          -3.1507e+03,  2.4183e+03],\n",
       "         [ 5.8881e+03,  5.0527e+02, -1.8213e+03,  ..., -6.0879e+03,\n",
       "           6.6150e+02,  7.9618e+03],\n",
       "         [ 9.3244e+03, -1.7300e+03, -1.9589e+03,  ..., -1.2378e+04,\n",
       "          -9.9628e+03,  4.6115e+03],\n",
       "         ...,\n",
       "         [-5.0162e-01, -2.3711e-01,  3.0317e-01,  ..., -6.7856e-01,\n",
       "          -5.3903e-01, -3.7626e-01],\n",
       "         [ 1.4621e-01, -6.8904e-01,  3.4657e-01,  ..., -1.3016e-01,\n",
       "          -1.1810e-01, -2.2751e-01],\n",
       "         [-4.1546e-01, -1.3146e-01,  2.8218e-01,  ..., -6.2758e-01,\n",
       "          -5.1843e-01, -3.9011e-01]], grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Preprocessing_module_my = Preprocessing()\n",
    "\n",
    "data = AtomicData.to_AtomicDataDict(dataset[0])\n",
    "\n",
    "Main_Module_my(Preprocessing_module_my(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "6adcdf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@compile_mode(\"script\")\n",
    "class Allegro_Module_my(GraphModuleMixin, torch.nn.Module):\n",
    "    # saved params\n",
    "    num_layers: int\n",
    "    field: str\n",
    "    out_field: str\n",
    "    num_types: int\n",
    "    env_embed_mul: int\n",
    "    weight_numel: int\n",
    "    latent_resnet: bool\n",
    "    embed_initial_edge: bool\n",
    "\n",
    "    # internal values\n",
    "    _env_builder_w_index: List[int]\n",
    "    _env_builder_n_irreps: int\n",
    "    _input_pad: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # required params\n",
    "        num_layers: int,\n",
    "        num_types: int,\n",
    "        r_max: float,\n",
    "        avg_num_neighbors: Optional[float] = None,\n",
    "        # cutoffs\n",
    "        r_start_cos_ratio: float = 0.8,\n",
    "        PolynomialCutoff_p: float = 6,\n",
    "        per_layer_cutoffs: Optional[List[float]] = None,\n",
    "        cutoff_type: str = \"polynomial\",\n",
    "        # general hyperparameters:\n",
    "        field: str = AtomicDataDict.EDGE_ATTRS_KEY,\n",
    "        edge_invariant_field: str = AtomicDataDict.EDGE_EMBEDDING_KEY,\n",
    "        node_invariant_field: str = AtomicDataDict.NODE_ATTRS_KEY,\n",
    "        env_embed_multiplicity: int = 32,\n",
    "        embed_initial_edge: bool = True,\n",
    "        linear_after_env_embed: bool = False,\n",
    "        nonscalars_include_parity: bool = True,\n",
    "        # MLP parameters:\n",
    "        #two_body_latent=ScalarMLPFunction,\n",
    "        #two_body_latent_kwargs={},\n",
    "        #env_embed=ScalarMLPFunction,\n",
    "        #env_embed_kwargs={},\n",
    "        #latent=ScalarMLPFunction,\n",
    "        #latent_kwargs={},\n",
    "        latent_resnet: bool = True,\n",
    "        #latent_resnet_update_ratios: Optional[List[float]] = None,\n",
    "        #latent_resnet_update_ratios_learnable: bool = False,\n",
    "        latent_out_field: Optional[str] = 'edge_features',\n",
    "        # Performance parameters:\n",
    "        pad_to_alignment: int = 1,\n",
    "        sparse_mode: Optional[str] = None,\n",
    "        # Other:\n",
    "        irreps_in=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        SCALAR = o3.Irrep(\"0e\")  # define for convinience\n",
    "\n",
    "        # save parameters\n",
    "        assert (\n",
    "            num_layers >= 1\n",
    "        )  # zero layers is \"two body\", but we don't need to support that fallback case\n",
    "        self.num_layers = num_layers\n",
    "        self.nonscalars_include_parity = nonscalars_include_parity\n",
    "        self.field = field\n",
    "        self.latent_out_field = latent_out_field\n",
    "        self.edge_invariant_field = edge_invariant_field\n",
    "        self.node_invariant_field = node_invariant_field\n",
    "        self.latent_resnet = latent_resnet\n",
    "        self.env_embed_mul = env_embed_multiplicity\n",
    "        self.r_start_cos_ratio = r_start_cos_ratio\n",
    "        self.polynomial_cutoff_p = float(PolynomialCutoff_p)\n",
    "        self.cutoff_type = cutoff_type\n",
    "        assert cutoff_type in (\"cosine\", \"polynomial\")\n",
    "        self.embed_initial_edge = embed_initial_edge\n",
    "        self.avg_num_neighbors = avg_num_neighbors\n",
    "        self.linear_after_env_embed = linear_after_env_embed\n",
    "        self.num_types = num_types\n",
    "\n",
    "        # set up irreps\n",
    "        self._init_irreps(\n",
    "            irreps_in=irreps_in,\n",
    "            required_irreps_in=[\n",
    "                self.field,\n",
    "                self.edge_invariant_field,\n",
    "                self.node_invariant_field,\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # for normalization of env embed sums\n",
    "        # one per layer\n",
    "        self.register_buffer(\n",
    "            \"env_sum_normalizations\",\n",
    "            # dividing by sqrt(N)\n",
    "            torch.as_tensor([avg_num_neighbors] * num_layers).rsqrt(),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.latents = torch.nn.ModuleList([])\n",
    "        self.env_embed_mlps = torch.nn.ModuleList([])\n",
    "        self.tps = torch.nn.ModuleList([])\n",
    "        self.linears = torch.nn.ModuleList([])\n",
    "        self.env_linears = torch.nn.ModuleList([])\n",
    "\n",
    "        # Embed to the spharm * it as mul\n",
    "        input_irreps = self.irreps_in[self.field]\n",
    "        # this is not inherant, but no reason to fix right now:\n",
    "        \n",
    "        irrep_hidden = o3.Irreps('32x0e + 32x0o + 32x1e + 32x1o + 32x2e + 32x2e')\n",
    "        irrep_hidden_last = o3.Irreps('32x0e')\n",
    "        \n",
    "        n_basis = 8\n",
    "        \n",
    "        \n",
    "        # First layer\n",
    "        self.latents.append(o3.Linear(o3.Irreps(f'{n_basis + 2 * num_types}x0e'),\n",
    "                                               '32x0e'))\n",
    "        \n",
    "        \n",
    "        self.env_embed_mlps.append(o3.Linear(self.latents[-1].irreps_out, self.latents[-1].irreps_out))\n",
    "        \n",
    "        self.env_linears.append(FullyConnectedTensorProduct(\n",
    "            self.env_embed_mlps[-1].irreps_out,\n",
    "            irreps_in['edge_attrs'],\n",
    "            o3.Irreps([(self.env_embed_mlps[-1].irreps_out[0][0], el[1]) for el in irreps_in['edge_attrs']])\n",
    "        ))\n",
    "        \n",
    "        self.tps.append(FullyConnectedTensorProduct(\n",
    "            irreps_in['edge_attrs'],\n",
    "            self.env_linears[-1].irreps_out,\n",
    "            irrep_hidden\n",
    "        ))\n",
    "        \n",
    "        self.linears.append(o3.Linear(self.tps[-1].irreps_out, self.tps[-1].irreps_out))\n",
    "        \n",
    "        # Second and later layers\n",
    "        for i in range(num_layers - 1):\n",
    "            self.latents.append(o3.Linear('64x0e', '32x0e'))\n",
    "            self.env_embed_mlps.append(o3.Linear(self.latents[-1].irreps_out, self.latents[-1].irreps_out))\n",
    "            \n",
    "            self.env_linears.append(FullyConnectedTensorProduct(\n",
    "                self.env_embed_mlps[-1].irreps_out,\n",
    "                irreps_in['edge_attrs'],\n",
    "                o3.Irreps([(self.env_embed_mlps[-1].irreps_out[0][0], el[1]) for el in irreps_in['edge_attrs']])\n",
    "            ))\n",
    "            \n",
    "            if i != num_layers - 1:\n",
    "                self.tps.append(FullyConnectedTensorProduct(\n",
    "                    irrep_hidden,\n",
    "                    self.env_linears[-1].irreps_out,\n",
    "                    irrep_hidden\n",
    "                ))\n",
    "            # Handling last layer other way\n",
    "            else:\n",
    "                self.tps.append(FullyConnectedTensorProduct(\n",
    "                    irrep_hidden,\n",
    "                    self.env_linears[-1].irreps_out,\n",
    "                    irrep_hidden_last\n",
    "                ))\n",
    "                \n",
    "            self.linears.append(o3.Linear(self.tps[-1].irreps_out, self.tps[-1].irreps_out))\n",
    "            \n",
    "        \n",
    "        self.final_latent = o3.Linear('64x0e', '32x0e')\n",
    "        self.irreps_out['edge_features'] = o3.Irreps('32x0e')\n",
    "        \n",
    "        \n",
    "    def forward(self, data: AtomicDataDict.Type) -> AtomicDataDict.Type:\n",
    "        \n",
    "        \"\"\"Evaluate.\n",
    "\n",
    "        :param data: AtomicDataDict.Type\n",
    "        :return: AtomicDataDict.Type\n",
    "        \"\"\"\n",
    "        data['forces'] = torch.zeros_like(data['pos'])\n",
    "        \n",
    "        edge_center = data[AtomicDataDict.EDGE_INDEX_KEY][0]\n",
    "        edge_neighbor = data[AtomicDataDict.EDGE_INDEX_KEY][1]\n",
    "\n",
    "        edge_attr = data[self.field]\n",
    "        # pad edge_attr\n",
    "        #if self._input_pad > 0:\n",
    "        #    edge_attr = torch.cat(\n",
    "        #        (\n",
    "        #           edge_attr,\n",
    "        #            self._zero.expand(len(edge_attr), self._input_pad),\n",
    "        #        ),\n",
    "        #        dim=-1,\n",
    "        #    )\n",
    "        # The nonscalar features. Initially, the edge data.\n",
    "        features = edge_attr\n",
    "        \n",
    "        \n",
    "        edge_length = data[AtomicDataDict.EDGE_LENGTH_KEY]\n",
    "        num_edges: int = len(edge_attr)\n",
    "        edge_invariants = data[self.edge_invariant_field]\n",
    "        node_invariants = data[self.node_invariant_field]\n",
    "\n",
    "        \n",
    "        # For the first layer, we use the input invariants:\n",
    "        # The center and neighbor invariants and edge invariants\n",
    "        latent_inputs_to_cat = [\n",
    "            node_invariants[edge_center],\n",
    "            node_invariants[edge_neighbor],\n",
    "            edge_invariants,\n",
    "        ]\n",
    "        \n",
    "        # cutoff\n",
    "        r_max = 5\n",
    "        \n",
    "        factor = 1/r_max\n",
    "        p = 6\n",
    "\n",
    "        x = edge_length * factor\n",
    "\n",
    "        cutoff = 1.0\n",
    "        cutoff = cutoff - (((p + 1.0) * (p + 2.0) / 2.0) * torch.pow(x, p))\n",
    "        cutoff = cutoff + (p * (p + 2.0) * torch.pow(x, p + 1.0))\n",
    "        cutoff = cutoff - ((p * (p + 1.0) / 2) * torch.pow(x, p + 2.0))\n",
    "        cutoff *= (x < 1.0)\n",
    "\n",
    "        cutoff_coeffs = cutoff\n",
    "        \n",
    "        layer_index = 0\n",
    "        # !!!! REMEMBER !!!! update final layer if update the code in main loop!!!\n",
    "        # This goes through layer0, layer1, ..., layer_max-1\n",
    "        for latent, env_embed_mlp, env_linear, tp, linear in zip(\n",
    "            self.latents, self.env_embed_mlps, self.env_linears, self.tps, self.linears\n",
    "        ):\n",
    "            # Determine which edges are still in play\n",
    "            new_latents = latent(torch.cat(latent_inputs_to_cat, dim=-1))\n",
    "            # Apply cutoff, which propagates through to everything else\n",
    "            new_latents = cutoff_coeffs.unsqueeze(-1) * new_latents\n",
    "\n",
    "            if self.latent_resnet and layer_index > 0:\n",
    "                # Residual update\n",
    "                latents += new_latents\n",
    "            else:\n",
    "                latents = new_latents\n",
    "                \n",
    "            # From the latents, compute the weights for active edges:\n",
    "            weights = env_embed_mlp(latents)\n",
    "            w_index: int = 0\n",
    "\n",
    "            # Produce right array of features with channels\n",
    "            local_env_per_edge = env_linear(weights, edge_attr)\n",
    "            \n",
    "            # Sum over the env\n",
    "            local_env_per_edge = scatter(local_env_per_edge[edge_neighbor], \n",
    "                                           edge_center, dim=0, dim_size=len(edge_center))\n",
    "            \n",
    "            # Now do the TP\n",
    "            # recursively tp current features with the environment embeddings\n",
    "            features = tp(features, local_env_per_edge)\n",
    "\n",
    "            \n",
    "            print(features.shape, local_env_per_edge.shape)\n",
    "            # Get invariants\n",
    "            # features has shape [z][mul][k]\n",
    "            # we know scalars are first\n",
    "            scalars = features[:, :32].reshape(\n",
    "                features.shape[0], 32)\n",
    "\n",
    "            # do the linear\n",
    "            features = linear(features)\n",
    "\n",
    "            # For layer2+, use the previous latents and scalars\n",
    "            # This makes it deep\n",
    "            latent_inputs_to_cat = [\n",
    "                latents,\n",
    "                scalars,\n",
    "            ]\n",
    "\n",
    "            # increment counter\n",
    "            layer_index += 1\n",
    "\n",
    "        # - final layer -\n",
    "        \n",
    "        new_latents = self.final_latent(\n",
    "            torch.cat(latent_inputs_to_cat, dim=-1)\n",
    "        )\n",
    "        new_latents = cutoff_coeffs.unsqueeze(-1) * new_latents\n",
    "        \n",
    "        latents += new_latents\n",
    "        # - end final layer -\n",
    "\n",
    "        # final latents\n",
    "        data['edge_features'] = latents\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f2ee493b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'edge_embedding': 8x0e, 'node_attrs': 3x0e, 'edge_attrs': 1x0e+1x1o+1x2e}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irreps_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "741d47ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Allegro_Module_my(\n",
       "  (latents): ModuleList(\n",
       "    (0): Linear(14x0e -> 32x0e | 448 weights)\n",
       "    (1-2): 2 x Linear(64x0e -> 32x0e | 2048 weights)\n",
       "  )\n",
       "  (env_embed_mlps): ModuleList(\n",
       "    (0-2): 3 x Linear(32x0e -> 32x0e | 1024 weights)\n",
       "  )\n",
       "  (tps): ModuleList(\n",
       "    (0): FullyConnectedTensorProduct(1x0e+1x1o+1x2e x 32x0e+32x1o+32x2e -> 32x0e+32x0o+32x1e+32x1o+64x2e | 17408 paths | 17408 weights)\n",
       "    (1-2): 2 x FullyConnectedTensorProduct(32x0e+32x0o+32x1e+32x1o+64x2e x 32x0e+32x1o+32x2e -> 32x0e+32x0o+32x1e+32x1o+64x2e | 1048576 paths | 1048576 weights)\n",
       "  )\n",
       "  (linears): ModuleList(\n",
       "    (0-2): 3 x Linear(32x0e+32x0o+32x1e+32x1o+32x2e+32x2e -> 32x0e+32x0o+32x1e+32x1o+32x2e+32x2e | 8192 weights)\n",
       "  )\n",
       "  (env_linears): ModuleList(\n",
       "    (0-2): 3 x FullyConnectedTensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 3072 paths | 3072 weights)\n",
       "  )\n",
       "  (final_latent): Linear(64x0e -> 32x0e | 2048 weights)\n",
       ")"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Main_Module_my = Allegro_Module_my(r_max = 5,\n",
    "               num_layers = 3,\n",
    "               num_types = 3,\n",
    "               avg_num_neighbors=10., \n",
    "               irreps_in=irreps_in)\n",
    "\n",
    "Main_Module_my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "4ee12401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([364, 576]) torch.Size([364, 288])\n",
      "torch.Size([364, 576]) torch.Size([364, 288])\n",
      "torch.Size([364, 576]) torch.Size([364, 288])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([364, 32])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Main_Module_my(data_in)['edge_features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "458183ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32x0e+32x1o+32x2e"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = o3.Irreps('32x0e') \n",
    "b = o3.Irreps('1x0e + 1x1o + 1x2e')\n",
    "\n",
    "\n",
    "o3.Irreps([(a[0][0], el[1]) for el in b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "97015f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphModel(\n",
       "  (model): RescaleOutput(\n",
       "    (model): GradientOutput(\n",
       "      (func): SequentialGraphNetwork(\n",
       "        (one_hot): OneHotAtomEncoding()\n",
       "        (radial_basis): RadialBasisEdgeEncoding(\n",
       "          (basis): NormalizedBasis(\n",
       "            (basis): BesselBasis()\n",
       "          )\n",
       "          (cutoff): PolynomialCutoff()\n",
       "        )\n",
       "        (spharm): SphericalHarmonicEdgeAttrs(\n",
       "          (sh): SphericalHarmonics()\n",
       "        )\n",
       "        (allegro): Allegro_Module(\n",
       "          (latents): ModuleList(\n",
       "            (0-1): 2 x ScalarMLPFunction(\n",
       "              (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
       "            )\n",
       "          )\n",
       "          (env_embed_mlps): ModuleList(\n",
       "            (0-1): 2 x ScalarMLPFunction(\n",
       "              (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
       "            )\n",
       "          )\n",
       "          (tps): ModuleList(\n",
       "            (0-1): 2 x RecursiveScriptModule(original_name=GraphModule)\n",
       "          )\n",
       "          (linears): ModuleList(\n",
       "            (0-1): 2 x RecursiveScriptModule(original_name=GraphModule)\n",
       "          )\n",
       "          (env_linears): ModuleList(\n",
       "            (0-1): 2 x Identity()\n",
       "          )\n",
       "          (_env_weighter): MakeWeightedChannels()\n",
       "          (final_latent): ScalarMLPFunction(\n",
       "            (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
       "          )\n",
       "        )\n",
       "        (edge_eng): ScalarMLP(\n",
       "          (_module): ScalarMLPFunction(\n",
       "            (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
       "          )\n",
       "        )\n",
       "        (edge_eng_sum): EdgewiseEnergySum()\n",
       "        (per_species_rescale): PerSpeciesScaleShift()\n",
       "        (total_energy_sum): AtomwiseReduce()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "7ee91fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('edge_features', 'edge_energy')"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allegro._keys import EDGE_FEATURES, EDGE_ENERGY\n",
    "\n",
    "EDGE_FEATURES, EDGE_ENERGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "50264df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6292e+03,  1.6206e+02, -3.1910e+03,  ..., -3.6218e+03,\n",
       "         -3.1507e+03,  2.4183e+03],\n",
       "        [ 5.8881e+03,  5.0527e+02, -1.8213e+03,  ..., -6.0879e+03,\n",
       "          6.6150e+02,  7.9618e+03],\n",
       "        [ 9.3244e+03, -1.7300e+03, -1.9589e+03,  ..., -1.2378e+04,\n",
       "         -9.9628e+03,  4.6115e+03],\n",
       "        ...,\n",
       "        [-5.0162e-01, -2.3711e-01,  3.0317e-01,  ..., -6.7856e-01,\n",
       "         -5.3903e-01, -3.7626e-01],\n",
       "        [ 1.4621e-01, -6.8904e-01,  3.4657e-01,  ..., -1.3016e-01,\n",
       "         -1.1810e-01, -2.2751e-01],\n",
       "        [-4.1546e-01, -1.3146e-01,  2.8218e-01,  ..., -6.2758e-01,\n",
       "         -5.1843e-01, -3.9011e-01]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in[EDGE_FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "89a83cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "instantiate Preprocessing\n",
      "...Preprocessing_param = dict(\n",
      "...   optional_args = {},\n",
      "...   positional_args = {'irreps_in': None})\n",
      "instantiate Allegro_Module_my\n",
      "   optional_args :                                          num_layers\n",
      "   optional_args :                                   avg_num_neighbors\n",
      "   optional_args :                                               r_max\n",
      "   optional_args :                                           num_types\n",
      "...Allegro_Module_my_param = dict(\n",
      "...   optional_args = {'avg_num_neighbors': 10.0, 'r_start_cos_ratio': 0.8, 'PolynomialCutoff_p': 6, 'per_layer_cutoffs': None, 'cutoff_type': 'polynomial', 'field': 'edge_attrs', 'edge_invariant_field': 'edge_embedding', 'node_invariant_field': 'node_attrs', 'env_embed_multiplicity': 32, 'embed_initial_edge': True, 'linear_after_env_embed': False, 'nonscalars_include_parity': True, 'latent_resnet': True, 'latent_out_field': 'edge_features', 'pad_to_alignment': 1, 'sparse_mode': None, 'r_max': 5, 'num_layers': 3, 'num_types': 3},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1o, 'edge_index': None, 'edge_embedding': 8x0e, 'node_attrs': 3x0e, 'edge_attrs': 1x0e+1x1o+1x2e}})\n",
      "instantiate ScalarMLP\n",
      "   optional_args :                                               field\n",
      "   optional_args :                                           out_field\n",
      "   optional_args :                               mlp_latent_dimensions\n",
      "   optional_args :                                mlp_output_dimension\n",
      "...ScalarMLP_param = dict(\n",
      "...   optional_args = {'mlp_nonlinearity': 'silu', 'mlp_initialization': 'uniform', 'mlp_dropout_p': 0.0, 'mlp_batchnorm': False, 'field': 'edge_features', 'out_field': 'edge_energy', 'mlp_output_dimension': 1, 'mlp_latent_dimensions': [32]},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1o, 'edge_index': None, 'edge_embedding': 8x0e, 'node_attrs': 3x0e, 'edge_attrs': 1x0e+1x1o+1x2e, 'edge_features': 32x0e}})\n",
      "instantiate EdgewiseEnergySum\n",
      "   optional_args :                                   avg_num_neighbors\n",
      "   optional_args :                                           num_types\n",
      "...EdgewiseEnergySum_param = dict(\n",
      "...   optional_args = {'avg_num_neighbors': 10.0, 'normalize_edge_energy_sum': True, 'per_edge_species_scale': False, 'num_types': 3.0},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1o, 'edge_index': None, 'edge_embedding': 8x0e, 'node_attrs': 3x0e, 'edge_attrs': 1x0e+1x1o+1x2e, 'edge_features': 32x0e, 'edge_energy': 1x0e}})\n",
      "instantiate AtomwiseReduce\n",
      "   optional_args :                                              reduce\n",
      "   optional_args :                                               field\n",
      "   optional_args :                                           out_field\n",
      "...AtomwiseReduce_param = dict(\n",
      "...   optional_args = {'out_field': 'total_energy', 'reduce': 'sum', 'avg_num_atoms': None, 'field': 'atomic_energy'},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1o, 'edge_index': None, 'edge_embedding': 8x0e, 'node_attrs': 3x0e, 'edge_attrs': 1x0e+1x1o+1x2e, 'edge_features': 32x0e, 'edge_energy': 1x0e, 'atomic_energy': 1x0e}})\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "import logging\n",
    "\n",
    "from e3nn import o3\n",
    "\n",
    "from nequip.data import AtomicDataDict, AtomicDataset\n",
    "\n",
    "from nequip.nn import SequentialGraphNetwork, AtomwiseReduce\n",
    "from nequip.nn.radial_basis import BesselBasis\n",
    "\n",
    "from nequip.nn.embedding import (\n",
    "    OneHotAtomEncoding,\n",
    "    SphericalHarmonicEdgeAttrs,\n",
    "    RadialBasisEdgeEncoding,\n",
    ")\n",
    "\n",
    "from allegro.nn import (\n",
    "    NormalizedBasis,\n",
    "    EdgewiseEnergySum,\n",
    "    Allegro_Module,\n",
    "    ScalarMLP,\n",
    ")\n",
    "from allegro._keys import EDGE_FEATURES, EDGE_ENERGY\n",
    "\n",
    "from nequip.model import builder_utils\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "layers = {\n",
    "        # -- Encode --\n",
    "        # Get various edge invariants\n",
    "        \"one_hot\": OneHotAtomEncoding,\n",
    "        \"radial_basis\": (\n",
    "            RadialBasisEdgeEncoding,\n",
    "            dict(\n",
    "                basis=(\n",
    "                    NormalizedBasis\n",
    "                    if config.get(\"normalize_basis\", True)\n",
    "                    else BesselBasis\n",
    "                ),\n",
    "                out_field=AtomicDataDict.EDGE_EMBEDDING_KEY,\n",
    "            ),\n",
    "        ),\n",
    "        # Get edge nonscalars\n",
    "        \"spharm\": SphericalHarmonicEdgeAttrs,\n",
    "        # The core allegro model:\n",
    "        \"allegro\": (\n",
    "            Allegro_Module,\n",
    "            dict(\n",
    "                field=AtomicDataDict.EDGE_ATTRS_KEY,  # initial input is the edge SH\n",
    "                edge_invariant_field=AtomicDataDict.EDGE_EMBEDDING_KEY,\n",
    "                node_invariant_field=AtomicDataDict.NODE_ATTRS_KEY,\n",
    "            ),\n",
    "        ),\n",
    "        \"edge_eng\": (\n",
    "            ScalarMLP,\n",
    "            dict(field=EDGE_FEATURES, out_field=EDGE_ENERGY, mlp_output_dimension=1),\n",
    "        ),\n",
    "        # Sum edgewise energies -> per-atom energies:\n",
    "        \"edge_eng_sum\": EdgewiseEnergySum,\n",
    "        # Sum system energy:\n",
    "        \"total_energy_sum\": (\n",
    "            AtomwiseReduce,\n",
    "            dict(\n",
    "                reduce=\"sum\",\n",
    "                field=AtomicDataDict.PER_ATOM_ENERGY_KEY,\n",
    "                out_field=AtomicDataDict.TOTAL_ENERGY_KEY,\n",
    "            ),\n",
    "        ),\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "layers = {\n",
    "    \"Preprocessing\": (Preprocessing,\n",
    "    dict()),\n",
    "    \"Main_Allegro\": (Allegro_Module_my,\n",
    "    dict(r_max = 5,\n",
    "               num_layers = 3,\n",
    "               num_types = 3,\n",
    "               avg_num_neighbors=10.\n",
    "        )),\n",
    "    \"edge_eng\": (\n",
    "            ScalarMLP,\n",
    "            dict(field=EDGE_FEATURES, \n",
    "                 out_field=EDGE_ENERGY, \n",
    "                 mlp_output_dimension=1,\n",
    "                 mlp_latent_dimensions= [32]),\n",
    "        ),\n",
    "        # Sum edgewise energies -> per-atom energies:\n",
    "        \"edge_eng_sum\": (EdgewiseEnergySum,\n",
    "            dict(avg_num_neighbors = 10.,\n",
    "                 num_types = 3.\n",
    "                ),\n",
    "                        ),\n",
    "        # Sum system energy:\n",
    "        \"total_energy_sum\": (\n",
    "            AtomwiseReduce,\n",
    "            dict(\n",
    "                reduce=\"sum\",\n",
    "                field=AtomicDataDict.PER_ATOM_ENERGY_KEY,\n",
    "                out_field=AtomicDataDict.TOTAL_ENERGY_KEY,\n",
    "            ),\n",
    "        ),\n",
    "}\n",
    "\n",
    "\n",
    "model_my = SequentialGraphNetwork({'Preprocessing': Preprocessing_module_my,\n",
    "                                   'Main_Allegro': Main_Module_my}) \n",
    "\n",
    "\n",
    "model_my_from_param = SequentialGraphNetwork.from_parameters(shared_params={},\n",
    "                                                             layers = layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "6aa812d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([364, 576]) torch.Size([364, 288])\n",
      "torch.Size([364, 576]) torch.Size([364, 288])\n",
      "torch.Size([364, 576]) torch.Size([364, 288])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 3])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_my_from_param(data_in)['forces'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "98727fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'total_energy'"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AtomicDataDict.TOTAL_ENERGY_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "722cf35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nequip.nn import GraphModel, GradientOutput\n",
    "\n",
    "\n",
    "trainer.model = GraphModel(GradientOutput(model_my_from_param,\n",
    "                                          of = 'total_energy',\n",
    "                                          wrt = 'pos',\n",
    "                                          out_field='forces'))\n",
    "\n",
    "model_my_final = GraphModel(GradientOutput(model_my_from_param,\n",
    "                                          of = 'total_energy',\n",
    "                                          wrt = 'pos',\n",
    "                                          out_field='forces'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "8e55be44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for el in dataset:\n",
    "#    el['forces'] = torch.zeros_like(el['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "1e38ad3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtomicData(atom_types=[21, 1], cell=[3, 3], edge_cell_shift=[364, 3], edge_index=[2, 364], forces=[21, 3], pbc=[3], pos=[21, 3], total_energy=[1])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "e78a3e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_dataset(dataset, validation_dataset)\n",
    "\n",
    "\n",
    "trainer.model = model_my_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "1ae0bb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "! Starting training ...\n",
      "Saved trainer to results/aspirin/example/trainer.pth\n",
      "Saved last model to to results/aspirin/example/last_model.pth\n",
      "instantiate Metrics\n",
      "...Metrics_param = dict(\n",
      "...   optional_args = {},\n",
      "...   positional_args = {'components': [('forces', 'mae', {'PerSpecies': False}), ('forces', 'rmse', {'PerSpecies': False}), ('total_energy', 'mae', {'PerSpecies': False}), ('total_energy', 'rmse', {'PerSpecies': False})]})\n",
      "instantiate L1Loss\n",
      "...L1Loss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      "instantiate L1Loss\n",
      "...L1Loss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      "instantiate L1Loss\n",
      "...L1Loss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      "instantiate L1Loss\n",
      "...L1Loss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1808, 576]) torch.Size([1808, 288])\n",
      "torch.Size([1808, 576]) torch.Size([1808, 288])\n",
      "torch.Size([1808, 576]) torch.Size([1808, 288])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1806, 576]) torch.Size([1806, 288])\n",
      "torch.Size([1806, 576]) torch.Size([1806, 288])\n",
      "torch.Size([1806, 576]) torch.Size([1806, 288])\n",
      "torch.Size([1804, 576]) torch.Size([1804, 288])\n",
      "torch.Size([1804, 576]) torch.Size([1804, 288])\n",
      "torch.Size([1804, 576]) torch.Size([1804, 288])\n",
      "torch.Size([1818, 576]) torch.Size([1818, 288])\n",
      "torch.Size([1818, 576]) torch.Size([1818, 288])\n",
      "torch.Size([1818, 576]) torch.Size([1818, 288])\n",
      "torch.Size([1808, 576]) torch.Size([1808, 288])\n",
      "torch.Size([1808, 576]) torch.Size([1808, 288])\n",
      "torch.Size([1808, 576]) torch.Size([1808, 288])\n",
      "torch.Size([1832, 576]) torch.Size([1832, 288])\n",
      "torch.Size([1832, 576]) torch.Size([1832, 288])\n",
      "torch.Size([1832, 576]) torch.Size([1832, 288])\n",
      "torch.Size([1806, 576]) torch.Size([1806, 288])\n",
      "torch.Size([1806, 576]) torch.Size([1806, 288])\n",
      "torch.Size([1806, 576]) torch.Size([1806, 288])\n",
      "torch.Size([1830, 576]) torch.Size([1830, 288])\n",
      "torch.Size([1830, 576]) torch.Size([1830, 288])\n",
      "torch.Size([1830, 576]) torch.Size([1830, 288])\n",
      "torch.Size([1812, 576]) torch.Size([1812, 288])\n",
      "torch.Size([1812, 576]) torch.Size([1812, 288])\n",
      "torch.Size([1812, 576]) torch.Size([1812, 288])\n",
      "torch.Size([1808, 576]) torch.Size([1808, 288])\n",
      "torch.Size([1808, 576]) torch.Size([1808, 288])\n",
      "torch.Size([1808, 576]) torch.Size([1808, 288])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      0    10     3.74e+08     9.54e+05     3.73e+08          320          977     4.06e+05     4.06e+05\n",
      "\n",
      "\n",
      "  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
      "! Initial Validation          0   55.380    0.001      1.6e+06     3.72e+08     3.74e+08          371     1.27e+03     4.05e+05     4.05e+05\n",
      "Wall time: 55.38995679800064\n",
      "! Best model        0 373679296.000\n",
      "Saved trainer to results/aspirin/example/trainer.pth\n",
      "Saved last model to to results/aspirin/example/last_model.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1814, 576]) torch.Size([1814, 288])\n",
      "torch.Size([1814, 576]) torch.Size([1814, 288])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[239], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/nequip/train/trainer.py:784\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_init_callback()\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_cond:\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_of_epoch_save()\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_callbacks:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/nequip/train/trainer.py:919\u001b[0m, in \u001b[0;36mTrainer.epoch_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset)\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mibatch, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n\u001b[0;32m--> 919\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVALIDATION\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_of_batch_log(batch_type\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_end_of_batch_callbacks:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/nequip/train/trainer.py:814\u001b[0m, in \u001b[0;36mTrainer.batch_step\u001b[0;34m(self, data, validation)\u001b[0m\n\u001b[1;32m    810\u001b[0m data_for_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39munscale(data, force_process\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    812\u001b[0m \u001b[38;5;66;03m# Run model\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;66;03m# We make a shallow copy of the input dict in case the model modifies it\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_for_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# If we're in evaluation mode (i.e. validation), then\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# data_for_loss's target prop is unnormalized, and out's has been rescaled to be in the same units\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# If we're in training, data_for_loss's target prop has been normalized, and out's hasn't been touched, so they're both in normalized units\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;66;03m# Note that either way all normalization was handled internally by GraphModel via RescaleOutput\u001b[39;00m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validation:\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# Actually do an optimization step, since we're training:\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/pytorch-intel-mps/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/nequip/nn/_graph_model.py:112\u001b[0m, in \u001b[0;36mGraphModel.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    110\u001b[0m         new_data[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# run the model\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Documents/GitHub/pytorch-intel-mps/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/nequip/nn/_grad_output.py:85\u001b[0m, in \u001b[0;36mGradientOutput.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     83\u001b[0m     wrt_tensors\u001b[38;5;241m.\u001b[39mappend(data[k])\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# run func\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Get grads\u001b[39;00m\n\u001b[1;32m     87\u001b[0m grads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# TODO:\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# This makes sense for scalar batch-level or batch-wise outputs, specifically because d(sum(batches))/d wrt = sum(d batch / d wrt) = d my_batch / d wrt\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m     create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,  \u001b[38;5;66;03m# needed to allow gradients of this output during training\u001b[39;00m\n\u001b[1;32m     95\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/pytorch-intel-mps/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/nequip/nn/_graph_mixin.py:366\u001b[0m, in \u001b[0;36mSequentialGraphNetwork.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: AtomicDataDict\u001b[38;5;241m.\u001b[39mType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AtomicDataDict\u001b[38;5;241m.\u001b[39mType:\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 366\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/pytorch-intel-mps/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[222], line 253\u001b[0m, in \u001b[0;36mAllegro_Module_my.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    248\u001b[0m local_env_per_edge \u001b[38;5;241m=\u001b[39m scatter(local_env_per_edge[edge_neighbor], \n\u001b[1;32m    249\u001b[0m                                edge_center, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dim_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(edge_center))\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Now do the TP\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# recursively tp current features with the environment embeddings\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mtp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_env_per_edge\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mprint\u001b[39m(features\u001b[38;5;241m.\u001b[39mshape, local_env_per_edge\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Get invariants\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# features has shape [z][mul][k]\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# we know scalars are first\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/pytorch-intel-mps/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/e3nn/o3/_tensor_product/_tensor_product.py:542\u001b[0m, in \u001b[0;36mTensorProduct.forward\u001b[0;34m(self, x, y, weight)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;66;03m# - PROFILER - with torch.autograd.profiler.record_function(self._profiling_str):\u001b[39;00m\n\u001b[1;32m    541\u001b[0m real_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_weights(weight)\n\u001b[0;32m--> 542\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compiled_main_left_right\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/pytorch-intel-mps/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "3d84c575",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'forces'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[193], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_my_final\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_in\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/pytorch-intel-mps/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/nequip/nn/_graph_model.py:112\u001b[0m, in \u001b[0;36mGraphModel.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    110\u001b[0m         new_data[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# run the model\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Documents/GitHub/pytorch-intel-mps/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/nequip/nn/_grad_output.py:85\u001b[0m, in \u001b[0;36mGradientOutput.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     83\u001b[0m     wrt_tensors\u001b[38;5;241m.\u001b[39mappend(data[k])\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# run func\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Get grads\u001b[39;00m\n\u001b[1;32m     87\u001b[0m grads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# TODO:\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# This makes sense for scalar batch-level or batch-wise outputs, specifically because d(sum(batches))/d wrt = sum(d batch / d wrt) = d my_batch / d wrt\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m     create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,  \u001b[38;5;66;03m# needed to allow gradients of this output during training\u001b[39;00m\n\u001b[1;32m     95\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/pytorch-intel-mps/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/nequip/nn/_graph_mixin.py:366\u001b[0m, in \u001b[0;36mSequentialGraphNetwork.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: AtomicDataDict\u001b[38;5;241m.\u001b[39mType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AtomicDataDict\u001b[38;5;241m.\u001b[39mType:\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 366\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/pytorch-intel-mps/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[71], line 37\u001b[0m, in \u001b[0;36mPreprocessing.forward\u001b[0;34m(self, data_in)\u001b[0m\n\u001b[1;32m     33\u001b[0m num_basis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m     34\u001b[0m r_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m---> 37\u001b[0m data_in \u001b[38;5;241m=\u001b[39m {key: torch\u001b[38;5;241m.\u001b[39mclone(data_in[key]) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data}\n\u001b[1;32m     38\u001b[0m data_in \u001b[38;5;241m=\u001b[39m AtomicDataDict\u001b[38;5;241m.\u001b[39mwith_edge_vectors(data_in, with_lengths\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     40\u001b[0m edge_length \u001b[38;5;241m=\u001b[39m data_my[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_lengths\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[71], line 37\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m num_basis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m     34\u001b[0m r_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m---> 37\u001b[0m data_in \u001b[38;5;241m=\u001b[39m {key: torch\u001b[38;5;241m.\u001b[39mclone(\u001b[43mdata_in\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data}\n\u001b[1;32m     38\u001b[0m data_in \u001b[38;5;241m=\u001b[39m AtomicDataDict\u001b[38;5;241m.\u001b[39mwith_edge_vectors(data_in, with_lengths\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     40\u001b[0m edge_length \u001b[38;5;241m=\u001b[39m data_my[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_lengths\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'forces'"
     ]
    }
   ],
   "source": [
    "model_my_final(data_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "76a1b488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "! Starting training ...\n",
      "Saved trainer to results/aspirin/example/trainer.pth\n",
      "Saved last model to to results/aspirin/example/last_model.pth\n",
      "instantiate Metrics\n",
      "...Metrics_param = dict(\n",
      "...   optional_args = {},\n",
      "...   positional_args = {'components': [('forces', 'mae', {'PerSpecies': False}), ('forces', 'rmse', {'PerSpecies': False}), ('total_energy', 'mae', {'PerSpecies': False}), ('total_energy', 'rmse', {'PerSpecies': False})]})\n",
      "instantiate L1Loss\n",
      "...L1Loss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      "instantiate L1Loss\n",
      "...L1Loss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      "instantiate L1Loss\n",
      "...L1Loss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      "instantiate L1Loss\n",
      "...L1Loss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'forces'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/nequip/train/trainer.py:784\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_init_callback()\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_cond:\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_of_epoch_save()\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_callbacks:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/nequip/train/trainer.py:919\u001b[0m, in \u001b[0;36mTrainer.epoch_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset)\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mibatch, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n\u001b[0;32m--> 919\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVALIDATION\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_of_batch_log(batch_type\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_end_of_batch_callbacks:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/nequip/train/trainer.py:814\u001b[0m, in \u001b[0;36mTrainer.batch_step\u001b[0;34m(self, data, validation)\u001b[0m\n\u001b[1;32m    810\u001b[0m data_for_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39munscale(data, force_process\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    812\u001b[0m \u001b[38;5;66;03m# Run model\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;66;03m# We make a shallow copy of the input dict in case the model modifies it\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_for_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# If we're in evaluation mode (i.e. validation), then\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# data_for_loss's target prop is unnormalized, and out's has been rescaled to be in the same units\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# If we're in training, data_for_loss's target prop has been normalized, and out's hasn't been touched, so they're both in normalized units\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;66;03m# Note that either way all normalization was handled internally by GraphModel via RescaleOutput\u001b[39;00m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validation:\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# Actually do an optimization step, since we're training:\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/pytorch-intel-mps/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/nequip/nn/_graph_model.py:112\u001b[0m, in \u001b[0;36mGraphModel.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    110\u001b[0m         new_data[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# run the model\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Documents/GitHub/pytorch-intel-mps/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/nequip/nn/_grad_output.py:85\u001b[0m, in \u001b[0;36mGradientOutput.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     83\u001b[0m     wrt_tensors\u001b[38;5;241m.\u001b[39mappend(data[k])\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# run func\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Get grads\u001b[39;00m\n\u001b[1;32m     87\u001b[0m grads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# TODO:\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# This makes sense for scalar batch-level or batch-wise outputs, specifically because d(sum(batches))/d wrt = sum(d batch / d wrt) = d my_batch / d wrt\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m     create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,  \u001b[38;5;66;03m# needed to allow gradients of this output during training\u001b[39;00m\n\u001b[1;32m     95\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/pytorch-intel-mps/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/nequip/nn/_graph_mixin.py:366\u001b[0m, in \u001b[0;36mSequentialGraphNetwork.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: AtomicDataDict\u001b[38;5;241m.\u001b[39mType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AtomicDataDict\u001b[38;5;241m.\u001b[39mType:\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 366\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/pytorch-intel-mps/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[71], line 37\u001b[0m, in \u001b[0;36mPreprocessing.forward\u001b[0;34m(self, data_in)\u001b[0m\n\u001b[1;32m     33\u001b[0m num_basis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m     34\u001b[0m r_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m---> 37\u001b[0m data_in \u001b[38;5;241m=\u001b[39m {key: torch\u001b[38;5;241m.\u001b[39mclone(data_in[key]) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data}\n\u001b[1;32m     38\u001b[0m data_in \u001b[38;5;241m=\u001b[39m AtomicDataDict\u001b[38;5;241m.\u001b[39mwith_edge_vectors(data_in, with_lengths\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     40\u001b[0m edge_length \u001b[38;5;241m=\u001b[39m data_my[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_lengths\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[71], line 37\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m num_basis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m     34\u001b[0m r_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m---> 37\u001b[0m data_in \u001b[38;5;241m=\u001b[39m {key: torch\u001b[38;5;241m.\u001b[39mclone(\u001b[43mdata_in\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data}\n\u001b[1;32m     38\u001b[0m data_in \u001b[38;5;241m=\u001b[39m AtomicDataDict\u001b[38;5;241m.\u001b[39mwith_edge_vectors(data_in, with_lengths\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     40\u001b[0m edge_length \u001b[38;5;241m=\u001b[39m data_my[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_lengths\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'forces'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1164e98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([364, 576]) torch.Size([364, 288])\n",
      "torch.Size([364, 576]) torch.Size([364, 288])\n",
      "torch.Size([364, 576]) torch.Size([364, 288])\n"
     ]
    }
   ],
   "source": [
    "data_out = model_my_from_param(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "5ca2d3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['forces'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a9d5350c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphModel(\n",
       "  (model): RescaleOutput(\n",
       "    (model): GradientOutput(\n",
       "      (func): SequentialGraphNetwork(\n",
       "        (one_hot): OneHotAtomEncoding()\n",
       "        (radial_basis): RadialBasisEdgeEncoding(\n",
       "          (basis): NormalizedBasis(\n",
       "            (basis): BesselBasis()\n",
       "          )\n",
       "          (cutoff): PolynomialCutoff()\n",
       "        )\n",
       "        (spharm): SphericalHarmonicEdgeAttrs(\n",
       "          (sh): SphericalHarmonics()\n",
       "        )\n",
       "        (allegro): Allegro_Module(\n",
       "          (latents): ModuleList(\n",
       "            (0-1): 2 x ScalarMLPFunction(\n",
       "              (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
       "            )\n",
       "          )\n",
       "          (env_embed_mlps): ModuleList(\n",
       "            (0-1): 2 x ScalarMLPFunction(\n",
       "              (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
       "            )\n",
       "          )\n",
       "          (tps): ModuleList(\n",
       "            (0-1): 2 x RecursiveScriptModule(original_name=GraphModule)\n",
       "          )\n",
       "          (linears): ModuleList(\n",
       "            (0-1): 2 x RecursiveScriptModule(original_name=GraphModule)\n",
       "          )\n",
       "          (env_linears): ModuleList(\n",
       "            (0-1): 2 x Identity()\n",
       "          )\n",
       "          (_env_weighter): MakeWeightedChannels()\n",
       "          (final_latent): ScalarMLPFunction(\n",
       "            (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
       "          )\n",
       "        )\n",
       "        (edge_eng): ScalarMLP(\n",
       "          (_module): ScalarMLPFunction(\n",
       "            (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
       "          )\n",
       "        )\n",
       "        (edge_eng_sum): EdgewiseEnergySum()\n",
       "        (per_species_rescale): PerSpeciesScaleShift()\n",
       "        (total_energy_sum): AtomwiseReduce()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4763debe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_mkl",
   "language": "python",
   "name": "torch_mkl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
