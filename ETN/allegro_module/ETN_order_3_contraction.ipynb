{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc7a2ba3",
   "metadata": {},
   "source": [
    "### First few cells is just data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b139a9f2",
   "metadata": {},
   "source": [
    "My implementation is build on top of nequip and allegro python libraries to simplify data load and preprocessing.\n",
    "The wigner simbols are from e3nn\n",
    "Here are the links\n",
    "https://github.com/mir-group/nequip\n",
    "https://github.com/mir-group/allegro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba17ba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference example\n",
    "from nequip.data import dataset_from_config\n",
    "from nequip.utils import Config\n",
    "#from nequip.utils.misc import get_default_device_name\n",
    "#from nequip.utils.config import _GLOBAL_ALL_ASKED_FOR_KEYS\n",
    "\n",
    "from nequip.model import model_from_config\n",
    "\n",
    "\n",
    "default_config = dict(\n",
    "    root=\"./\",\n",
    "    tensorboard=False,\n",
    "    wandb=False,\n",
    "    model_builders=[\n",
    "        \"SimpleIrrepsConfig\",\n",
    "        \"EnergyModel\",\n",
    "        \"PerSpeciesRescale\",\n",
    "        \"StressForceOutput\",\n",
    "        \"RescaleEnergyEtc\",\n",
    "    ],\n",
    "    dataset_statistics_stride=1,\n",
    "    device='cpu',\n",
    "    default_dtype=\"float64\",\n",
    "    model_dtype=\"float32\",\n",
    "    allow_tf32=True,\n",
    "    verbose=\"INFO\",\n",
    "    model_debug_mode=False,\n",
    "    equivariance_test=False,\n",
    "    grad_anomaly_mode=False,\n",
    "    gpu_oom_offload=False,\n",
    "    append=False,\n",
    "    warn_unused=False,\n",
    "    _jit_bailout_depth=2,  # avoid 20 iters of pain, see https://github.com/pytorch/pytorch/issues/52286\n",
    "    # Quote from eelison in PyTorch slack:\n",
    "    # https://pytorch.slack.com/archives/CDZD1FANA/p1644259272007529?thread_ts=1644064449.039479&cid=CDZD1FANA\n",
    "    # > Right now the default behavior is to specialize twice on static shapes and then on dynamic shapes.\n",
    "    # > To reduce warmup time you can do something like setFusionStrartegy({{FusionBehavior::DYNAMIC, 3}})\n",
    "    # > ... Although we would wouldn't really expect to recompile a dynamic shape fusion in a model,\n",
    "    # > provided broadcasting patterns remain fixed\n",
    "    # We default to DYNAMIC alone because the number of edges is always dynamic,\n",
    "    # even if the number of atoms is fixed:\n",
    "    _jit_fusion_strategy=[(\"DYNAMIC\", 3)],\n",
    "    # Due to what appear to be ongoing bugs with nvFuser, we default to NNC (fuser1) for now:\n",
    "    # TODO: still default to NNC on CPU regardless even if change this for GPU\n",
    "    # TODO: default for ROCm?\n",
    "    _jit_fuser=\"fuser1\",\n",
    ")\n",
    "\n",
    "# All default_config keys are valid / requested\n",
    "#_GLOBAL_ALL_ASKED_FOR_KEYS.update(default_config.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09bac80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtomicData(atom_types=[21, 1], cell=[3, 3], edge_cell_shift=[364, 3], edge_index=[2, 364], forces=[21, 3], pbc=[3], pos=[21, 3], total_energy=[1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config.from_file('./configs/example_ETN.yaml', defaults=default_config)\n",
    "    \n",
    "\n",
    "dataset = dataset_from_config(config, prefix=\"dataset\")\n",
    "\n",
    "validation_dataset = None\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65e9c080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:* Initialize Output\n",
      "  ...generate file name results/aspirin/example/log\n",
      "  ...open log file results/aspirin/example/log\n",
      "  ...generate file name results/aspirin/example/metrics_epoch.csv\n",
      "  ...open log file results/aspirin/example/metrics_epoch.csv\n",
      "  ...generate file name results/aspirin/example/metrics_initialization.csv\n",
      "  ...open log file results/aspirin/example/metrics_initialization.csv\n",
      "  ...generate file name results/aspirin/example/metrics_batch_train.csv\n",
      "  ...open log file results/aspirin/example/metrics_batch_train.csv\n",
      "  ...generate file name results/aspirin/example/metrics_batch_val.csv\n",
      "  ...open log file results/aspirin/example/metrics_batch_val.csv\n",
      "  ...generate file name results/aspirin/example/best_model.pth\n",
      "  ...generate file name results/aspirin/example/last_model.pth\n",
      "  ...generate file name results/aspirin/example/trainer.pth\n",
      "  ...generate file name results/aspirin/example/config.yaml\n",
      "Torch device: cpu\n",
      "instantiate Loss\n",
      "...Loss_param = dict(\n",
      "...   optional_args = {'coeff_schedule': 'constant'},\n",
      "...   positional_args = {'coeffs': {'forces': 1.0, 'total_energy': [1.0, 'PerAtomMSELoss']}})\n",
      "instantiate MSELoss\n",
      "...MSELoss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      " parsing forces 1.0\n",
      " parsing 1.0 MSELoss\n",
      "instantiate MSELoss\n",
      "...MSELoss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      " parsing total_energy [1.0, 'PerAtomMSELoss']\n",
      " parsing 1.0 PerAtomMSELoss\n",
      "create loss instance <class 'nequip.train._loss.PerAtomLoss'>\n",
      "instantiate MSELoss\n",
      "...MSELoss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      "Building ETN model...\n",
      "instantiate PairTypeEmbedding\n",
      "        all_args :                                           num_types\n",
      "...PairTypeEmbedding_param = dict(\n",
      "...   optional_args = {'num_types': 3},\n",
      "...   positional_args = {'irreps_in': None})\n",
      "instantiate OneHotAtomEncoding\n",
      "        all_args :                                           num_types\n",
      "...OneHotAtomEncoding_param = dict(\n",
      "...   optional_args = {'set_features': True, 'num_types': 3},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee}})\n",
      "instantiate RadialBasisEdgeEncoding\n",
      "        all_args :                                  basis_kwargs.r_max <-                                              r_max\n",
      "        all_args :            basis_kwargs.original_basis_kwargs.r_max <-                                              r_max\n",
      "        all_args :        basis_kwargs.original_basis_kwargs.trainable <-                              BesselBasis_trainable\n",
      "        all_args :                                 cutoff_kwargs.r_max <-                                              r_max\n",
      "        all_args :                                     cutoff_kwargs.p <-                                 PolynomialCutoff_p\n",
      "   optional_args :                                           out_field\n",
      "   optional_args :                                               basis\n",
      "...RadialBasisEdgeEncoding_param = dict(\n",
      "...   optional_args = {'basis': <class 'allegro.nn._norm_basis.NormalizedBasis'>, 'cutoff': <class 'nequip.nn.cutoffs.PolynomialCutoff'>, 'basis_kwargs': {'r_min': 0.0, 'original_basis': <class 'nequip.nn.radial_basis.BesselBasis'>, 'original_basis_kwargs': {'num_basis': 8, 'trainable': True, 'r_max': 6.0}, 'n': 4000, 'norm_basis_mean_shift': True, 'r_max': 6.0}, 'cutoff_kwargs': {'p': 6, 'r_max': 6.0}, 'out_field': 'edge_embedding'},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee, 'node_attrs': 3x0ee, 'node_features': 3x0ee}})\n",
      "instantiate SphericalHarmonicEdgeAttrs\n",
      "        all_args :                                      irreps_edge_sh\n",
      "...SphericalHarmonicEdgeAttrs_param = dict(\n",
      "...   optional_args = {'edge_sh_normalization': 'component', 'edge_sh_normalize': True, 'out_field': 'edge_attrs', 'irreps_edge_sh': '1x0ee+1x1oe+1x2ee'},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee, 'node_attrs': 3x0ee, 'node_features': 3x0ee, 'edge_embedding': 8x0ee, 'edge_cutoff': 1x0ee}})\n",
      "instantiate EdgeFeatures_F\n",
      "        all_args :                                           num_types\n",
      "   optional_args :                                                  Nc\n",
      "   optional_args :                                         N_rank_spec\n",
      "   optional_args :                                           out_field\n",
      "...EdgeFeatures_F_param = dict(\n",
      "...   optional_args = {'num_basis': 8, 'N_rank_spec': 4, 'out_field': 'edge_features_F', 'Nc': 10, 'num_types': 3},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee, 'node_attrs': 3x0ee, 'node_features': 3x0ee, 'edge_embedding': 8x0ee, 'edge_cutoff': 1x0ee, 'edge_attrs': 1x0ee+1x1oe+1x2ee}})\n",
      "instantiate EdgewiseFSum\n",
      "        all_args :                                   avg_num_neighbors\n",
      "        all_args :                                           num_types\n",
      "...EdgewiseFSum_param = dict(\n",
      "...   optional_args = {'avg_num_neighbors': 17.211328506469727, 'normalize_edge_features_f': True, 'per_edge_species_scale': False, 'num_types': 3},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee, 'node_attrs': 3x0ee, 'node_features': 3x0ee, 'edge_embedding': 8x0ee, 'edge_cutoff': 1x0ee, 'edge_attrs': 1x0ee+1x1oe+1x2ee, 'edge_features_F': 10x0ee+10x1oe+10x2ee}})\n",
      "instantiate ETN_Module\n",
      "   optional_args :                                           out_field\n",
      "   optional_args :                                                   d\n",
      "   optional_args :                                          N_rank_ett\n",
      "...ETN_Module_param = dict(\n",
      "...   optional_args = {'out_field': 'atomic_energy', 'd': 4, 'N_rank_ett': [4, 4, 4]},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee, 'node_attrs': 3x0ee, 'node_features': 3x0ee, 'edge_embedding': 8x0ee, 'edge_cutoff': 1x0ee, 'edge_attrs': 1x0ee+1x1oe+1x2ee, 'edge_features_F': 10x0ee+10x1oe+10x2ee, 'node_features_F': 10x0ee+10x1oe+10x2ee}})\n",
      "instantiate AtomwiseReduce\n",
      "   optional_args :                                           out_field\n",
      "   optional_args :                                              reduce\n",
      "   optional_args :                                               field\n",
      "...AtomwiseReduce_param = dict(\n",
      "...   optional_args = {'out_field': 'total_energy', 'reduce': 'sum', 'avg_num_atoms': None, 'field': 'atomic_energy'},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee, 'node_attrs': 3x0ee, 'node_features': 3x0ee, 'edge_embedding': 8x0ee, 'edge_cutoff': 1x0ee, 'edge_attrs': 1x0ee+1x1oe+1x2ee, 'edge_features_F': 10x0ee+10x1oe+10x2ee, 'node_features_F': 10x0ee+10x1oe+10x2ee, 'node_features_ETN': 10x0ee+10x1oe+10x2ee, 'atomic_energy': 1x0ee}})\n",
      "Replace string dataset_forces_rms to 31.252248764038086\n",
      "Replace string dataset_per_atom_total_energy_mean to -19318.35546875\n",
      "Atomic outputs are scaled by: [H, C, O: 31.252249], shifted by [H, C, O: -19318.355469].\n",
      "instantiate PerSpeciesScaleShift\n",
      "        all_args :                                           num_types\n",
      "        all_args :                                          type_names\n",
      "        all_args :                                       default_dtype\n",
      "   optional_args :                                              scales\n",
      "   optional_args :                                              shifts\n",
      "   optional_args :                                               field\n",
      "   optional_args :                          arguments_in_dataset_units\n",
      "   optional_args :                                           out_field\n",
      "...PerSpeciesScaleShift_param = dict(\n",
      "...   optional_args = {'out_field': 'atomic_energy', 'scales_trainable': False, 'shifts_trainable': False, 'default_dtype': 'float32', 'num_types': 3, 'type_names': ['H', 'C', 'O'], 'field': 'atomic_energy', 'shifts': tensor(-19318.3555), 'scales': tensor(31.2522), 'arguments_in_dataset_units': True},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee, 'node_attrs': 3x0ee, 'node_features': 3x0ee, 'edge_embedding': 8x0ee, 'edge_cutoff': 1x0ee, 'edge_attrs': 1x0ee+1x1oe+1x2ee, 'edge_features_F': 10x0ee+10x1oe+10x2ee, 'node_features_F': 10x0ee+10x1oe+10x2ee, 'node_features_ETN': 10x0ee+10x1oe+10x2ee, 'atomic_energy': 1x0ee}})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replace string dataset_forces_rms to 31.252248764038086\n",
      "Initially outputs are globally scaled by: 31.252248764038086, total_energy are globally shifted by None.\n",
      "PerSpeciesScaleShift's arguments were in dataset units; rescaling:\n",
      "  Original scales: [H: 31.252249, C: 31.252249, O: 31.252249] shifts: [H: -19318.355469, C: -19318.355469, O: -19318.355469]\n",
      "  New scales: [H: 1.000000, C: 1.000000, O: 1.000000] shifts: [H: -618.142883, C: -618.142883, O: -618.142883]\n"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "from nequip.train.trainer import Trainer\n",
    "from e3nn import o3\n",
    "\n",
    "trainer = Trainer(model=None, **Config.as_dict(config))\n",
    "\n",
    "# what is this\n",
    "# to update wandb data?\n",
    "config.update(trainer.params)\n",
    "\n",
    "# = Train/test split =\n",
    "trainer.set_dataset(dataset, validation_dataset)\n",
    "\n",
    "# Some hyperparameteres\n",
    "Nc = 10 # number of chennels for F features from ETN paper\n",
    "N_rank_spec = 4 # hidden rank of reduction for type radial tensor\n",
    "config['Nc'] = Nc\n",
    "config['N_rank_spec'] = N_rank_spec\n",
    "\n",
    "# ETN parameters\n",
    "config['d'] = 4 # dimention of the tensor train\n",
    "config['N_rank_ett'] = [4, 4, 4] # ranks of tensor train\n",
    "\n",
    "\n",
    "# = Build model =\n",
    "final_model = model_from_config(\n",
    "    config=config, initialize=True, dataset=trainer.dataset_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c64aa38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "from nequip.data import AtomicData, AtomicDataDict\n",
    "from torch.nn.functional import one_hot\n",
    "from e3nn.nn import FullyConnectedNet\n",
    "from allegro import with_edge_spin_length\n",
    "from allegro import _keys\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "trainer.model = final_model\n",
    "\n",
    "# Test configuration stores as dict of parameters\n",
    "data0 = AtomicData.to_AtomicDataDict(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f984d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "from nequip.data import AtomicData, AtomicDataDict\n",
    "from torch.nn.functional import one_hot\n",
    "from e3nn.nn import FullyConnectedNet\n",
    "    \n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "# forward pass\n",
    "data_new = final_model(data0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322b300",
   "metadata": {},
   "source": [
    "### Main module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b93d6",
   "metadata": {},
   "source": [
    "I'll skip the implementation of feature vector F because it heavily relies on nequip code for neighbor lists operations.\n",
    "If you want you can look into the following files or ask me to add it to the notebook.\n",
    "\n",
    "Otherwise look into this file\n",
    "\n",
    "allegro/modules/ETN.py \n",
    "\n",
    "and corresponding layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b02739dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Tuple\n",
    "import math\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch_runstats.scatter import scatter\n",
    "\n",
    "from e3nn import o3\n",
    "from e3nn.util.jit import compile_mode\n",
    "\n",
    "from nequip.data import AtomicDataDict\n",
    "from nequip.nn import GraphModuleMixin\n",
    "from nequip.utils.tp_utils import tp_path_exists\n",
    "\n",
    "from allegro.nn._fc import ScalarMLPFunction\n",
    "from allegro import _keys\n",
    "\n",
    "from allegro.nn._strided import Contracter_ETN\n",
    "from allegro.nn.cutoffs import cosine_cutoff, polynomial_cutoff\n",
    "from e3nn.o3 import wigner_3j\n",
    "\n",
    "\n",
    "\n",
    "# Triangular ineguality for path existance\n",
    "def tri_ineq(l1, l2, l3):\n",
    "    return max([l1, l2, l3]) <= min([l1 + l2, l2 + l3, l1 + l3])\n",
    "\n",
    "\n",
    "@compile_mode(\"script\")\n",
    "class ETN_Module_opt(nn.Module, GraphModuleMixin):\n",
    "    def __init__(self,\n",
    "                 d: int,\n",
    "                 N_rank_ett: List[int], \n",
    "                 irreps_in=None,\n",
    "                 out_field: str = AtomicDataDict.PER_ATOM_ENERGY_KEY):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.out_field = out_field\n",
    "        \n",
    "        \n",
    "        self.d = d\n",
    "        self.Nc = irreps_in[_keys.NODE_FEATURES_F][0][0]\n",
    "        self.register_buffer(\"N_rank_ett\", torch.as_tensor(N_rank_ett, dtype=torch.long))\n",
    "        \n",
    "        # set up irreps\n",
    "        self._init_irreps(\n",
    "            irreps_in=irreps_in,\n",
    "            required_irreps_in=[\n",
    "                _keys.NODE_FEATURES_F\n",
    "            ],\n",
    "            irreps_out={_keys.NODE_FEATURES_ETN: o3.Irreps(\n",
    "                    [(self.Nc, ir) for _, ir in irreps_in[_keys.NODE_FEATURES_F] ]),\n",
    "                        out_field: o3.Irreps([(1, (0, 1))])}\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Parameters of the network\n",
    "        \n",
    "        # tensors for atomic features encoding\n",
    "        lmax = irreps_in[_keys.EDGE_FEATURES_F].lmax # maximum spherical harmonic\n",
    "        self.lmax = lmax\n",
    "        \n",
    "        # Second order cores(first and last)\n",
    "        self.core2_1 = torch.nn.Parameter(torch.Tensor(lmax+1, self.Nc, N_rank_ett[0]))\n",
    "        self.core2_d = torch.nn.Parameter(torch.Tensor(lmax+1, N_rank_ett[-1], self.Nc))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Third order cores\n",
    "        # Assume irreps does not change \n",
    "        base_in1 = o3.Irreps([el[1] for el in irreps_in[_keys.EDGE_FEATURES_F]])\n",
    "        base_in2 = o3.Irreps([el[1] for el in irreps_in[_keys.EDGE_FEATURES_F]])\n",
    "        base_out = o3.Irreps([el[1] for el in irreps_in[_keys.EDGE_FEATURES_F]])\n",
    "        \n",
    "\n",
    "        # Building instructions\n",
    "        instructions: List[Tuple[int, int, int]] = []\n",
    "        tmp_i_out: int = 0\n",
    "        for i_out, (_, ir_out) in enumerate(base_out):\n",
    "            for i_1, (_, ir_in1) in enumerate(base_in1):\n",
    "                for i_2, (_, ir_in2) in enumerate(base_in2):\n",
    "                    if ir_out in ir_in1 * ir_in2:\n",
    "                        instructions.append((i_1, i_2, i_out))\n",
    "        \n",
    "                        tmp_i_out += 1\n",
    "\n",
    "        self.instructions = instructions\n",
    "        \n",
    "        # building large w3j\n",
    "        w3j_values = []\n",
    "        w3j_index = []\n",
    "        for i_in1, i_in2, i_out in instructions:\n",
    "            mul_ir_in1 = base_in1[i_in1]\n",
    "            mul_ir_in2 = base_in2[i_in2]\n",
    "            mul_ir_out = base_out[i_out]\n",
    "    \n",
    "            assert mul_ir_in1.ir.p * mul_ir_in2.ir.p == mul_ir_out.ir.p\n",
    "            assert (\n",
    "                tri_ineq(mul_ir_in1.ir.l, mul_ir_in2.ir.l, mul_ir_out.ir.l)\n",
    "            )\n",
    "    \n",
    "            if mul_ir_in1.dim == 0 or mul_ir_in2.dim == 0 or mul_ir_out.dim == 0:\n",
    "                raise ValueError\n",
    "    \n",
    "            this_w3j = o3.wigner_3j(mul_ir_in1.ir.l, mul_ir_in2.ir.l, mul_ir_out.ir.l)\n",
    "            this_w3j_index = this_w3j.nonzero()\n",
    "            w3j_values.append(\n",
    "                this_w3j[this_w3j_index[:, 0], this_w3j_index[:, 1], this_w3j_index[:, 2]]\n",
    "            )\n",
    "    \n",
    "            \n",
    "            this_w3j_index[:, 0] += base_in1[: i_in1].dim\n",
    "            this_w3j_index[:, 1] += base_in2[: i_in2].dim\n",
    "            this_w3j_index[:, 2] += base_out[: i_out].dim\n",
    "            # Now need to flatten the index to be for [pk][ij]\n",
    "            w3j_index.append(\n",
    "                torch.cat(\n",
    "                    (   this_w3j_index[:, 2].unsqueeze(-1),\n",
    "                        this_w3j_index[:, 0].unsqueeze(-1) * base_in2.dim\n",
    "                        + this_w3j_index[:, 1].unsqueeze(-1),\n",
    "                    ),\n",
    "                    dim=1,\n",
    "                )\n",
    "            )\n",
    "    \n",
    "        num_paths: int = len(instructions)\n",
    "    \n",
    "        w3j = torch.sparse_coo_tensor(\n",
    "            indices=torch.cat(w3j_index, dim=0).t(),\n",
    "            values=torch.cat(w3j_values, dim=0),\n",
    "            size=(\n",
    "                num_paths * base_out.dim,\n",
    "                base_in1.dim * base_in2.dim,\n",
    "            ),\n",
    "        ).coalesce()\n",
    "        \n",
    "        # in dense, must shape it for einsum:\n",
    "        kij_shape = (\n",
    "            base_out.dim,\n",
    "            base_in1.dim,\n",
    "            base_in2.dim,\n",
    "        )\n",
    "        \n",
    "        # save to buffer in sparce mode + shape\n",
    "        self.register_buffer(\"w3j\", w3j)\n",
    "        self.w3j_shape = (num_paths, ) + kij_shape\n",
    "        \n",
    "        # third order free parameters\n",
    "        self.cores3 = [torch.nn.Parameter(torch.Tensor(N_rank_ett[r], self.Nc, N_rank_ett[r+1], num_paths)).to(w3j.device) for r in range(d - 2)] \n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "        print(w3j.device)\n",
    "        # Register layers\n",
    "        self.tps = [Contracter_ETN(base_in1, \n",
    "                                   N_rank_ett[r], \n",
    "                                   base_in2, \n",
    "                                   self.Nc, \n",
    "                                   base_out, \n",
    "                                   N_rank_ett[r+1], \n",
    "                                   num_paths) for r in range(self.d - 2)]\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, data: AtomicDataDict.Type) -> AtomicDataDict.Type:\n",
    "        \n",
    "        # Input features\n",
    "        F = data[_keys.NODE_FEATURES_F]\n",
    "        \n",
    "        # Defining tensors for TorchScript\n",
    "        u_out = torch.zeros((F.shape[0], F.shape[1], self.N_rank_ett[-1]), dtype=F.dtype,\n",
    "            device=F.device) # temporary verctor output of etn\n",
    "            \n",
    "        \n",
    "        data[_keys.NODE_FEATURES_ETN] = torch.zeros_like(F, dtype=F.dtype,\n",
    "            device=F.device) # final feature output\n",
    "        \n",
    "        slices = self.irreps_in[AtomicDataDict.EDGE_ATTRS_KEY].slices() # slices over irreps\n",
    "\n",
    "        # getting w3j in dense mode\n",
    "        w3j_dense = (\n",
    "            self.w3j.to_dense()\n",
    "            .reshape(self.w3j_shape)\n",
    "            .contiguous()\n",
    "        )   \n",
    "        \n",
    "        # First transform using second order tensors\n",
    "        for i, slice in enumerate(slices):\n",
    "            u_out[:, slice, :] = torch.einsum('ij,Nmj->Nmi', self.core2_d[i], F[:, slice, :])\n",
    "\n",
    "        # Series third order tensors\n",
    "        for i in range(self.d - 2 - 1, -1, -1):\n",
    "\n",
    "            # big contruction\n",
    "            u_out = self.tps[i](u_out, F, w3j_dense, self.cores3[i].to(F.device))\n",
    "\n",
    "        # Last transform using second order tensor\n",
    "        for i, slice in enumerate(slices):\n",
    "            data[_keys.NODE_FEATURES_ETN][:, slice, :] = torch.einsum('ij,Nmj->Nmi', self.core2_1[i], u_out[:, slice, :])\n",
    "        \n",
    "        \n",
    "        # Reduction to scalar\n",
    "        data[self.out_field] = (( data[_keys.NODE_FEATURES_ETN] * F ).sum(dim = (-2, -1) )).unsqueeze(-1)\n",
    "        \n",
    "\n",
    "        return data\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_uniform_(self.core2_1, a=math.sqrt(3))\n",
    "        torch.nn.init.kaiming_uniform_(self.core2_d, a=math.sqrt(3))\n",
    "        \n",
    "        for i in range(len(self.cores3)):\n",
    "            torch.nn.init.kaiming_uniform_(self.cores3[i], a=math.sqrt(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1c8ee1",
   "metadata": {},
   "source": [
    "### Contruction of order 3 tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "599467d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nc = [10, 10, 10, 10]\n",
    "rank = [1, 4, 4, 4, 1]\n",
    "d = 4\n",
    "\n",
    "base_in1 = o3.Irreps('1x0e + 1x1o + 2x2e')\n",
    "base_in2 = o3.Irreps('1x0e + 1x1o + 2x2e')\n",
    "base_out = o3.Irreps('1x0e + 1x1o + 2x2e')\n",
    "\n",
    "# Building instructions\n",
    "instructions: List[Tuple[int, int, int]] = []\n",
    "tmp_i_out: int = 0\n",
    "for i_out, (_, ir_out) in enumerate(base_out):\n",
    "    for i_1, (_, ir_in1) in enumerate(base_in1):\n",
    "        for i_2, (_, ir_in2) in enumerate(base_in2):\n",
    "            if ir_out in ir_in1 * ir_in2:\n",
    "                instructions.append((i_1, i_2, i_out))\n",
    "\n",
    "                tmp_i_out += 1\n",
    "\n",
    "\n",
    "# building large w3j\n",
    "w3j_values = []\n",
    "w3j_index = []\n",
    "for i_in1, i_in2, i_out in instructions:\n",
    "    mul_ir_in1 = base_in1[i_in1]\n",
    "    mul_ir_in2 = base_in2[i_in2]\n",
    "    mul_ir_out = base_out[i_out]\n",
    "\n",
    "    assert mul_ir_in1.ir.p * mul_ir_in2.ir.p == mul_ir_out.ir.p\n",
    "    assert (\n",
    "        tri_ineq(mul_ir_in1.ir.l, mul_ir_in2.ir.l, mul_ir_out.ir.l)\n",
    "    )\n",
    "\n",
    "    if mul_ir_in1.dim == 0 or mul_ir_in2.dim == 0 or mul_ir_out.dim == 0:\n",
    "        raise ValueError\n",
    "\n",
    "    this_w3j = o3.wigner_3j(mul_ir_in1.ir.l, mul_ir_in2.ir.l, mul_ir_out.ir.l)\n",
    "    this_w3j_index = this_w3j.nonzero()\n",
    "    w3j_values.append(\n",
    "        this_w3j[this_w3j_index[:, 0], this_w3j_index[:, 1], this_w3j_index[:, 2]]\n",
    "    )\n",
    "\n",
    "\n",
    "    this_w3j_index[:, 0] += base_in1[: i_in1].dim\n",
    "    this_w3j_index[:, 1] += base_in2[: i_in2].dim\n",
    "    this_w3j_index[:, 2] += base_out[: i_out].dim\n",
    "    # Now need to flatten the index to be for [pk][ij]\n",
    "    w3j_index.append(\n",
    "        torch.cat(\n",
    "            (   this_w3j_index[:, 2].unsqueeze(-1),\n",
    "                this_w3j_index[:, 0].unsqueeze(-1) * base_in2.dim\n",
    "                + this_w3j_index[:, 1].unsqueeze(-1),\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "num_paths: int = len(instructions)\n",
    "\n",
    "w3j = torch.sparse_coo_tensor(\n",
    "    indices=torch.cat(w3j_index, dim=0).t(),\n",
    "    values=torch.cat(w3j_values, dim=0),\n",
    "    size=(\n",
    "        num_paths * base_out.dim,\n",
    "        base_in1.dim * base_in2.dim,\n",
    "    ),\n",
    ").coalesce()\n",
    "\n",
    "# in dense, must shape it for einsum:\n",
    "kij_shape = (\n",
    "    base_out.dim,\n",
    "    base_in1.dim,\n",
    "    base_in2.dim,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "01138fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cores3 = [torch.randn(num_paths, rank[r], Nc[r], rank[r+1]) for r in range(d)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "acc73481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 1, 10, 4])\n",
      "torch.Size([11, 4, 10, 4])\n",
      "torch.Size([11, 4, 10, 4])\n",
      "torch.Size([11, 4, 10, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(el.shape) for el in cores3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b63a537e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 0),\n",
       " (1, 1, 0),\n",
       " (2, 2, 0),\n",
       " (0, 1, 1),\n",
       " (1, 0, 1),\n",
       " (1, 2, 1),\n",
       " (2, 1, 1),\n",
       " (0, 2, 2),\n",
       " (1, 1, 2),\n",
       " (2, 0, 2),\n",
       " (2, 2, 2)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "16649a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left orthogonalize cores 1, 2\n",
    "\n",
    "l = 5\n",
    "\n",
    "instr_left = [el for i, el in enumerate(instructions) if el[-1] == 2]\n",
    "\n",
    "ind_left = [i for i, el in enumerate(instructions) if el[-1] == 2]\n",
    "ind_right = [i for i, el in enumerate(instructions) if el[0] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3b404275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([160, 4]), torch.Size([4, 160]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supercore_left = cores3[1][ind_left].flatten(0, -2)\n",
    "supercore_right =torch.stack([cores3[2][ind, :, ...] for ind in ind_right], dim = -3).flatten(1)\n",
    "\n",
    "supercore_left.shape, supercore_right.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dcff579f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 6, 9, 10]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instr_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1c44b246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([160, 4]), torch.Size([4, 4]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q, R = torch.linalg.qr(supercore_left)\n",
    "\n",
    "Q.shape, R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8806338c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00,  1.9558e-08, -1.3039e-08,  7.4506e-09],\n",
       "        [ 1.9558e-08,  1.0000e+00,  1.4901e-08,  2.9802e-08],\n",
       "        [-1.3039e-08,  1.4901e-08,  1.0000e+00, -6.5193e-08],\n",
       "        [ 7.4506e-09,  2.9802e-08, -6.5193e-08,  1.0000e+00]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.T @ Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "de1126c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 10, 4])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = Q.view((len(ind_left), 4, 10, 4))\n",
    "\n",
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0c27da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e3nn.o3 import wigner_3j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2d927718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 2)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ins = instr_left[3]\n",
    "\n",
    "ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e521ef46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 3.1922e-09],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 3.1922e-09, 0.0000e+00, 1.0000e+00]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ijk,ijl->kl', wigner_3j(*ins), wigner_3j(*ins))*(2*l + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc5131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_mkl",
   "language": "python",
   "name": "torch_mkl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
