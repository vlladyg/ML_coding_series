{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc7a2ba3",
   "metadata": {},
   "source": [
    "### First few cells is just data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b139a9f2",
   "metadata": {},
   "source": [
    "My implementation is build on top of nequip and allegro python libraries to simplify data load and preprocessing.\n",
    "The wigner simbols are from e3nn\n",
    "Here are the links\n",
    "https://github.com/mir-group/nequip\n",
    "https://github.com/mir-group/allegro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba17ba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference example\n",
    "from nequip.data import dataset_from_config\n",
    "from nequip.utils import Config\n",
    "#from nequip.utils.misc import get_default_device_name\n",
    "#from nequip.utils.config import _GLOBAL_ALL_ASKED_FOR_KEYS\n",
    "\n",
    "from nequip.model import model_from_config\n",
    "\n",
    "\n",
    "default_config = dict(\n",
    "    root=\"./\",\n",
    "    tensorboard=False,\n",
    "    wandb=False,\n",
    "    model_builders=[\n",
    "        \"SimpleIrrepsConfig\",\n",
    "        \"EnergyModel\",\n",
    "        \"PerSpeciesRescale\",\n",
    "        \"StressForceOutput\",\n",
    "        \"RescaleEnergyEtc\",\n",
    "    ],\n",
    "    dataset_statistics_stride=1,\n",
    "    device='cpu',\n",
    "    default_dtype=\"float64\",\n",
    "    model_dtype=\"float32\",\n",
    "    allow_tf32=True,\n",
    "    verbose=\"INFO\",\n",
    "    model_debug_mode=False,\n",
    "    equivariance_test=False,\n",
    "    grad_anomaly_mode=False,\n",
    "    gpu_oom_offload=False,\n",
    "    append=False,\n",
    "    warn_unused=False,\n",
    "    _jit_bailout_depth=2,  # avoid 20 iters of pain, see https://github.com/pytorch/pytorch/issues/52286\n",
    "    # Quote from eelison in PyTorch slack:\n",
    "    # https://pytorch.slack.com/archives/CDZD1FANA/p1644259272007529?thread_ts=1644064449.039479&cid=CDZD1FANA\n",
    "    # > Right now the default behavior is to specialize twice on static shapes and then on dynamic shapes.\n",
    "    # > To reduce warmup time you can do something like setFusionStrartegy({{FusionBehavior::DYNAMIC, 3}})\n",
    "    # > ... Although we would wouldn't really expect to recompile a dynamic shape fusion in a model,\n",
    "    # > provided broadcasting patterns remain fixed\n",
    "    # We default to DYNAMIC alone because the number of edges is always dynamic,\n",
    "    # even if the number of atoms is fixed:\n",
    "    _jit_fusion_strategy=[(\"DYNAMIC\", 3)],\n",
    "    # Due to what appear to be ongoing bugs with nvFuser, we default to NNC (fuser1) for now:\n",
    "    # TODO: still default to NNC on CPU regardless even if change this for GPU\n",
    "    # TODO: default for ROCm?\n",
    "    _jit_fuser=\"fuser1\",\n",
    ")\n",
    "\n",
    "# All default_config keys are valid / requested\n",
    "#_GLOBAL_ALL_ASKED_FOR_KEYS.update(default_config.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09bac80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtomicData(atom_types=[21, 1], cell=[3, 3], edge_cell_shift=[364, 3], edge_index=[2, 364], forces=[21, 3], pbc=[3], pos=[21, 3], total_energy=[1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config.from_file('./configs/example_ETN.yaml', defaults=default_config)\n",
    "    \n",
    "\n",
    "dataset = dataset_from_config(config, prefix=\"dataset\")\n",
    "\n",
    "validation_dataset = None\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65e9c080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:* Initialize Output\n",
      "  ...generate file name results/aspirin/example/log\n",
      "  ...open log file results/aspirin/example/log\n",
      "  ...generate file name results/aspirin/example/metrics_epoch.csv\n",
      "  ...open log file results/aspirin/example/metrics_epoch.csv\n",
      "  ...generate file name results/aspirin/example/metrics_initialization.csv\n",
      "  ...open log file results/aspirin/example/metrics_initialization.csv\n",
      "  ...generate file name results/aspirin/example/metrics_batch_train.csv\n",
      "  ...open log file results/aspirin/example/metrics_batch_train.csv\n",
      "  ...generate file name results/aspirin/example/metrics_batch_val.csv\n",
      "  ...open log file results/aspirin/example/metrics_batch_val.csv\n",
      "  ...generate file name results/aspirin/example/best_model.pth\n",
      "  ...generate file name results/aspirin/example/last_model.pth\n",
      "  ...generate file name results/aspirin/example/trainer.pth\n",
      "  ...generate file name results/aspirin/example/config.yaml\n",
      "Torch device: cpu\n",
      "instantiate Loss\n",
      "...Loss_param = dict(\n",
      "...   optional_args = {'coeff_schedule': 'constant'},\n",
      "...   positional_args = {'coeffs': {'forces': 1.0, 'total_energy': [1.0, 'PerAtomMSELoss']}})\n",
      "instantiate MSELoss\n",
      "...MSELoss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      " parsing forces 1.0\n",
      " parsing 1.0 MSELoss\n",
      "instantiate MSELoss\n",
      "...MSELoss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      " parsing total_energy [1.0, 'PerAtomMSELoss']\n",
      " parsing 1.0 PerAtomMSELoss\n",
      "create loss instance <class 'nequip.train._loss.PerAtomLoss'>\n",
      "instantiate MSELoss\n",
      "...MSELoss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      "Building ETN model...\n",
      "instantiate PairTypeEmbedding\n",
      "        all_args :                                           num_types\n",
      "...PairTypeEmbedding_param = dict(\n",
      "...   optional_args = {'num_types': 3},\n",
      "...   positional_args = {'irreps_in': None})\n",
      "instantiate OneHotAtomEncoding\n",
      "        all_args :                                           num_types\n",
      "...OneHotAtomEncoding_param = dict(\n",
      "...   optional_args = {'set_features': True, 'num_types': 3},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee}})\n",
      "instantiate RadialBasisEdgeEncoding\n",
      "        all_args :                                  basis_kwargs.r_max <-                                              r_max\n",
      "        all_args :            basis_kwargs.original_basis_kwargs.r_max <-                                              r_max\n",
      "        all_args :        basis_kwargs.original_basis_kwargs.trainable <-                              BesselBasis_trainable\n",
      "        all_args :                                 cutoff_kwargs.r_max <-                                              r_max\n",
      "        all_args :                                     cutoff_kwargs.p <-                                 PolynomialCutoff_p\n",
      "   optional_args :                                           out_field\n",
      "   optional_args :                                               basis\n",
      "...RadialBasisEdgeEncoding_param = dict(\n",
      "...   optional_args = {'basis': <class 'allegro.nn._norm_basis.NormalizedBasis'>, 'cutoff': <class 'nequip.nn.cutoffs.PolynomialCutoff'>, 'basis_kwargs': {'r_min': 0.0, 'original_basis': <class 'nequip.nn.radial_basis.BesselBasis'>, 'original_basis_kwargs': {'num_basis': 8, 'trainable': True, 'r_max': 6.0}, 'n': 4000, 'norm_basis_mean_shift': True, 'r_max': 6.0}, 'cutoff_kwargs': {'p': 6, 'r_max': 6.0}, 'out_field': 'edge_embedding'},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee, 'node_attrs': 3x0ee, 'node_features': 3x0ee}})\n",
      "instantiate SphericalHarmonicEdgeAttrs\n",
      "        all_args :                                      irreps_edge_sh\n",
      "...SphericalHarmonicEdgeAttrs_param = dict(\n",
      "...   optional_args = {'edge_sh_normalization': 'component', 'edge_sh_normalize': True, 'out_field': 'edge_attrs', 'irreps_edge_sh': '1x0ee+1x1oe+1x2ee'},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee, 'node_attrs': 3x0ee, 'node_features': 3x0ee, 'edge_embedding': 8x0ee, 'edge_cutoff': 1x0ee}})\n",
      "instantiate EdgeFeatures_F\n",
      "        all_args :                                           num_types\n",
      "   optional_args :                                                  Nc\n",
      "   optional_args :                                         N_rank_spec\n",
      "   optional_args :                                           out_field\n",
      "...EdgeFeatures_F_param = dict(\n",
      "...   optional_args = {'num_basis': 8, 'N_rank_spec': 4, 'out_field': 'edge_features_F', 'Nc': 10, 'num_types': 3},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee, 'node_attrs': 3x0ee, 'node_features': 3x0ee, 'edge_embedding': 8x0ee, 'edge_cutoff': 1x0ee, 'edge_attrs': 1x0ee+1x1oe+1x2ee}})\n",
      "instantiate EdgewiseFSum\n",
      "        all_args :                                   avg_num_neighbors\n",
      "        all_args :                                           num_types\n",
      "...EdgewiseFSum_param = dict(\n",
      "...   optional_args = {'avg_num_neighbors': 17.211328506469727, 'normalize_edge_features_f': True, 'per_edge_species_scale': False, 'num_types': 3},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee, 'node_attrs': 3x0ee, 'node_features': 3x0ee, 'edge_embedding': 8x0ee, 'edge_cutoff': 1x0ee, 'edge_attrs': 1x0ee+1x1oe+1x2ee, 'edge_features_F': 10x0ee+10x1oe+10x2ee}})\n",
      "instantiate ETN_Module\n",
      "   optional_args :                                                   d\n",
      "   optional_args :                                           out_field\n",
      "   optional_args :                                          N_rank_ett\n",
      "...ETN_Module_param = dict(\n",
      "...   optional_args = {'out_field': 'atomic_energy', 'd': 4, 'N_rank_ett': [4, 4, 4]},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee, 'node_attrs': 3x0ee, 'node_features': 3x0ee, 'edge_embedding': 8x0ee, 'edge_cutoff': 1x0ee, 'edge_attrs': 1x0ee+1x1oe+1x2ee, 'edge_features_F': 10x0ee+10x1oe+10x2ee, 'node_features_F': 10x0ee+10x1oe+10x2ee}})\n",
      "instantiate AtomwiseReduce\n",
      "   optional_args :                                           out_field\n",
      "   optional_args :                                              reduce\n",
      "   optional_args :                                               field\n",
      "...AtomwiseReduce_param = dict(\n",
      "...   optional_args = {'out_field': 'total_energy', 'reduce': 'sum', 'avg_num_atoms': None, 'field': 'atomic_energy'},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee, 'node_attrs': 3x0ee, 'node_features': 3x0ee, 'edge_embedding': 8x0ee, 'edge_cutoff': 1x0ee, 'edge_attrs': 1x0ee+1x1oe+1x2ee, 'edge_features_F': 10x0ee+10x1oe+10x2ee, 'node_features_F': 10x0ee+10x1oe+10x2ee, 'node_features_ETN': 10x0ee+10x1oe+10x2ee, 'atomic_energy': 1x0ee}})\n",
      "Replace string dataset_forces_rms to 31.252248764038086\n",
      "Replace string dataset_per_atom_total_energy_mean to -19318.35546875\n",
      "Atomic outputs are scaled by: [H, C, O: 31.252249], shifted by [H, C, O: -19318.355469].\n",
      "instantiate PerSpeciesScaleShift\n",
      "        all_args :                                       default_dtype\n",
      "        all_args :                                          type_names\n",
      "        all_args :                                           num_types\n",
      "   optional_args :                          arguments_in_dataset_units\n",
      "   optional_args :                                              shifts\n",
      "   optional_args :                                           out_field\n",
      "   optional_args :                                               field\n",
      "   optional_args :                                              scales\n",
      "...PerSpeciesScaleShift_param = dict(\n",
      "...   optional_args = {'out_field': 'atomic_energy', 'scales_trainable': False, 'shifts_trainable': False, 'default_dtype': 'float32', 'num_types': 3, 'type_names': ['H', 'C', 'O'], 'field': 'atomic_energy', 'shifts': tensor(-19318.3555), 'scales': tensor(31.2522), 'arguments_in_dataset_units': True},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee, 'node_attrs': 3x0ee, 'node_features': 3x0ee, 'edge_embedding': 8x0ee, 'edge_cutoff': 1x0ee, 'edge_attrs': 1x0ee+1x1oe+1x2ee, 'edge_features_F': 10x0ee+10x1oe+10x2ee, 'node_features_F': 10x0ee+10x1oe+10x2ee, 'node_features_ETN': 10x0ee+10x1oe+10x2ee, 'atomic_energy': 1x0ee}})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replace string dataset_forces_rms to 31.252248764038086\n",
      "Initially outputs are globally scaled by: 31.252248764038086, total_energy are globally shifted by None.\n",
      "PerSpeciesScaleShift's arguments were in dataset units; rescaling:\n",
      "  Original scales: [H: 31.252249, C: 31.252249, O: 31.252249] shifts: [H: -19318.355469, C: -19318.355469, O: -19318.355469]\n",
      "  New scales: [H: 1.000000, C: 1.000000, O: 1.000000] shifts: [H: -618.142883, C: -618.142883, O: -618.142883]\n"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "from nequip.train.trainer import Trainer\n",
    "from e3nn import o3\n",
    "\n",
    "trainer = Trainer(model=None, **Config.as_dict(config))\n",
    "\n",
    "# what is this\n",
    "# to update wandb data?\n",
    "config.update(trainer.params)\n",
    "\n",
    "# = Train/test split =\n",
    "trainer.set_dataset(dataset, validation_dataset)\n",
    "\n",
    "# Some hyperparameteres\n",
    "Nc = 10 # number of chennels for F features from ETN paper\n",
    "N_rank_spec = 4 # hidden rank of reduction for type radial tensor\n",
    "config['Nc'] = Nc\n",
    "config['N_rank_spec'] = N_rank_spec\n",
    "\n",
    "# ETN parameters\n",
    "config['d'] = 4 # dimention of the tensor train\n",
    "config['N_rank_ett'] = [4, 4, 4] # ranks of tensor train\n",
    "\n",
    "\n",
    "# = Build model =\n",
    "final_model = model_from_config(\n",
    "    config=config, initialize=True, dataset=trainer.dataset_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c64aa38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "from nequip.data import AtomicData, AtomicDataDict\n",
    "from torch.nn.functional import one_hot\n",
    "from e3nn.nn import FullyConnectedNet\n",
    "from allegro import with_edge_spin_length\n",
    "from allegro import _keys\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "trainer.model = final_model\n",
    "\n",
    "# Test configuration stores as dict of parameters\n",
    "data0 = AtomicData.to_AtomicDataDict(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f984d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "from nequip.data import AtomicData, AtomicDataDict\n",
    "from torch.nn.functional import one_hot\n",
    "from e3nn.nn import FullyConnectedNet\n",
    "    \n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "# forward pass\n",
    "data_new = final_model(data0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322b300",
   "metadata": {},
   "source": [
    "### Main module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b93d6",
   "metadata": {},
   "source": [
    "I'll skip the implementation of feature vector F because it heavily relies on nequip code for neighbor lists operations.\n",
    "If you want you can look into the following files or ask me to add it to the notebook.\n",
    "\n",
    "Otherwise look into this file\n",
    "\n",
    "allegro/modules/ETN.py \n",
    "\n",
    "and corresponding layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b02739dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "import math\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from e3nn import o3\n",
    "from e3nn.util.jit import compile_mode\n",
    "\n",
    "from nequip.data import AtomicDataDict # dict of base keys\n",
    "from nequip.nn import GraphModuleMixin # base class for GNN (stores irreps in and out)\n",
    "\n",
    "from allegro import _keys # just dict of additional keys\n",
    "\n",
    "from e3nn.o3 import wigner_3j # wigner_3j matrixes for three spherical harmonics coupling\n",
    "\n",
    "\n",
    "\n",
    "# Triangular ineguality for path existance\n",
    "def tri_ineq(l1, l2, l3):\n",
    "    return max([l1, l2, l3]) <= min([l1 + l2, l2 + l3, l1 + l3])\n",
    "\n",
    "\n",
    "@compile_mode(\"script\")\n",
    "class ETN_Module(nn.Module, GraphModuleMixin):\n",
    "    def __init__(self,\n",
    "                 d: int,\n",
    "                 N_rank_ett: List[int], \n",
    "                 irreps_in=None,\n",
    "                 out_field: str = AtomicDataDict.PER_ATOM_ENERGY_KEY):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.out_field = out_field\n",
    "        \n",
    "        \n",
    "        self.d = d\n",
    "        self.Nc = irreps_in[_keys.NODE_FEATURES_F][0][0]\n",
    "        self.register_buffer(\"N_rank_ett\", torch.as_tensor(N_rank_ett, dtype=torch.long))\n",
    "        \n",
    "        # set up irreps\n",
    "        self._init_irreps(\n",
    "            irreps_in=irreps_in,\n",
    "            required_irreps_in=[\n",
    "                _keys.NODE_FEATURES_F\n",
    "            ],\n",
    "            irreps_out={_keys.NODE_FEATURES_ETN: o3.Irreps(\n",
    "                    [(self.Nc, ir) for _, ir in irreps_in[_keys.NODE_FEATURES_F] ]),\n",
    "                        out_field: o3.Irreps([(1, (0, 1))])}\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Parameters of the network\n",
    "        \n",
    "        # tensors for atomic features encoding\n",
    "        lmax = irreps_in[_keys.EDGE_FEATURES_F].lmax # maximum spherical harmonic\n",
    "        self.lmax = lmax\n",
    "        \n",
    "        # Second order cores(first and last)\n",
    "        self.core2_1 = torch.nn.Parameter(torch.Tensor(lmax+1, self.Nc, N_rank_ett[0]))\n",
    "        self.core2_d = torch.nn.Parameter(torch.Tensor(lmax+1, N_rank_ett[-1], self.Nc))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Third order cores\n",
    "        \n",
    "        # all wigner3j \n",
    "        self.w3j_big = [[[wigner_3j(l1, l2, l3) if tri_ineq(l1, l2, l3) else None for l3 in range (lmax+1)] for l2 in range(lmax+1)] for l1 in range(lmax+1)]\n",
    "        \n",
    "        # all third order free parameters\n",
    "        self.cores3 = [{(l1, l2, l3): torch.nn.Parameter(torch.Tensor(N_rank_ett[r], self.Nc, N_rank_ett[r+1])) for l1 in range (lmax+1) for l2 in range(lmax+1) for l3 in range(lmax+1) if tri_ineq(l1, l2, l3)} for r in range(d - 2)] \n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def forward(self, data: AtomicDataDict.Type) -> AtomicDataDict.Type:\n",
    "        \n",
    "        # Input features\n",
    "        F = data[_keys.NODE_FEATURES_F]\n",
    "        \n",
    "        # Defining tensors for TorchScript\n",
    "        u_out = torch.zeros((F.shape[0], F.shape[1], self.N_rank_ett[-1]), dtype=F.dtype,\n",
    "            device=F.device) # temporary verctor output of etn\n",
    "            \n",
    "        \n",
    "        data[_keys.NODE_FEATURES_ETN] = torch.zeros_like(F, dtype=F.dtype,\n",
    "            device=F.device) # final feature output\n",
    "        \n",
    "        slices = self.irreps_in[AtomicDataDict.EDGE_ATTRS_KEY].slices() # slices over irreps\n",
    "        \n",
    "        # First transform using second order tensors\n",
    "        for i, slice in enumerate(slices):\n",
    "            u_out[:, slice, :] = torch.einsum('ij,Nmj->Nmi', self.core2_d[i], F[:, slice, :])\n",
    "        \n",
    "        # Series third order tensors\n",
    "        for i in range(self.d - 2 - 1, -1, -1):\n",
    "            \n",
    "            # TODO: now define localy, mb define for all, to ensure computational graph\n",
    "            T_2_tmp = [[torch.zeros(F.shape[0], 2*l1+1, 2*l2+1, self.N_rank_ett[i - 1], self.Nc, dtype=F.dtype, device=F.device) for l2 in range(self.lmax + 1)] for l1 in range(self.lmax + 1)] # result of first reduction of order 3 tensor\n",
    "            \n",
    "            # First contraction with previous feature vector\n",
    "            for l1 in range(self.lmax + 1):\n",
    "                for l2 in range(self.lmax + 1):\n",
    "                    for l3, slice in enumerate(slices):\n",
    "                        if tri_ineq(l1, l2, l3):\n",
    "                            T_3 = self.w3j_big[l1][l2][l3][..., None, None, None] * self.cores3[i][(l1, l2, l3)][None, None, None, ...]\n",
    "                            T_2_tmp[l1][l2] += torch.einsum('abcijk,Nck->Nabij', T_3, u_out[:, slice, :])\n",
    "            \n",
    "            \n",
    "            # Second contraction with F vector\n",
    "            u_out_new = torch.zeros((F.shape[0], F.shape[1], self.N_rank_ett[i - 1]), dtype=F.dtype,\n",
    "                            device=F.device) # temporary verctor output of etn\n",
    "            \n",
    "            for l1 in range(self.lmax + 1):    \n",
    "                for l2, slice in enumerate(slices):\n",
    "                    u_out_new[:, slices[l1], :] += torch.einsum('Nabij,Nbj->Nai', T_2_tmp[l1][l2], F[:, slice, :])\n",
    "            \n",
    "            \n",
    "            u_out = u_out_new\n",
    "            \n",
    "        # Last transform using second order tensor\n",
    "        for i, slice in enumerate(slices):\n",
    "            data[_keys.NODE_FEATURES_ETN][:, slice, :] = torch.einsum('ij,Nmj->Nmi', self.core2_1[i], u_out[:, slice, :])\n",
    "        \n",
    "        \n",
    "        # Reduction to scalar\n",
    "        data[self.out_field] = ( data[_keys.NODE_FEATURES_ETN] * F ).sum(dim = (-2, -1))\n",
    "        \n",
    "\n",
    "        return data\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_uniform_(self.core2_1, a=math.sqrt(3))\n",
    "        torch.nn.init.kaiming_uniform_(self.core2_d, a=math.sqrt(3))\n",
    "        \n",
    "        for core in self.cores3:\n",
    "            for key in core:\n",
    "                torch.nn.init.kaiming_uniform_(core[key], a=math.sqrt(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1c8ee1",
   "metadata": {},
   "source": [
    "### Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c3d6905",
   "metadata": {},
   "outputs": [],
   "source": [
    "ETN = ETN_Module(d = config['d'],\n",
    "                         N_rank_ett = config['N_rank_ett'], \n",
    "                         irreps_in = final_model.irreps_out,\n",
    "                         out_field = AtomicDataDict.PER_ATOM_ENERGY_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "093cff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new_new = ETN(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b32c73",
   "metadata": {},
   "source": [
    "### Some equivariance testing of the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9ff115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "from nequip.data import AtomicData, AtomicDataDict\n",
    "from torch.nn.functional import one_hot\n",
    "from e3nn.nn import FullyConnectedNet\n",
    "from allegro import with_edge_spin_length\n",
    "from allegro import _keys\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "data = data0\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "data_rot = {key: torch.clone(data0[key]) for key in data0}\n",
    "\n",
    "irreps_sh = o3.Irreps('1x0e + 1x1o + 1x2e') #o3.Irreps.spherical_harmonics(lmax=2)\n",
    "irreps_sh_r = o3.Irreps('1x1o')\n",
    "\n",
    "alpha, beta, gamma = o3.rand_angles(100)\n",
    "\n",
    "rot_matrix = irreps_sh.D_from_angles(alpha[0], beta[0], gamma[0])\n",
    "rot_matrix_r = irreps_sh_r.D_from_angles(alpha[0], beta[0], gamma[0])\n",
    "\n",
    "\n",
    "data_rot['pos'] = data_rot['pos'] @ rot_matrix_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29a9f7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F is equivariant\n",
      "ETN forward output is equivariant\n",
      "atomic energy is invariant\n",
      "total energy is invariant\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(32)\n",
    "\n",
    "data_out = final_model(data)\n",
    "\n",
    "data_out_rot =final_model(data_rot)\n",
    "\n",
    "\n",
    "F = data_out['node_features_F'] # features\n",
    "F_rot =data_out_rot['node_features_F'] # features from rotated positions\n",
    "F_rot_rot = torch.einsum('Njn,jk->Nkn', F_rot, rot_matrix.T) # rotated features from rotated positions\n",
    "\n",
    "assert torch.allclose(F, F_rot_rot, atol=1e-05)\n",
    "print('F is equivariant')\n",
    "    \n",
    "    \n",
    "ETN_out = data_out['node_features_ETN'] # ETN out features\n",
    "ETN_out_rot =data_out_rot['node_features_ETN'] # ETN out features from rotated positions\n",
    "ETN_out_rot_rot = torch.einsum('Njn,jk->Nkn', ETN_out_rot, rot_matrix.T) # rotated ETN out features from rotated positions\n",
    "\n",
    "assert torch.allclose(ETN_out, ETN_out_rot_rot, atol=1e-05)\n",
    "print('ETN forward output is equivariant')\n",
    "\n",
    "at_en = data_out['atomic_energy'] # atomic energy\n",
    "at_en_rot = data_out_rot['atomic_energy'] # atomic energy from rotated positions\n",
    "\n",
    "assert torch.allclose(at_en, at_en_rot, atol=1e-05)\n",
    "print('atomic energy is invariant')\n",
    "\n",
    "tot_en = data_out['total_energy'] # atomic energy\n",
    "tot_en_rot = data_out_rot['total_energy'] # atomic energy from rotated positions\n",
    "\n",
    "assert torch.allclose(tot_en, tot_en_rot, atol=1e-05)\n",
    "print('total energy is invariant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "350e5c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETN forward output is equivariant\n"
     ]
    }
   ],
   "source": [
    "f_out = data_out['forces'] # ETN out features\n",
    "f_out_rot =data_out_rot['forces'] # ETN out features from rotated positions\n",
    "#f_out_rot_rot = torch.einsum('Njn,jk->Nkn', f_out_rot, rot_matrix_r.T) # rotated ETN out features from rotated positions\n",
    "\n",
    "f_out_rot_rot = f_out_rot @ rot_matrix_r.T\n",
    "\n",
    "assert torch.allclose(f_out, f_out_rot_rot, atol=1e-05)\n",
    "print('ETN forward output is equivariant')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_mkl",
   "language": "python",
   "name": "torch_mkl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
