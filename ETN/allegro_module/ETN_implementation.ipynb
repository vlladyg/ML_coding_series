{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a96b2808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference example\n",
    "from nequip.data import dataset_from_config\n",
    "from nequip.utils import Config\n",
    "#from nequip.utils.misc import get_default_device_name\n",
    "#from nequip.utils.config import _GLOBAL_ALL_ASKED_FOR_KEYS\n",
    "\n",
    "from nequip.model import model_from_config\n",
    "\n",
    "\n",
    "default_config = dict(\n",
    "    root=\"./\",\n",
    "    tensorboard=False,\n",
    "    wandb=False,\n",
    "    model_builders=[\n",
    "        \"SimpleIrrepsConfig\",\n",
    "        \"EnergyModel\",\n",
    "        \"PerSpeciesRescale\",\n",
    "        \"StressForceOutput\",\n",
    "        \"RescaleEnergyEtc\",\n",
    "    ],\n",
    "    dataset_statistics_stride=1,\n",
    "    device='cpu',\n",
    "    default_dtype=\"float64\",\n",
    "    model_dtype=\"float32\",\n",
    "    allow_tf32=True,\n",
    "    verbose=\"INFO\",\n",
    "    model_debug_mode=False,\n",
    "    equivariance_test=False,\n",
    "    grad_anomaly_mode=False,\n",
    "    gpu_oom_offload=False,\n",
    "    append=False,\n",
    "    warn_unused=False,\n",
    "    _jit_bailout_depth=2,  # avoid 20 iters of pain, see https://github.com/pytorch/pytorch/issues/52286\n",
    "    # Quote from eelison in PyTorch slack:\n",
    "    # https://pytorch.slack.com/archives/CDZD1FANA/p1644259272007529?thread_ts=1644064449.039479&cid=CDZD1FANA\n",
    "    # > Right now the default behavior is to specialize twice on static shapes and then on dynamic shapes.\n",
    "    # > To reduce warmup time you can do something like setFusionStrartegy({{FusionBehavior::DYNAMIC, 3}})\n",
    "    # > ... Although we would wouldn't really expect to recompile a dynamic shape fusion in a model,\n",
    "    # > provided broadcasting patterns remain fixed\n",
    "    # We default to DYNAMIC alone because the number of edges is always dynamic,\n",
    "    # even if the number of atoms is fixed:\n",
    "    _jit_fusion_strategy=[(\"DYNAMIC\", 3)],\n",
    "    # Due to what appear to be ongoing bugs with nvFuser, we default to NNC (fuser1) for now:\n",
    "    # TODO: still default to NNC on CPU regardless even if change this for GPU\n",
    "    # TODO: default for ROCm?\n",
    "    _jit_fuser=\"fuser1\",\n",
    ")\n",
    "\n",
    "# All default_config keys are valid / requested\n",
    "#_GLOBAL_ALL_ASKED_FOR_KEYS.update(default_config.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "657c2035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search for AtomicData_options with prefix dataset\n",
      "search for r_max with prefix dataset\n",
      "          0_args :                                               r_max\n",
      "instantiate TypeMapper\n",
      "   optional_args :                             chemical_symbol_to_type\n",
      "...TypeMapper_param = dict(\n",
      "...   optional_args = {'type_names': None, 'chemical_symbol_to_type': {'H': 0, 'C': 1, 'O': 2}, 'type_to_chemical_symbol': None, 'chemical_symbols': None},\n",
      "...   positional_args = {})\n",
      "instantiate register_fields\n",
      "...register_fields_param = dict(\n",
      "...   optional_args = {'node_fields': [], 'edge_fields': [], 'graph_fields': [], 'long_fields': []},\n",
      "...   positional_args = {})\n",
      "instantiate NpzDataset\n",
      "   optional_args :                                                root\n",
      "   optional_args :                                         key_mapping\n",
      "   optional_args :                                npz_fixed_field_keys\n",
      "   optional_args :                                  AtomicData_options <-                         dataset_AtomicData_options\n",
      "   optional_args :                                           file_name <-                                  dataset_file_name\n",
      "   optional_args :                                                 url <-                                        dataset_url\n",
      "...NpzDataset_param = dict(\n",
      "...   optional_args = {'key_mapping': {'z': 'atomic_numbers', 'E': 'total_energy', 'F': 'forces', 'R': 'pos'}, 'include_keys': [], 'npz_fixed_field_keys': ['atomic_numbers'], 'file_name': './benchmark_data/aspirin_ccsd-train.npz', 'url': 'http://quantum-machine.org/gdml/data/npz/aspirin_ccsd.zip', 'AtomicData_options': {'r_max': 6.0}, 'include_frames': None, 'root': 'results/aspirin'},\n",
      "...   positional_args = {'type_mapper': <nequip.data.transforms.TypeMapper object at 0x7fbecc22c910>})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AtomicData(atom_types=[21, 1], cell=[3, 3], edge_cell_shift=[364, 3], edge_index=[2, 364], forces=[21, 3], pbc=[3], pos=[21, 3], total_energy=[1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config.from_file('./configs/example_ETN.yaml', defaults=default_config)\n",
    "    \n",
    "\n",
    "dataset = dataset_from_config(config, prefix=\"dataset\")\n",
    "\n",
    "validation_dataset = None\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d94e777a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Initialize Trainer\n",
      "* Initialize Output\n",
      "  ...generate file name results/aspirin/example/log\n",
      "  ...open log file results/aspirin/example/log\n",
      "  ...generate file name results/aspirin/example/metrics_epoch.csv\n",
      "  ...open log file results/aspirin/example/metrics_epoch.csv\n",
      "  ...generate file name results/aspirin/example/metrics_initialization.csv\n",
      "  ...open log file results/aspirin/example/metrics_initialization.csv\n",
      "  ...generate file name results/aspirin/example/metrics_batch_train.csv\n",
      "  ...open log file results/aspirin/example/metrics_batch_train.csv\n",
      "  ...generate file name results/aspirin/example/metrics_batch_val.csv\n",
      "  ...open log file results/aspirin/example/metrics_batch_val.csv\n",
      "  ...generate file name results/aspirin/example/best_model.pth\n",
      "  ...generate file name results/aspirin/example/last_model.pth\n",
      "  ...generate file name results/aspirin/example/trainer.pth\n",
      "  ...generate file name results/aspirin/example/config.yaml\n",
      "Torch device: cpu\n",
      "instantiate Loss\n",
      "...Loss_param = dict(\n",
      "...   optional_args = {'coeff_schedule': 'constant'},\n",
      "...   positional_args = {'coeffs': {'forces': 1.0, 'total_energy': [1.0, 'PerAtomMSELoss']}})\n",
      "instantiate MSELoss\n",
      "...MSELoss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      " parsing forces 1.0\n",
      " parsing 1.0 MSELoss\n",
      "instantiate MSELoss\n",
      "...MSELoss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      " parsing total_energy [1.0, 'PerAtomMSELoss']\n",
      " parsing 1.0 PerAtomMSELoss\n",
      "create loss instance <class 'nequip.train._loss.PerAtomLoss'>\n",
      "instantiate MSELoss\n",
      "...MSELoss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      "Building ETN model...\n",
      "instantiate PairTypeEmbedding\n",
      "        all_args :                                           num_types\n",
      "...PairTypeEmbedding_param = dict(\n",
      "...   optional_args = {'num_types': 3},\n",
      "...   positional_args = {'irreps_in': None})\n",
      "instantiate OneHotAtomEncoding\n",
      "        all_args :                                           num_types\n",
      "...OneHotAtomEncoding_param = dict(\n",
      "...   optional_args = {'set_features': True, 'num_types': 3},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee}})\n",
      "instantiate RadialBasisEdgeEncoding\n",
      "        all_args :                                  basis_kwargs.r_max <-                                              r_max\n",
      "        all_args :            basis_kwargs.original_basis_kwargs.r_max <-                                              r_max\n",
      "        all_args :        basis_kwargs.original_basis_kwargs.trainable <-                              BesselBasis_trainable\n",
      "        all_args :                                 cutoff_kwargs.r_max <-                                              r_max\n",
      "        all_args :                                     cutoff_kwargs.p <-                                 PolynomialCutoff_p\n",
      "   optional_args :                                               basis\n",
      "   optional_args :                                           out_field\n",
      "...RadialBasisEdgeEncoding_param = dict(\n",
      "...   optional_args = {'basis': <class 'allegro.nn._norm_basis.NormalizedBasis'>, 'cutoff': <class 'nequip.nn.cutoffs.PolynomialCutoff'>, 'basis_kwargs': {'r_min': 0.0, 'original_basis': <class 'nequip.nn.radial_basis.BesselBasis'>, 'original_basis_kwargs': {'num_basis': 8, 'trainable': True, 'r_max': 6.0}, 'n': 4000, 'norm_basis_mean_shift': True, 'r_max': 6.0}, 'cutoff_kwargs': {'p': 6, 'r_max': 6.0}, 'out_field': 'edge_embedding'},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee, 'node_attrs': 3x0ee, 'node_features': 3x0ee}})\n",
      "instantiate SphericalHarmonicEdgeAttrs\n",
      "        all_args :                                      irreps_edge_sh\n",
      "...SphericalHarmonicEdgeAttrs_param = dict(\n",
      "...   optional_args = {'edge_sh_normalization': 'component', 'edge_sh_normalize': True, 'out_field': 'edge_attrs', 'irreps_edge_sh': '1x0ee+1x1oe+1x2ee'},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee, 'node_attrs': 3x0ee, 'node_features': 3x0ee, 'edge_embedding': 8x0ee, 'edge_cutoff': 1x0ee}})\n",
      "instantiate EdgeFeatures_F\n",
      "        all_args :                                           num_types\n",
      "   optional_args :                                                  Nc\n",
      "   optional_args :                                           out_field\n",
      "   optional_args :                                         N_rank_spec\n",
      "...EdgeFeatures_F_param = dict(\n",
      "...   optional_args = {'num_basis': 8, 'N_rank_spec': 4, 'out_field': 'edge_features_F', 'Nc': 10, 'num_types': 3},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee, 'node_attrs': 3x0ee, 'node_features': 3x0ee, 'edge_embedding': 8x0ee, 'edge_cutoff': 1x0ee, 'edge_attrs': 1x0ee+1x1oe+1x2ee}})\n",
      "instantiate EdgewiseFSum\n",
      "        all_args :                                   avg_num_neighbors\n",
      "        all_args :                                           num_types\n",
      "...EdgewiseFSum_param = dict(\n",
      "...   optional_args = {'avg_num_neighbors': 17.211328506469727, 'normalize_edge_features_f': True, 'per_edge_species_scale': False, 'num_types': 3},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee, 'node_attrs': 3x0ee, 'node_features': 3x0ee, 'edge_embedding': 8x0ee, 'edge_cutoff': 1x0ee, 'edge_attrs': 1x0ee+1x1oe+1x2ee, 'edge_features_F': 10x0ee+10x1oe+10x2ee}})\n",
      "instantiate ETN_Module\n",
      "   optional_args :                                          N_rank_ett\n",
      "   optional_args :                                                   d\n",
      "   optional_args :                                           out_field\n",
      "...ETN_Module_param = dict(\n",
      "...   optional_args = {'out_field': 'atomic_energy', 'N_rank_ett': [4, 4, 4], 'd': 4},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee, 'node_attrs': 3x0ee, 'node_features': 3x0ee, 'edge_embedding': 8x0ee, 'edge_cutoff': 1x0ee, 'edge_attrs': 1x0ee+1x1oe+1x2ee, 'edge_features_F': 10x0ee+10x1oe+10x2ee, 'node_features_F': 10x0ee+10x1oe+10x2ee}})\n",
      "instantiate AtomwiseReduce\n",
      "   optional_args :                                               field\n",
      "   optional_args :                                           out_field\n",
      "   optional_args :                                              reduce\n",
      "...AtomwiseReduce_param = dict(\n",
      "...   optional_args = {'out_field': 'total_energy', 'reduce': 'sum', 'avg_num_atoms': None, 'field': 'atomic_energy'},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee, 'node_attrs': 3x0ee, 'node_features': 3x0ee, 'edge_embedding': 8x0ee, 'edge_cutoff': 1x0ee, 'edge_attrs': 1x0ee+1x1oe+1x2ee, 'edge_features_F': 10x0ee+10x1oe+10x2ee, 'node_features_F': 10x0ee+10x1oe+10x2ee, 'node_features_ETN': 10x0ee+10x1oe+10x2ee, 'atomic_energy': 1x0ee}})\n",
      "Replace string dataset_forces_rms to 31.252248764038086\n",
      "Replace string dataset_per_atom_total_energy_mean to -19318.35546875\n",
      "Atomic outputs are scaled by: [H, C, O: 31.252249], shifted by [H, C, O: -19318.355469].\n",
      "instantiate PerSpeciesScaleShift\n",
      "        all_args :                                          type_names\n",
      "        all_args :                                       default_dtype\n",
      "        all_args :                                           num_types\n",
      "   optional_args :                                              scales\n",
      "   optional_args :                                               field\n",
      "   optional_args :                                           out_field\n",
      "   optional_args :                                              shifts\n",
      "   optional_args :                          arguments_in_dataset_units\n",
      "...PerSpeciesScaleShift_param = dict(\n",
      "...   optional_args = {'out_field': 'atomic_energy', 'scales_trainable': False, 'shifts_trainable': False, 'default_dtype': 'float32', 'num_types': 3, 'type_names': ['H', 'C', 'O'], 'field': 'atomic_energy', 'shifts': tensor(-19318.3555), 'scales': tensor(31.2522), 'arguments_in_dataset_units': True},\n",
      "...   positional_args = {'irreps_in': {'pos': 1x1oe, 'edge_index': None, 'edge_types': 1x0ee, 'node_attrs': 3x0ee, 'node_features': 3x0ee, 'edge_embedding': 8x0ee, 'edge_cutoff': 1x0ee, 'edge_attrs': 1x0ee+1x1oe+1x2ee, 'edge_features_F': 10x0ee+10x1oe+10x2ee, 'node_features_F': 10x0ee+10x1oe+10x2ee, 'node_features_ETN': 10x0ee+10x1oe+10x2ee, 'atomic_energy': 1x0ee}})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replace string dataset_forces_rms to 31.252248764038086\n",
      "Initially outputs are globally scaled by: 31.252248764038086, total_energy are globally shifted by None.\n",
      "PerSpeciesScaleShift's arguments were in dataset units; rescaling:\n",
      "  Original scales: [H: 31.252249, C: 31.252249, O: 31.252249] shifts: [H: -19318.355469, C: -19318.355469, O: -19318.355469]\n",
      "  New scales: [H: 1.000000, C: 1.000000, O: 1.000000] shifts: [H: -618.142883, C: -618.142883, O: -618.142883]\n"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "from nequip.train.trainer import Trainer\n",
    "from e3nn import o3\n",
    "\n",
    "trainer = Trainer(model=None, **Config.as_dict(config))\n",
    "\n",
    "# what is this\n",
    "# to update wandb data?\n",
    "config.update(trainer.params)\n",
    "\n",
    "# = Train/test split =\n",
    "trainer.set_dataset(dataset, validation_dataset)\n",
    "\n",
    "#config['model_input_fields'] = {'node_spin': o3.Irreps('1x1e')}\n",
    "Nc = 10\n",
    "N_rank_spec = 4\n",
    "config['Nc'] = Nc\n",
    "config['N_rank_spec'] = N_rank_spec\n",
    "config['N_rank_ett'] = [4, 4, 4]\n",
    "config['d'] = 4\n",
    "\n",
    "\n",
    "# = Build model =\n",
    "final_model = model_from_config(\n",
    "    config=config, initialize=True, dataset=trainer.dataset_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca78d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "from nequip.data import AtomicData, AtomicDataDict\n",
    "from torch.nn.functional import one_hot\n",
    "from e3nn.nn import FullyConnectedNet\n",
    "from allegro import with_edge_spin_length\n",
    "from allegro import _keys\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "trainer.model = final_model\n",
    "data0 = AtomicData.to_AtomicDataDict(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6897ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of weights: 1316\n",
      "Number of trainable weights: 1316\n",
      "instantiate Adam\n",
      "        all_args :                                             amsgrad <-                           optimizer_params.amsgrad\n",
      "        all_args :                                                 eps <-                               optimizer_params.eps\n",
      "        all_args :                                        weight_decay <-                      optimizer_params.weight_decay\n",
      "        all_args :                                               betas <-                             optimizer_params.betas\n",
      "...Adam_param = dict(\n",
      "...   optional_args = {'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False, 'foreach': None, 'maximize': False, 'capturable': False, 'differentiable': False, 'fused': None},\n",
      "...   positional_args = {'params': <generator object Module.parameters at 0x7fbece1b7060>, 'lr': 0.001})\n",
      "instantiate ReduceLROnPlateau\n",
      "        all_args :                                              factor <-                                lr_scheduler_factor\n",
      "        all_args :                                            patience <-                              lr_scheduler_patience\n",
      "...ReduceLROnPlateau_param = dict(\n",
      "...   optional_args = {'mode': 'min', 'factor': 0.5, 'patience': 50, 'threshold': 0.0001, 'threshold_mode': 'rel', 'cooldown': 0, 'min_lr': 0, 'eps': 1e-08, 'verbose': False},\n",
      "...   positional_args = {'optimizer': Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      ")})\n",
      "get args for EarlyStopping\n",
      "        all_args :                                        upper_bounds <-                        early_stopping_upper_bounds\n",
      "        all_args :                                           patiences <-                           early_stopping_patiences\n",
      "        all_args :                                        lower_bounds <-                        early_stopping_lower_bounds\n",
      "...EarlyStopping_param = dict(\n",
      "...   optional_args = {'lower_bounds': {'LR': 1e-05}, 'upper_bounds': {'cumulative_wall': 604800.0}, 'patiences': {'validation_loss': 100}, 'delta': {}, 'cumulative_delta': False},\n",
      "...   positional_args = {})\n",
      "! Starting training ...\n",
      "Saved trainer to results/aspirin/example/trainer.pth\n",
      "Saved last model to to results/aspirin/example/last_model.pth\n",
      "instantiate Metrics\n",
      "...Metrics_param = dict(\n",
      "...   optional_args = {},\n",
      "...   positional_args = {'components': [('forces', 'mae', {'PerSpecies': False}), ('forces', 'rmse', {'PerSpecies': False}), ('total_energy', 'mae', {'PerSpecies': False}), ('total_energy', 'rmse', {'PerSpecies': False})]})\n",
      "instantiate L1Loss\n",
      "...L1Loss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      "instantiate L1Loss\n",
      "...L1Loss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      "instantiate L1Loss\n",
      "...L1Loss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      "instantiate L1Loss\n",
      "...L1Loss_param = dict(\n",
      "...   optional_args = {'size_average': None, 'reduce': None},\n",
      "...   positional_args = {'reduction': 'none'})\n",
      "/Users/temporary/Documents/GitHub/pytorch-intel-mps/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([5, 105])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/temporary/Documents/GitHub/pytorch-intel-mps/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([5, 105])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "      0    10         1.16         1.16     0.000198         25.2         33.7         7.61         9.23\n",
      "\n",
      "\n",
      "  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
      "! Initial Validation          0    0.509    0.001        0.941      9.4e-05        0.941         22.8         30.3          4.9         6.37\n",
      "Wall time: 0.5111326509795617\n",
      "! Best model        0    0.941\n",
      "Saved trainer to results/aspirin/example/trainer.pth\n",
      "Saved last model to to results/aspirin/example/last_model.pth\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "      1   100        0.594        0.594     9.62e-05         17.3         24.1         5.32         6.43\n",
      "      1   190        0.533        0.532     0.000291         15.6         22.8         8.99         11.2\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "      1    10        0.474        0.474     0.000451         14.6         21.5         11.4           14\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
      "! Train               1   17.160    0.001        0.684     0.000174        0.684         18.5         25.8         6.68         8.66\n",
      "! Validation          1   17.160    0.001        0.438     0.000206        0.438         14.2         20.7         7.26         9.42\n",
      "Wall time: 17.16215481699328\n",
      "! Best model        1    0.438\n",
      "Saved trainer to results/aspirin/example/trainer.pth\n",
      "Saved last model to to results/aspirin/example/last_model.pth\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "      2   100        0.297        0.297     0.000142         11.7           17         6.03         7.82\n",
      "      2   190        0.253        0.253     0.000107         10.5         15.7         5.52         6.77\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "      2    10        0.263        0.262     0.000499         11.1           16         12.1         14.7\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
      "! Train               2   33.717    0.001         0.32     0.000247         0.32         12.1         17.7         8.03         10.3\n",
      "! Validation          2   33.717    0.001        0.248     0.000234        0.248         10.7         15.6         7.78           10\n",
      "Wall time: 33.71974817098817\n",
      "! Best model        2    0.248\n",
      "Saved trainer to results/aspirin/example/trainer.pth\n",
      "Saved last model to to results/aspirin/example/last_model.pth\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "      3   100        0.151        0.151     0.000171         8.78         12.1         6.86         8.58\n",
      "      3   190        0.172        0.172     0.000272         9.97           13         8.53         10.8\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "      3    10        0.186        0.186     0.000495         9.47         13.5         12.1         14.6\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
      "! Train               3   50.069    0.001        0.193     0.000257        0.193         9.77         13.7         8.19         10.5\n",
      "! Validation          3   50.069    0.001        0.166      0.00023        0.167         9.05         12.7         7.71         9.95\n",
      "Wall time: 50.07162014200003\n",
      "! Best model        3    0.167\n",
      "Saved trainer to results/aspirin/example/trainer.pth\n",
      "Saved last model to to results/aspirin/example/last_model.pth\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      4   100        0.147        0.147     0.000185         8.48           12         6.93         8.93\n",
      "      4   190         0.14        0.139     0.000492          8.6         11.7         11.9         14.6\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "      4    10        0.148        0.148     0.000491         8.56           12           12         14.5\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
      "! Train               4   66.744    0.001        0.144     0.000252        0.144         8.56         11.9         8.13         10.4\n",
      "! Validation          4   66.744    0.001         0.13     0.000227         0.13         8.14         11.3         7.68         9.88\n",
      "Wall time: 66.74657141198986\n",
      "! Best model        4    0.130\n",
      "Saved trainer to results/aspirin/example/trainer.pth\n",
      "Saved last model to to results/aspirin/example/last_model.pth\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "      5   100       0.0761       0.0759     0.000247         6.38         8.61         8.08         10.3\n",
      "      5   190        0.117        0.117     0.000171         7.61         10.7         6.88         8.59\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "      5    10        0.124        0.124     0.000478         7.77           11         11.9         14.4\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
      "! Train               5   82.958    0.001        0.118     0.000242        0.118         7.79         10.7            8         10.2\n",
      "! Validation          5   82.958    0.001        0.108     0.000218        0.109         7.44         10.3         7.56         9.68\n",
      "Wall time: 82.9606486519915\n",
      "! Best model        5    0.109\n",
      "Saved trainer to results/aspirin/example/trainer.pth\n",
      "Saved last model to to results/aspirin/example/last_model.pth\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "      6   100       0.0771       0.0769     0.000112         6.24         8.67         5.53         6.93\n",
      "      6   190       0.0627       0.0625     0.000134         5.69         7.82         6.31          7.6\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "      6    10        0.105        0.105     0.000469         7.07         10.1         11.8         14.2\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
      "! Train               6   99.397    0.001        0.102     0.000235        0.102         7.25         9.99         7.93         10.1\n",
      "! Validation          6   99.397    0.001       0.0929     0.000211       0.0931         6.87         9.53         7.49         9.54\n",
      "Wall time: 99.39923706499394\n",
      "! Best model        6    0.093\n",
      "Saved trainer to results/aspirin/example/trainer.pth\n",
      "Saved last model to to results/aspirin/example/last_model.pth\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "      7   100        0.102        0.101     0.000288         7.29         9.95         8.68         11.1\n",
      "      7   190       0.0938       0.0935     0.000319         7.16         9.56         9.91         11.7\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "      7    10       0.0886       0.0882     0.000463         6.48         9.28         11.8         14.1\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
      "! Train               7  115.483    0.001       0.0886     0.000231       0.0889         6.77          9.3         7.87         9.97\n",
      "! Validation          7  115.483    0.001       0.0813     0.000207       0.0815         6.43         8.91         7.44         9.44\n",
      "Wall time: 115.4855627049983\n",
      "! Best model        7    0.082\n",
      "Saved trainer to results/aspirin/example/trainer.pth\n",
      "Saved last model to to results/aspirin/example/last_model.pth\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "      8   100       0.0742       0.0739     0.000305          6.3          8.5         9.13         11.5\n",
      "      8   190       0.0652        0.065     0.000242         5.81         7.97         8.49         10.2\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "      8    10       0.0782       0.0777     0.000462         6.11         8.71         11.8         14.1\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
      "! Train               8  131.103    0.001       0.0781     0.000231       0.0783         6.37         8.73         7.89         9.97\n",
      "! Validation          8  131.103    0.001       0.0728     0.000206        0.073         6.12         8.43         7.44         9.43\n",
      "Wall time: 131.10582591500133\n",
      "! Best model        8    0.073\n",
      "Saved trainer to results/aspirin/example/trainer.pth\n",
      "Saved last model to to results/aspirin/example/last_model.pth\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "      9   100       0.0704       0.0701     0.000289         5.99         8.28         8.91         11.2\n",
      "      9   190        0.049       0.0488     0.000254         5.18          6.9         8.22         10.5\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "      9    10        0.069       0.0686     0.000462         5.77         8.18         11.8         14.1\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
      "! Train               9  147.412    0.001       0.0718     0.000231        0.072         6.12         8.37         7.89         9.96\n",
      "! Validation          9  147.412    0.001        0.066     0.000207       0.0663         5.85         8.03         7.45         9.43\n",
      "Wall time: 147.41411542898277\n",
      "! Best model        9    0.066\n",
      "Saved trainer to results/aspirin/example/trainer.pth\n",
      "Saved last model to to results/aspirin/example/last_model.pth\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "     10   100       0.0615       0.0612     0.000314         5.86         7.73          9.6         11.6\n",
      "     10   190       0.0499       0.0497     0.000244         5.16         6.97          8.6         10.2\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "     10    10       0.0634       0.0629     0.000463         5.55         7.84         11.8         14.1\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
      "! Train              10  163.651    0.001       0.0645     0.000231       0.0648         5.81         7.94          7.9         9.97\n",
      "! Validation         10  163.651    0.001        0.061     0.000207       0.0612         5.63         7.72         7.47         9.45\n",
      "Wall time: 163.65350061797653\n",
      "! Best model       10    0.061\n",
      "Saved trainer to results/aspirin/example/trainer.pth\n",
      "Saved last model to to results/aspirin/example/last_model.pth\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "     11   100       0.0693       0.0691     0.000207         5.62         8.21         7.44         9.44\n",
      "     11   190       0.0554       0.0553     0.000166          5.5         7.35         6.84         8.45\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "     11    10        0.058       0.0576     0.000463         5.35          7.5         11.8         14.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
      "! Train              11  179.485    0.001       0.0601     0.000231       0.0604         5.61         7.66         7.91         9.97\n",
      "! Validation         11  179.485    0.001       0.0567     0.000207        0.057         5.44         7.44         7.47         9.44\n",
      "Wall time: 179.48706013397896\n",
      "! Best model       11    0.057\n",
      "Saved trainer to results/aspirin/example/trainer.pth\n",
      "Saved last model to to results/aspirin/example/last_model.pth\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "     12   100       0.0526       0.0524     0.000182         5.11         7.16         7.34         8.84\n",
      "     12   190       0.0526       0.0522     0.000383         5.17         7.14         10.1         12.8\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "     12    10       0.0546       0.0541     0.000463         5.25         7.27         11.8         14.1\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
      "! Train              12  195.455    0.001       0.0554      0.00023       0.0556         5.38         7.35         7.89         9.95\n",
      "! Validation         12  195.455    0.001       0.0531     0.000207       0.0533         5.27          7.2         7.46         9.43\n",
      "Wall time: 195.45709387699026\n",
      "! Best model       12    0.053\n",
      "Saved trainer to results/aspirin/example/trainer.pth\n",
      "Saved last model to to results/aspirin/example/last_model.pth\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "     13   100       0.0418       0.0417     0.000113         4.59         6.38         5.87         6.99\n",
      "     13   190       0.0466       0.0463     0.000208         5.17         6.73         7.73         9.46\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "     13    10       0.0514       0.0509     0.000462         5.14         7.05         11.8         14.1\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
      "! Train              13  211.508    0.001       0.0525      0.00023       0.0528         5.25         7.16         7.89         9.94\n",
      "! Validation         13  211.508    0.001       0.0498     0.000206         0.05         5.11         6.97         7.45         9.41\n",
      "Wall time: 211.51069838699186\n",
      "! Best model       13    0.050\n",
      "Saved trainer to results/aspirin/example/trainer.pth\n",
      "Saved last model to to results/aspirin/example/last_model.pth\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "     14   100        0.039       0.0388     0.000151         4.54         6.16         6.67         8.06\n",
      "     14   190       0.0349       0.0347     0.000167          4.2         5.82         7.04         8.47\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "     14    10       0.0485       0.0481      0.00046         5.03         6.85         11.8         14.1\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
      "! Train              14  227.250    0.001       0.0506     0.000227       0.0508         5.15         7.03         7.85         9.89\n",
      "! Validation         14  227.250    0.001       0.0471     0.000204       0.0473         4.97         6.78         7.41         9.37\n",
      "Wall time: 227.25282640897785\n",
      "! Best model       14    0.047\n",
      "Saved trainer to results/aspirin/example/trainer.pth\n",
      "Saved last model to to results/aspirin/example/last_model.pth\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67e6c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "from nequip.data import AtomicData, AtomicDataDict\n",
    "from torch.nn.functional import one_hot\n",
    "from e3nn.nn import FullyConnectedNet\n",
    "    \n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "data_new = final_model(data0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c615ae4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 9, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new['node_features_ETN'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6d29168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 9, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new['node_features_F'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "156e8a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "from nequip.data import AtomicData, AtomicDataDict\n",
    "from torch.nn.functional import one_hot\n",
    "from e3nn.nn import FullyConnectedNet\n",
    "from allegro import with_edge_spin_length\n",
    "from allegro import _keys\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "data = data0\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "data_rot = {key: torch.clone(data0[key]) for key in data0}\n",
    "\n",
    "irreps_sh = o3.Irreps('1x0e + 1x1o + 1x2e') #o3.Irreps.spherical_harmonics(lmax=2)\n",
    "irreps_sh_r = o3.Irreps('1x1o')\n",
    "\n",
    "alpha, beta, gamma = o3.rand_angles(100)\n",
    "\n",
    "rot_matrix = irreps_sh.D_from_angles(alpha[0], beta[0], gamma[0])\n",
    "rot_matrix_r = irreps_sh_r.D_from_angles(alpha[0], beta[0], gamma[0])\n",
    "\n",
    "\n",
    "data_rot['pos'] = data_rot['pos'] @ rot_matrix_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3c0aaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F is equivariant\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(32)\n",
    "\n",
    "F = final_model(data)['node_features_F']\n",
    "\n",
    "torch.manual_seed(32)\n",
    "\n",
    "F_rot =final_model(data_rot)['node_features_F']\n",
    "\n",
    "\n",
    "F_rot_rot = torch.einsum('Njn,jk->Nkn', F_rot, rot_matrix.T)\n",
    "\n",
    "if torch.allclose(F, F_rot_rot, atol=1e-05):\n",
    "    print('F is equivariant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e4ffccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F is equivariant\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(32)\n",
    "\n",
    "F = final_model(data)['node_features_ETN']\n",
    "\n",
    "\n",
    "torch.manual_seed(32)\n",
    "\n",
    "F_rot =final_model(data_rot)['node_features_ETN']\n",
    "\n",
    "\n",
    "F_rot_rot = torch.einsum('Njn,jk->Nkn', F_rot, rot_matrix.T)\n",
    "\n",
    "if torch.allclose(F, F_rot_rot, atol=1e-05):\n",
    "    print('F is equivariant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25f604cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atomic energy is invariant\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(32)\n",
    "\n",
    "F = final_model(data)['atomic_energy']\n",
    "\n",
    "\n",
    "torch.manual_seed(32)\n",
    "\n",
    "F_rot =final_model(data_rot)['atomic_energy']\n",
    "\n",
    "\n",
    "if torch.allclose(F, F_rot, atol=1e-05):\n",
    "    print('atomic energy is invariant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc4b3385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atomic energy is invariant\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(32)\n",
    "\n",
    "F = final_model(data)['total_energy']\n",
    "\n",
    "\n",
    "torch.manual_seed(32)\n",
    "\n",
    "F_rot =final_model(data_rot)['total_energy']\n",
    "\n",
    "\n",
    "if torch.allclose(F, F_rot, atol=1e-05):\n",
    "    print('atomic energy is invariant')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_mkl",
   "language": "python",
   "name": "torch_mkl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
