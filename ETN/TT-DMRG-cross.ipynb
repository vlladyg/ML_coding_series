{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f36d0aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.11022302e-16])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ttml.tt_cross import estimator_to_tt_cross\n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(np.sum(x, axis=1))\n",
    "\n",
    "# Same thresholds for every feature: [0,0.2,0.4,0.6,0.8]\n",
    "thresholds = [np.linspace(0, 1, 11)] * 5\n",
    "\n",
    "tt = estimator_to_tt_cross(f, thresholds)\n",
    "tt.gather(np.array([[2, 2, 2, 2, 2]])) - np.sin(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4894656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2.62706082e+02, 1.10057236e+02, 1.01854161e-13, 7.11972404e-14,\n",
       "        4.37889373e-14]),\n",
       " array([2.50489642e+02, 1.35580309e+02, 5.40010485e-14, 2.48204966e-14,\n",
       "        1.87124926e-14]),\n",
       " array([2.50489642e+02, 1.35580309e+02, 3.23567263e-14, 2.47347881e-14,\n",
       "        1.88056492e-14]),\n",
       " array([2.62706082e+02, 1.10057236e+02, 2.87919777e-14, 2.27094898e-14,\n",
       "        1.03424501e-14])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.sing_vals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dc880b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorTrain of order 5 with outer dimensions (11, 11, 11, 11, 11), TT-rank (2, 2, 2, 2), and orthogonalized at mode 4>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt2 = estimator_to_tt_cross(f, thresholds, max_rank=2)\n",
    "\n",
    "tt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b6ea261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.916953631940722e-13"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tt - tt2).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5fe8d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorTrain of order 5 with outer dimensions (11, 11, 11, 11, 11), TT-rank (2, 2, 2, 2), and orthogonalized at mode 4>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X = np.random.normal(size=(1000, 5))\n",
    "y = np.exp(np.sum(X, axis=1))\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest.fit(X, y)\n",
    "\n",
    "thresholds = [np.linspace(0, 1, 11)] * 5\n",
    "\n",
    "tt = estimator_to_tt_cross(forest.predict, thresholds, max_rank=2)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "897683ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = estimator_to_tt_cross(forest.predict, thresholds, method='dmrg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bea86fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly as tl\n",
    "from tensorly.contrib.decomposition import tensor_train_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05fb9324-d27f-4467-97cc-a083b2c547e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tl.tensor(np.arange(5**3).reshape(5,5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a26d379e-46a9-4b0c-858f-060a56d39b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = [1, 3, 3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1986c33-5563-43e3-906c-ffbdac36eeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = tensor_train_cross(tensor, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b03cbdf-3bbd-4762-b72f-8ff07a2b9033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 24   7   0]\n",
      "  [ 49  32  25]\n",
      "  [ 74  57  50]\n",
      "  [ 99  82  75]\n",
      "  [124 107 100]]]\n"
     ]
    }
   ],
   "source": [
    "print(factors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "717a81bf-367b-4337-bd2b-f95da9dc2ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly as tl\n",
    "from tensorly import tt_to_tensor\n",
    "import numpy as np\n",
    "\n",
    "def tensor_train_cross(input_tensor, rank, tol=1e-4, n_iter_max=100, random_state=None):\n",
    "    \"\"\"TT (tensor-train) decomposition via cross-approximation (TTcross) [1]\n",
    "\n",
    "    Decomposes `input_tensor` into a sequence of order-3 tensors of given rank. (factors/cores)\n",
    "    Rather than directly decompose the whole tensor, we sample fibers based on skeleton decomposition.\n",
    "    We initialize a random tensor-train and sweep from left to right and right to left.\n",
    "    On each core, we shape the core as a matrix and choose the fibers indices by finding maximum-volume submatrix and update the core.\n",
    "\n",
    "    * Advantage: faster\n",
    "        The main advantage of TTcross is that it doesn't need to evaluate all the entries of the tensor.\n",
    "        For a tensor_shape^tensor_order tensor, SVD needs O(tensor_shape^tensor_order) runtime, but TTcross' runtime is linear in tensor_shape and tensor_order, which makes it feasible in high dimension.\n",
    "    * Disadvantage: less accurate\n",
    "        TTcross may underestimate the error, since it only evaluates partial entries of the tensor.\n",
    "        Besides, in contrast to its practical fast performance, there is no theoretical guarantee of it convergence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_tensor : tensorly.tensor\n",
    "            The tensor to decompose.\n",
    "    rank : {int, int list}\n",
    "            maximum allowable TT rank of the factors\n",
    "            if int, then this is the same for all the factors\n",
    "            if int list, then rank[k] is the rank of the kth factor\n",
    "    tol : float\n",
    "            accuracy threshold for outer while-loop\n",
    "    n_iter_max : int\n",
    "            maximum iterations of outer while-loop (the 'crosses' or 'sweeps' sampled)\n",
    "    random_state : {None, int, np.random.RandomState}\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    factors : TT factors\n",
    "              order-3 tensors of the TT decomposition\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    Generate a 5^3 tensor, and decompose it into tensor-train of 3 factors, with rank = [1,3,3,1]\n",
    "\n",
    "    >>> tensor = tl.tensor(np.arange(5**3).reshape(5,5,5))\n",
    "    >>> rank = [1, 3, 3, 1]\n",
    "    >>> factors = tensor_train_cross(tensor, rank)\n",
    "    >>> # print the first core:\n",
    "    >>> print(factors[0])\n",
    "    [[[ 24.   0.   4.]\n",
    "      [ 49.  25.  29.]\n",
    "      [ 74.  50.  54.]\n",
    "      [ 99.  75.  79.]\n",
    "      [124. 100. 104.]]]\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Pseudo-code [2]:\n",
    "\n",
    "    1. Initialization tensor_order cores and column indices\n",
    "\n",
    "    2. while (error > tol)\n",
    "\n",
    "    3. update the tensor-train from left to right:\n",
    "\n",
    "       .. code:: python\n",
    "\n",
    "           for Core 1 to Core tensor_order:\n",
    "           approximate the skeleton-decomposition by QR and maxvol\n",
    "\n",
    "    4. update the tensor-train from right to left:\n",
    "\n",
    "       .. code:: python\n",
    "\n",
    "            for Core tensor_order to Core 1\n",
    "                approximate the skeleton-decomposition by QR and maxvol\n",
    "\n",
    "    5. end while\n",
    "\n",
    "    Acknowledgement: the main body of the code is modified based on TensorToolbox by Daniele Bigoni.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Ivan Oseledets and Eugene Tyrtyshnikov.  Tt-cross approximation for multidimensional arrays.\n",
    "            LinearAlgebra and its Applications, 432(1):70–88, 2010.\n",
    "    .. [2] Sergey Dolgov and Robert Scheichl. A hybrid alternating least squares–tt cross algorithm for parametricpdes.\n",
    "            arXiv preprint arXiv:1707.04562, 2017.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check user input for errors\n",
    "    tensor_shape = tl.shape(input_tensor)\n",
    "    tensor_order = tl.ndim(input_tensor)\n",
    "\n",
    "    if isinstance(rank, int):\n",
    "        rank = [rank] * (tensor_order + 1)\n",
    "    elif tensor_order + 1 != len(rank):\n",
    "        message = (\n",
    "            \"Provided incorrect number of ranks. Should verify \"\n",
    "            + f\"len(rank) == tl.ndim(tensor)+1, but len(rank) = {len(rank)} \"\n",
    "            + f\"while tl.ndim(tensor) + 1  = {tensor_order}\"\n",
    "        )\n",
    "        raise (ValueError(message))\n",
    "\n",
    "    # Make sure iter's not a tuple but a list\n",
    "    rank = list(rank)\n",
    "\n",
    "    # Initialize rank\n",
    "    if rank[0] != 1:\n",
    "        message = f\"Provided rank[0] == {rank[0]} but boundary conditions dictate rank[0] == rank[-1] == 1.\"\n",
    "        raise ValueError(message)\n",
    "    if rank[-1] != 1:\n",
    "        message = f\"Provided rank[-1] == {rank[-1]} but boundary conditions dictate rank[0] == rank[-1] == 1.\"\n",
    "        raise ValueError(message)\n",
    "\n",
    "    # list col_idx: column indices (right indices) for skeleton-decomposition: indicate which columns used in each core.\n",
    "    # list row_idx: row indices    (left indices)  for skeleton-decomposition: indicate which rows used in each core.\n",
    "\n",
    "    # Initialize indice: random selection of column indices\n",
    "    rng = tl.check_random_state(random_state)\n",
    "\n",
    "    col_idx = [None] * tensor_order\n",
    "    for k_col_idx in range(tensor_order - 1):\n",
    "        col_idx[k_col_idx] = []\n",
    "        for i in range(rank[k_col_idx + 1]):\n",
    "            newidx = tuple(\n",
    "                [\n",
    "                    rng.randint(tensor_shape[j])\n",
    "                    for j in range(k_col_idx + 1, tensor_order)\n",
    "                ]\n",
    "            )\n",
    "            while newidx in col_idx[k_col_idx]:\n",
    "                newidx = tuple(\n",
    "                    [\n",
    "                        rng.randint(tensor_shape[j])\n",
    "                        for j in range(k_col_idx + 1, tensor_order)\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            col_idx[k_col_idx].append(newidx)\n",
    "\n",
    "    # Initialize the cores of tensor-train\n",
    "    factor_old = [\n",
    "        tl.zeros((rank[k], tensor_shape[k], rank[k + 1]), **tl.context(input_tensor))\n",
    "        for k in range(tensor_order)\n",
    "    ]\n",
    "    factor_new = [\n",
    "        tl.tensor(\n",
    "            rng.random_sample((rank[k], tensor_shape[k], rank[k + 1])),\n",
    "            **tl.context(input_tensor),\n",
    "        )\n",
    "        for k in range(tensor_order)\n",
    "    ]\n",
    "\n",
    "    iter = 0\n",
    "\n",
    "    error = tl.norm(tt_to_tensor(factor_old) - tt_to_tensor(factor_new), 2)\n",
    "    threshold = tol * tl.norm(tt_to_tensor(factor_new), 2)\n",
    "    for iter in range(n_iter_max):\n",
    "        if error < threshold:\n",
    "            break\n",
    "\n",
    "        factor_old = factor_new\n",
    "        factor_new = [None for i in range(tensor_order)]\n",
    "\n",
    "        ######################################\n",
    "        # left-to-right step\n",
    "        left_to_right_fiberlist = []\n",
    "        # list row_idx: list of (tensor_order-1) of lists of left indices\n",
    "        row_idx = [[()]]\n",
    "        if iter == 0:\n",
    "            print(row_idx)\n",
    "            print(col_idx)\n",
    "        for k in range(tensor_order - 1):\n",
    "            (next_row_idx, fibers_list) = left_right_ttcross_step(\n",
    "                input_tensor, k, rank, row_idx, col_idx\n",
    "            )\n",
    "            # update row indices\n",
    "            left_to_right_fiberlist.extend(fibers_list)\n",
    "            row_idx.append(next_row_idx)\n",
    "\n",
    "        # end left-to-right step\n",
    "        ###############################################\n",
    "\n",
    "        ###############################################\n",
    "        # right-to-left step\n",
    "        right_to_left_fiberlist = []\n",
    "        # list col_idx: list (tensor_order-1) of lists of right indices\n",
    "        col_idx = [None] * tensor_order\n",
    "        col_idx[-1] = [()]\n",
    "        for k in range(tensor_order, 1, -1):\n",
    "            (next_col_idx, fibers_list, Q_skeleton) = right_left_ttcross_step(\n",
    "                input_tensor, k, rank, row_idx, col_idx\n",
    "            )\n",
    "            # update col indices\n",
    "            right_to_left_fiberlist.extend(fibers_list)\n",
    "            col_idx[k - 2] = next_col_idx\n",
    "\n",
    "            # Compute cores\n",
    "            try:\n",
    "                factor_new[k - 1] = tl.transpose(Q_skeleton)\n",
    "                factor_new[k - 1] = tl.reshape(\n",
    "                    factor_new[k - 1], (rank[k - 1], tensor_shape[k - 1], rank[k])\n",
    "                )\n",
    "            except:\n",
    "                # The rank should not be larger than the input tensor's size\n",
    "                raise (\n",
    "                    ValueError(\n",
    "                        \"The rank is too large compared to the size of the tensor. Try with small rank.\"\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Add the last core\n",
    "        idx = (slice(None, None, None),) + tuple(zip(*col_idx[0]))\n",
    "\n",
    "        core = input_tensor[idx]\n",
    "        core = tl.reshape(core, (tensor_shape[0], 1, rank[1]))\n",
    "        core = tl.transpose(core, (1, 0, 2))\n",
    "\n",
    "        factor_new[0] = core\n",
    "\n",
    "        # end right-to-left step\n",
    "        ################################################\n",
    "\n",
    "        # check the error for while-loop\n",
    "        error = tl.norm(tt_to_tensor(factor_old) - tt_to_tensor(factor_new), 2)\n",
    "        threshold = tol * tl.norm(tt_to_tensor(factor_new), 2)\n",
    "\n",
    "    # check convergence\n",
    "    if iter >= n_iter_max:\n",
    "        raise ValueError(\"Maximum number of iterations reached.\")\n",
    "    if tl.norm(tt_to_tensor(factor_old) - tt_to_tensor(factor_new), 2) > tol * tl.norm(\n",
    "        tt_to_tensor(factor_new), 2\n",
    "    ):\n",
    "        raise ValueError(\"Low Rank Approximation algorithm did not converge.\")\n",
    "\n",
    "    return factor_new\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def left_right_ttcross_step(input_tensor, k, rank, row_idx, col_idx):\n",
    "    \"\"\"Compute the next (right) core's row indices by QR decomposition.\n",
    "\n",
    "    For the current Tensor train core, we use the row indices and col indices to extract the entries from the input tensor\n",
    "    and compute the next core's row indices by QR and max volume algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    k: int\n",
    "            the actual sweep iteration\n",
    "    rank: list of int\n",
    "            list of upper ranks (tensor_order)\n",
    "    row_idx: list of list of int\n",
    "            list of (tensor_order-1) of lists of left indices\n",
    "    col_idx: list of list of int\n",
    "            list of (tensor_order-1) of lists of right indices\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    next_row_idx : list of int\n",
    "            the list of new row indices,\n",
    "    fibers_list : list of slice\n",
    "            the used fibers,\n",
    "    Q_skeleton : matrix\n",
    "            approximation of Q as product of Q and inverse of its maximum volume submatrix\n",
    "    \"\"\"\n",
    "\n",
    "    tensor_shape = tl.shape(input_tensor)\n",
    "    tensor_order = tl.ndim(input_tensor)\n",
    "    fibers_list = []\n",
    "\n",
    "    # Extract fibers according to the row and col indices\n",
    "    for i in range(rank[k]):\n",
    "        for j in range(rank[k + 1]):\n",
    "            fiber = row_idx[k][i] + (slice(None, None, None),) + col_idx[k][j]\n",
    "            fibers_list.append(fiber)\n",
    "    if k == 0:  # Is[k] will be empty\n",
    "        idx = (slice(None, None, None),) + tuple(zip(*col_idx[k]))\n",
    "    else:\n",
    "        idx = [[] for i in range(tensor_order)]\n",
    "        for lidx in row_idx[k]:\n",
    "            for ridx in col_idx[k]:\n",
    "                for j, jj in enumerate(lidx):\n",
    "                    idx[j].append(jj)\n",
    "                for j, jj in enumerate(ridx):\n",
    "                    idx[len(lidx) + 1 + j].append(jj)\n",
    "        idx[k] = slice(None, None, None)\n",
    "        idx = tuple(idx)\n",
    "\n",
    "    # Extract the core\n",
    "    core = input_tensor[idx]\n",
    "    # shape the core as a 3-tensor_order cube\n",
    "    if k == 0:\n",
    "        core = tl.reshape(core, (tensor_shape[k], rank[k], rank[k + 1]))\n",
    "        core = tl.transpose(core, (1, 0, 2))\n",
    "    else:\n",
    "        core = tl.reshape(core, (rank[k], rank[k + 1], tensor_shape[k]))\n",
    "        core = tl.transpose(core, (0, 2, 1))\n",
    "\n",
    "    # merge r_k and n_k, get a matrix\n",
    "    core = tl.reshape(core, (rank[k] * tensor_shape[k], rank[k + 1]))\n",
    "\n",
    "    # Compute QR decomposition\n",
    "    (Q, R) = tl.qr(core)\n",
    "\n",
    "    # Maxvol\n",
    "    (I, _) = maxvol(Q)\n",
    "\n",
    "    # Retrive indices in folded tensor\n",
    "    new_idx = [\n",
    "        np.unravel_index(idx, [rank[k], tensor_shape[k]]) for idx in I\n",
    "    ]  # First retrive idx in folded core\n",
    "    next_row_idx = [\n",
    "        row_idx[k][ic[0]] + (ic[1],) for ic in new_idx\n",
    "    ]  # Then reconstruct the idx in the tensor\n",
    "\n",
    "    return (next_row_idx, fibers_list)\n",
    "\n",
    "\n",
    "def right_left_ttcross_step(input_tensor, k, rank, row_idx, col_idx):\n",
    "    \"\"\"Compute the next (left) core's col indices by QR decomposition.\n",
    "\n",
    "    For the current Tensor train core, we use the row indices and col indices to extract the entries from the input tensor\n",
    "    and compute the next core's col indices by QR and max volume algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    k: int\n",
    "            the actual sweep iteration\n",
    "    rank: list of int\n",
    "            list of upper rank (tensor_order)\n",
    "    row_idx: list of list of int\n",
    "            list of (tensor_order-1) of lists of left indices\n",
    "    col_idx: list of list of int\n",
    "            list of (tensor_order-1) of lists of right indices\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    next_col_idx : list of int\n",
    "            the list of new col indices,\n",
    "    fibers_list : list of slice\n",
    "            the used fibers,\n",
    "    Q_skeleton : matrix\n",
    "            approximation of Q as product of Q and inverse of its maximum volume submatrix\n",
    "    \"\"\"\n",
    "\n",
    "    tensor_shape = tl.shape(input_tensor)\n",
    "    tensor_order = tl.ndim(input_tensor)\n",
    "    fibers_list = []\n",
    "\n",
    "    # Extract fibers\n",
    "    for i in range(rank[k - 1]):\n",
    "        for j in range(rank[k]):\n",
    "            fiber = row_idx[k - 1][i] + (slice(None, None, None),) + col_idx[k - 1][j]\n",
    "            fibers_list.append(fiber)\n",
    "\n",
    "    if k == tensor_order:  # Is[k] will be empty\n",
    "        idx = tuple(zip(*row_idx[k - 1])) + (slice(None, None, None),)\n",
    "    else:\n",
    "        idx = [[] for i in range(tensor_order)]\n",
    "        for lidx in row_idx[k - 1]:\n",
    "            for ridx in col_idx[k - 1]:\n",
    "                for j, jj in enumerate(lidx):\n",
    "                    idx[j].append(jj)\n",
    "                for j, jj in enumerate(ridx):\n",
    "                    idx[len(lidx) + 1 + j].append(jj)\n",
    "        idx[k - 1] = slice(None, None, None)\n",
    "        idx = tuple(idx)\n",
    "\n",
    "    core = input_tensor[idx]\n",
    "    # shape the core as a 3-tensor_order cube\n",
    "    core = tl.reshape(core, (rank[k - 1], rank[k], tensor_shape[k - 1]))\n",
    "    core = tl.transpose(core, (0, 2, 1))\n",
    "    # merge n_{k-1} and r_k, get a matrix\n",
    "    core = tl.reshape(core, (rank[k - 1], tensor_shape[k - 1] * rank[k]))\n",
    "    core = tl.transpose(core)\n",
    "\n",
    "    # Compute QR decomposition\n",
    "    (Q, R) = tl.qr(core)\n",
    "    # Maxvol\n",
    "    (J, Q_inv) = maxvol(Q)\n",
    "    Q_inv = tl.tensor(Q_inv)\n",
    "    Q_skeleton = tl.dot(Q, Q_inv)\n",
    "\n",
    "    # Retrive indices in folded tensor\n",
    "    new_idx = [\n",
    "        np.unravel_index(idx, [tensor_shape[k - 1], rank[k]]) for idx in J\n",
    "    ]  # First retrive idx in folded core\n",
    "    next_col_idx = [\n",
    "        (jc[0],) + col_idx[k - 1][jc[1]] for jc in new_idx\n",
    "    ]  # Then reconstruct the idx in the tensor\n",
    "\n",
    "    return (next_col_idx, fibers_list, Q_skeleton)\n",
    "\n",
    "\n",
    "def maxvol(A):\n",
    "    \"\"\"Find the rxr submatrix of maximal volume in A(nxr), n>=r\n",
    "\n",
    "    We want to decompose matrix A as `A = A[:,J] * (A[I,J])^-1 * A[I,:]`.\n",
    "    This algorithm helps us find this submatrix A[I,J] from A, which has the largest determinant.\n",
    "    We greedily find vector of max norm, and subtract its projection from the rest of rows.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    A: matrix\n",
    "        The matrix to find maximal volume\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    row_idx: list of int\n",
    "        is the list or rows of A forming the matrix with maximal volume,\n",
    "    A_inv: matrix\n",
    "        is the inverse of the matrix with maximal volume.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    S. A. Goreinov, I. V. Oseledets, D. V. Savostyanov, E. E. Tyrtyshnikov, N. L. Zamarashkin.\n",
    "    How to find a good submatrix.Goreinov, S. A., et al.\n",
    "    Matrix Methods: Theory, Algorithms and Applications: Dedicated to the Memory of Gene Golub. 2010. 247-256.\n",
    "\n",
    "    Ali Çivril, Malik Magdon-Ismail\n",
    "    On selecting a maximum volume sub-matrix of a matrix and related problems\n",
    "    Theoretical Computer Science. Volume 410, Issues 47–49, 6 November 2009, Pages 4801-4811\n",
    "    \"\"\"\n",
    "\n",
    "    (n, r) = tl.shape(A)\n",
    "\n",
    "    # The index of row of the submatrix\n",
    "    row_idx = tl.zeros(r, dtype=tl.int64)\n",
    "\n",
    "    # Rest of rows / unselected rows\n",
    "    rest_of_rows = tl.tensor(list(range(n)), dtype=tl.int64)\n",
    "\n",
    "    # Find r rows iteratively\n",
    "    i = 0\n",
    "    A_new = A\n",
    "    while i < r:\n",
    "        mask = list(range(tl.shape(A_new)[0]))\n",
    "        # Compute the square of norm of each row\n",
    "        rows_norms = tl.sum(A_new**2, axis=1)\n",
    "\n",
    "        # If there is only one row of A left, let's just return it.\n",
    "        if tl.shape(rows_norms) == ():\n",
    "            row_idx[i] = rest_of_rows\n",
    "            break\n",
    "\n",
    "        # If a row is 0, we delete it.\n",
    "        if any(rows_norms == 0):\n",
    "            zero_idx = tl.argmin(rows_norms, axis=0)\n",
    "            mask.pop(zero_idx)\n",
    "            rest_of_rows = rest_of_rows[mask]\n",
    "            A_new = A_new[mask, :]\n",
    "            continue\n",
    "\n",
    "        # Find the row of max norm\n",
    "        max_row_idx = tl.argmax(rows_norms, axis=0)\n",
    "        max_row = A[rest_of_rows[max_row_idx], :]\n",
    "\n",
    "        # Compute the projection of max_row to other rows\n",
    "        # projection a to b is computed as: <a,b> / sqrt(|a|*|b|)\n",
    "        projection = tl.dot(A_new, tl.transpose(max_row))\n",
    "        normalization = tl.sqrt(rows_norms[max_row_idx] * rows_norms)\n",
    "        # make sure normalization vector is of the same shape of projection\n",
    "        normalization = tl.reshape(normalization, tl.shape(projection))\n",
    "        projection = projection / normalization\n",
    "\n",
    "        # Subtract the projection from A_new:  b <- b - a * projection\n",
    "        A_new = A_new - A_new * tl.reshape(projection, (tl.shape(A_new)[0], 1))\n",
    "\n",
    "        # Delete the selected row\n",
    "        mask.pop(tl.to_numpy(max_row_idx))\n",
    "        A_new = A_new[mask, :]\n",
    "\n",
    "        # update the row_idx and rest_of_rows\n",
    "        row_idx = tl.index_update(row_idx, i, rest_of_rows[max_row_idx])\n",
    "        rest_of_rows = rest_of_rows[tl.tensor(mask, dtype=tl.int64)]\n",
    "        i = i + 1\n",
    "\n",
    "    row_idx = tl.tensor(row_idx, dtype=tl.int64)\n",
    "    inverse = tl.solve(\n",
    "        A[row_idx, :], tl.eye(tl.shape(A[row_idx, :])[0], **tl.context(A))\n",
    "    )\n",
    "    row_idx = tl.to_numpy(row_idx)\n",
    "\n",
    "    return row_idx, inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "200bcb48-8b49-4998-a10c-1acbdde45fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, slice(None, None, None), 20)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(10,) + (slice(None, None, None),) + (20,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "728b945a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[()]]\n",
      "[[(4, 2), (4, 3), (4, 0)], [(0,), (3,), (1,)], None]\n"
     ]
    }
   ],
   "source": [
    "tensor = tl.tensor(np.arange(5**3).reshape(5,5,5))\n",
    "rank = [1, 3, 3, 1]\n",
    "factors = tensor_train_cross(tensor, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eaf3ce16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4],\n",
       "       [ 5,  6,  7,  8,  9],\n",
       "       [10, 11, 12, 13, 14],\n",
       "       [15, 16, 17, 18, 19],\n",
       "       [20, 21, 22, 23, 24]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91b0e0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(slice(None, None, None), (4, 4, 4), (2, 3, 0))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 22,  23,  20],\n",
       "       [ 47,  48,  45],\n",
       "       [ 72,  73,  70],\n",
       "       [ 97,  98,  95],\n",
       "       [122, 123, 120]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_idx = [[()]]\n",
    "col_idx = [[(4, 2), (4, 3), (4, 0)], [(0,), (3,), (1,)], None]\n",
    "tensor_order = 2\n",
    "k = 0\n",
    "idx = (slice(None, None, None),) + tuple(zip(*col_idx[k]))\n",
    "\n",
    "idx = tuple(idx)\n",
    "print(idx)\n",
    "tensor[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed1b5660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0,   1,   2,   3,   4],\n",
       "        [  5,   6,   7,   8,   9],\n",
       "        [ 10,  11,  12,  13,  14],\n",
       "        [ 15,  16,  17,  18,  19],\n",
       "        [ 20,  21,  22,  23,  24]],\n",
       "\n",
       "       [[ 25,  26,  27,  28,  29],\n",
       "        [ 30,  31,  32,  33,  34],\n",
       "        [ 35,  36,  37,  38,  39],\n",
       "        [ 40,  41,  42,  43,  44],\n",
       "        [ 45,  46,  47,  48,  49]],\n",
       "\n",
       "       [[ 50,  51,  52,  53,  54],\n",
       "        [ 55,  56,  57,  58,  59],\n",
       "        [ 60,  61,  62,  63,  64],\n",
       "        [ 65,  66,  67,  68,  69],\n",
       "        [ 70,  71,  72,  73,  74]],\n",
       "\n",
       "       [[ 75,  76,  77,  78,  79],\n",
       "        [ 80,  81,  82,  83,  84],\n",
       "        [ 85,  86,  87,  88,  89],\n",
       "        [ 90,  91,  92,  93,  94],\n",
       "        [ 95,  96,  97,  98,  99]],\n",
       "\n",
       "       [[100, 101, 102, 103, 104],\n",
       "        [105, 106, 107, 108, 109],\n",
       "        [110, 111, 112, 113, 114],\n",
       "        [115, 116, 117, 118, 119],\n",
       "        [120, 121, 122, 123, 124]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "52e8ee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import svd, norm\n",
    "\n",
    "def initialize_tt(tensor, max_rank):\n",
    "    \"\"\"\n",
    "    Initialize TT decomposition using random cores.\n",
    "    \"\"\"\n",
    "    shape = tensor.shape\n",
    "    d = len(shape)\n",
    "    ranks = [1] + [min(max_rank, shape[i]) for i in range(1, d)] + [1]\n",
    "    tt_cores = []\n",
    "    \n",
    "    for i in range(d):\n",
    "        core_shape = (ranks[i], shape[i], ranks[i+1])\n",
    "        tt_cores.append(np.random.randn(*core_shape))\n",
    "    \n",
    "    return tt_cores\n",
    "\n",
    "def compute_left_contraction(tt_cores, k, tensor_shape):\n",
    "    \"\"\"\n",
    "    Compute the left contraction up to core k.\n",
    "    \"\"\"\n",
    "    left_contract = np.eye(tt_cores[0].shape[0])  # Start with identity for contraction\n",
    "    for i in range(k):\n",
    "        left_core = tt_cores[i]\n",
    "        left_contract = np.tensordot(left_contract, left_core, axes=[-1, 0])\n",
    "        left_contract = left_contract.reshape(-1, tensor_shape[i])  # Reshape for next contraction\n",
    "    return left_contract\n",
    "\n",
    "def compute_right_contraction(tt_cores, k, tensor_shape):\n",
    "    \"\"\"\n",
    "    Compute the right contraction from core k+1.\n",
    "    \"\"\"\n",
    "    right_contract = np.eye(tt_cores[-1].shape[-1])  # Start with identity for contraction\n",
    "    for i in range(len(tt_cores) - 1, k, -1):\n",
    "        right_core = tt_cores[i]\n",
    "        right_contract = np.tensordot(right_core, right_contract, axes=[-1, 0])\n",
    "        right_contract = right_contract.reshape(tensor_shape[i], -1)  # Reshape for next contraction\n",
    "    return right_contract\n",
    "\n",
    "def flatten_tensor(left_contract, right_contract, tensor, mode_dim):\n",
    "    \"\"\"\n",
    "    Flatten the tensor for matrix representation (cross approximation).\n",
    "    \"\"\"\n",
    "    full_contract = np.tensordot(left_contract, right_contract, axes=0)\n",
    "    flat_tensor = np.tensordot(tensor, full_contract, axes=mode_dim)\n",
    "    return flat_tensor.reshape(-1, flat_tensor.shape[-1])  # Matrix format\n",
    "\n",
    "def cross_approximation(matrix, max_rank):\n",
    "    \"\"\"\n",
    "    Perform cross approximation or SVD on the matrix.\n",
    "    \"\"\"\n",
    "    u, s, vh = svd(matrix, full_matrices=False)\n",
    "    rank = min(len(s), max_rank)\n",
    "    truncated_core = u[:, :rank] @ np.diag(s[:rank]) @ vh[:rank, :]\n",
    "    return truncated_core\n",
    "\n",
    "def truncate_core(core, max_rank):\n",
    "    \"\"\"\n",
    "    Truncate a TT core using SVD.\n",
    "    \"\"\"\n",
    "    unfolded = core.reshape(core.shape[0] * core.shape[1], -1)\n",
    "    u, s, vh = svd(unfolded, full_matrices=False)\n",
    "    rank = min(len(s), max_rank)\n",
    "    truncated_core = u[:, :rank] @ np.diag(s[:rank]) @ vh[:rank, :]\n",
    "    return truncated_core\n",
    "\n",
    "def tensor_train_dmrg_cross(tensor, max_rank, tol):\n",
    "    \"\"\"\n",
    "    Perform Tensor Train DMRG Cross Algorithm.\n",
    "    \"\"\"\n",
    "    shape = tensor.shape\n",
    "    tt_cores = initialize_tt(tensor, max_rank)\n",
    "    \n",
    "    converged = False\n",
    "    while not converged:\n",
    "        max_error = 0\n",
    "        for k in range(len(tt_cores)):\n",
    "            # Compute left and right contractions\n",
    "            left_contract = compute_left_contraction(tt_cores, k, shape)\n",
    "            right_contract = compute_right_contraction(tt_cores, k, shape)\n",
    "            \n",
    "            # Flatten tensor and compute cross approximation\n",
    "            mode_dim = (list(range(k)), list(range(k + 1, len(tt_cores))))\n",
    "            flat_tensor = flatten_tensor(left_contract, right_contract, tensor, mode_dim)\n",
    "            updated_core = cross_approximation(flat_tensor, max_rank)\n",
    "            \n",
    "            # Update and truncate the core\n",
    "            tt_cores[k] = truncate_core(updated_core, max_rank)\n",
    "        \n",
    "            # Calculate error for convergence\n",
    "            approx_tensor = reconstruct_tensor(tt_cores)\n",
    "            error = norm(tensor - approx_tensor) / norm(tensor)\n",
    "            max_error = max(max_error, error)\n",
    "        \n",
    "        if max_error < tol:\n",
    "            converged = True\n",
    "    \n",
    "    return tt_cores\n",
    "\n",
    "def reconstruct_tensor(tt_cores):\n",
    "    \"\"\"\n",
    "    Reconstruct the full tensor from TT cores.\n",
    "    \"\"\"\n",
    "    tensor = tt_cores[0]\n",
    "    for i in range(1, len(tt_cores)):\n",
    "        tensor = np.tensordot(tensor, tt_cores[i], axes=[-1, 0])\n",
    "    return tensor.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "01ca26b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shape-mismatch for sum",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m tol \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Decompose tensor into TT format\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m tt_cores \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_train_dmrg_cross\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Reconstruct the tensor and verify the approximation\u001b[39;00m\n\u001b[1;32m     12\u001b[0m approx_tensor \u001b[38;5;241m=\u001b[39m reconstruct_tensor(tt_cores)\n",
      "Cell \u001b[0;32mIn[55], line 81\u001b[0m, in \u001b[0;36mtensor_train_dmrg_cross\u001b[0;34m(tensor, max_rank, tol)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tt_cores)):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Compute left and right contractions\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     left_contract \u001b[38;5;241m=\u001b[39m compute_left_contraction(tt_cores, k, shape)\n\u001b[0;32m---> 81\u001b[0m     right_contract \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_right_contraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtt_cores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# Flatten tensor and compute cross approximation\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     mode_dim \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(k)), \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(k \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tt_cores))))\n",
      "Cell \u001b[0;32mIn[55], line 37\u001b[0m, in \u001b[0;36mcompute_right_contraction\u001b[0;34m(tt_cores, k, tensor_shape)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tt_cores) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, k, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     36\u001b[0m     right_core \u001b[38;5;241m=\u001b[39m tt_cores[i]\n\u001b[0;32m---> 37\u001b[0m     right_contract \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensordot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright_core\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_contract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     right_contract \u001b[38;5;241m=\u001b[39m right_contract\u001b[38;5;241m.\u001b[39mreshape(tensor_shape[i], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Reshape for next contraction\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m right_contract\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/numpy/core/numeric.py:1099\u001b[0m, in \u001b[0;36mtensordot\u001b[0;34m(a, b, axes)\u001b[0m\n\u001b[1;32m   1097\u001b[0m             axes_b[k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ndb\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m equal:\n\u001b[0;32m-> 1099\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape-mismatch for sum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;66;03m# Move the axes to sum over to the end of \"a\"\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;66;03m# and to the front of \"b\"\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m notin \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nda) \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m axes_a]\n",
      "\u001b[0;31mValueError\u001b[0m: shape-mismatch for sum"
     ]
    }
   ],
   "source": [
    "# Define a 3D tensor\n",
    "tensor = np.random.randn(10, 10, 10)\n",
    "\n",
    "# Set maximum TT rank and tolerance\n",
    "max_rank = 5\n",
    "tol = 1e-6\n",
    "\n",
    "# Decompose tensor into TT format\n",
    "tt_cores = tensor_train_dmrg_cross(tensor, max_rank, tol)\n",
    "\n",
    "# Reconstruct the tensor and verify the approximation\n",
    "approx_tensor = reconstruct_tensor(tt_cores)\n",
    "error = norm(tensor - approx_tensor) / norm(tensor)\n",
    "print(f\"Relative approximation error: {error:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d636f87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_mkl",
   "language": "python",
   "name": "torch_mkl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
