{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f36d0aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.33226763e-15])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ttml.tt_cross import estimator_to_tt_cross\n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(np.sum(x, axis=1))\n",
    "\n",
    "# Same thresholds for every feature: [0,0.2,0.4,0.6,0.8]\n",
    "thresholds = [np.linspace(0, 1, 11)] * 5\n",
    "\n",
    "tt = estimator_to_tt_cross(f, thresholds)\n",
    "tt.gather(np.array([[2, 2, 2, 2, 2]])) - np.sin(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4894656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2.62706082e+02, 1.10057236e+02, 3.76917023e-14, 3.12951706e-14,\n",
       "        2.54384564e-14]),\n",
       " array([2.50489642e+02, 1.35580309e+02, 5.75686287e-14, 3.40266541e-14,\n",
       "        2.76409009e-14]),\n",
       " array([2.50489642e+02, 1.35580309e+02, 3.98017475e-14, 3.31172563e-14,\n",
       "        2.56110372e-14]),\n",
       " array([2.62706082e+02, 1.10057236e+02, 2.55305838e-14, 2.38288327e-14,\n",
       "        2.14653461e-14])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.sing_vals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dc880b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorTrain of order 5 with outer dimensions (11, 11, 11, 11, 11), TT-rank (2, 2, 2, 2), and orthogonalized at mode 4>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt2 = estimator_to_tt_cross(f, thresholds, max_rank=2)\n",
    "\n",
    "tt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b6ea261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.36536092976611e-13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tt - tt2).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5fe8d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorTrain of order 5 with outer dimensions (11, 11, 11, 11, 11), TT-rank (2, 2, 2, 2), and orthogonalized at mode 4>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X = np.random.normal(size=(1000, 5))\n",
    "y = np.exp(np.sum(X, axis=1))\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest.fit(X, y)\n",
    "\n",
    "thresholds = [np.linspace(0, 1, 11)] * 5\n",
    "\n",
    "tt = estimator_to_tt_cross(forest.predict, thresholds, max_rank=2)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "897683ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = estimator_to_tt_cross(forest.predict, thresholds, method='dmrg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bea86fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly as tl\n",
    "from tensorly.contrib.decomposition import tensor_train_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05fb9324-d27f-4467-97cc-a083b2c547e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tl.tensor(np.arange(5**3).reshape(5,5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a26d379e-46a9-4b0c-858f-060a56d39b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = [1, 3, 3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1986c33-5563-43e3-906c-ffbdac36eeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = tensor_train_cross(tensor, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b03cbdf-3bbd-4762-b72f-8ff07a2b9033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  0  24   2]\n",
      "  [ 25  49  27]\n",
      "  [ 50  74  52]\n",
      "  [ 75  99  77]\n",
      "  [100 124 102]]]\n"
     ]
    }
   ],
   "source": [
    "print(factors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "717a81bf-367b-4337-bd2b-f95da9dc2ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly as tl\n",
    "from tensorly import tt_to_tensor\n",
    "import numpy as np\n",
    "\n",
    "def tensor_train_cross(input_tensor, rank, tol=1e-4, n_iter_max=100, random_state=None):\n",
    "    \"\"\"TT (tensor-train) decomposition via cross-approximation (TTcross) [1]\n",
    "\n",
    "    Decomposes `input_tensor` into a sequence of order-3 tensors of given rank. (factors/cores)\n",
    "    Rather than directly decompose the whole tensor, we sample fibers based on skeleton decomposition.\n",
    "    We initialize a random tensor-train and sweep from left to right and right to left.\n",
    "    On each core, we shape the core as a matrix and choose the fibers indices by finding maximum-volume submatrix and update the core.\n",
    "\n",
    "    * Advantage: faster\n",
    "        The main advantage of TTcross is that it doesn't need to evaluate all the entries of the tensor.\n",
    "        For a tensor_shape^tensor_order tensor, SVD needs O(tensor_shape^tensor_order) runtime, but TTcross' runtime is linear in tensor_shape and tensor_order, which makes it feasible in high dimension.\n",
    "    * Disadvantage: less accurate\n",
    "        TTcross may underestimate the error, since it only evaluates partial entries of the tensor.\n",
    "        Besides, in contrast to its practical fast performance, there is no theoretical guarantee of it convergence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_tensor : tensorly.tensor\n",
    "            The tensor to decompose.\n",
    "    rank : {int, int list}\n",
    "            maximum allowable TT rank of the factors\n",
    "            if int, then this is the same for all the factors\n",
    "            if int list, then rank[k] is the rank of the kth factor\n",
    "    tol : float\n",
    "            accuracy threshold for outer while-loop\n",
    "    n_iter_max : int\n",
    "            maximum iterations of outer while-loop (the 'crosses' or 'sweeps' sampled)\n",
    "    random_state : {None, int, np.random.RandomState}\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    factors : TT factors\n",
    "              order-3 tensors of the TT decomposition\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    Generate a 5^3 tensor, and decompose it into tensor-train of 3 factors, with rank = [1,3,3,1]\n",
    "\n",
    "    >>> tensor = tl.tensor(np.arange(5**3).reshape(5,5,5))\n",
    "    >>> rank = [1, 3, 3, 1]\n",
    "    >>> factors = tensor_train_cross(tensor, rank)\n",
    "    >>> # print the first core:\n",
    "    >>> print(factors[0])\n",
    "    [[[ 24.   0.   4.]\n",
    "      [ 49.  25.  29.]\n",
    "      [ 74.  50.  54.]\n",
    "      [ 99.  75.  79.]\n",
    "      [124. 100. 104.]]]\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Pseudo-code [2]:\n",
    "\n",
    "    1. Initialization tensor_order cores and column indices\n",
    "\n",
    "    2. while (error > tol)\n",
    "\n",
    "    3. update the tensor-train from left to right:\n",
    "\n",
    "       .. code:: python\n",
    "\n",
    "           for Core 1 to Core tensor_order:\n",
    "           approximate the skeleton-decomposition by QR and maxvol\n",
    "\n",
    "    4. update the tensor-train from right to left:\n",
    "\n",
    "       .. code:: python\n",
    "\n",
    "            for Core tensor_order to Core 1\n",
    "                approximate the skeleton-decomposition by QR and maxvol\n",
    "\n",
    "    5. end while\n",
    "\n",
    "    Acknowledgement: the main body of the code is modified based on TensorToolbox by Daniele Bigoni.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Ivan Oseledets and Eugene Tyrtyshnikov.  Tt-cross approximation for multidimensional arrays.\n",
    "            LinearAlgebra and its Applications, 432(1):70–88, 2010.\n",
    "    .. [2] Sergey Dolgov and Robert Scheichl. A hybrid alternating least squares–tt cross algorithm for parametricpdes.\n",
    "            arXiv preprint arXiv:1707.04562, 2017.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check user input for errors\n",
    "    tensor_shape = tl.shape(input_tensor)\n",
    "    tensor_order = tl.ndim(input_tensor)\n",
    "\n",
    "    if isinstance(rank, int):\n",
    "        rank = [rank] * (tensor_order + 1)\n",
    "    elif tensor_order + 1 != len(rank):\n",
    "        message = (\n",
    "            \"Provided incorrect number of ranks. Should verify \"\n",
    "            + f\"len(rank) == tl.ndim(tensor)+1, but len(rank) = {len(rank)} \"\n",
    "            + f\"while tl.ndim(tensor) + 1  = {tensor_order}\"\n",
    "        )\n",
    "        raise (ValueError(message))\n",
    "\n",
    "    # Make sure iter's not a tuple but a list\n",
    "    rank = list(rank)\n",
    "\n",
    "    # Initialize rank\n",
    "    if rank[0] != 1:\n",
    "        message = f\"Provided rank[0] == {rank[0]} but boundary conditions dictate rank[0] == rank[-1] == 1.\"\n",
    "        raise ValueError(message)\n",
    "    if rank[-1] != 1:\n",
    "        message = f\"Provided rank[-1] == {rank[-1]} but boundary conditions dictate rank[0] == rank[-1] == 1.\"\n",
    "        raise ValueError(message)\n",
    "\n",
    "    # list col_idx: column indices (right indices) for skeleton-decomposition: indicate which columns used in each core.\n",
    "    # list row_idx: row indices    (left indices)  for skeleton-decomposition: indicate which rows used in each core.\n",
    "\n",
    "    # Initialize indice: random selection of column indices\n",
    "    rng = tl.check_random_state(random_state)\n",
    "\n",
    "    col_idx = [None] * tensor_order\n",
    "    for k_col_idx in range(tensor_order - 1):\n",
    "        col_idx[k_col_idx] = []\n",
    "        for i in range(rank[k_col_idx + 1]):\n",
    "            newidx = tuple(\n",
    "                [\n",
    "                    rng.randint(tensor_shape[j])\n",
    "                    for j in range(k_col_idx + 1, tensor_order)\n",
    "                ]\n",
    "            )\n",
    "            while newidx in col_idx[k_col_idx]:\n",
    "                newidx = tuple(\n",
    "                    [\n",
    "                        rng.randint(tensor_shape[j])\n",
    "                        for j in range(k_col_idx + 1, tensor_order)\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            col_idx[k_col_idx].append(newidx)\n",
    "\n",
    "    # Initialize the cores of tensor-train\n",
    "    factor_old = [\n",
    "        tl.zeros((rank[k], tensor_shape[k], rank[k + 1]), **tl.context(input_tensor))\n",
    "        for k in range(tensor_order)\n",
    "    ]\n",
    "    factor_new = [\n",
    "        tl.tensor(\n",
    "            rng.random_sample((rank[k], tensor_shape[k], rank[k + 1])),\n",
    "            **tl.context(input_tensor),\n",
    "        )\n",
    "        for k in range(tensor_order)\n",
    "    ]\n",
    "\n",
    "    iter = 0\n",
    "\n",
    "    error = tl.norm(tt_to_tensor(factor_old) - tt_to_tensor(factor_new), 2)\n",
    "    threshold = tol * tl.norm(tt_to_tensor(factor_new), 2)\n",
    "    for iter in range(n_iter_max):\n",
    "        if error < threshold:\n",
    "            break\n",
    "\n",
    "        factor_old = factor_new\n",
    "        factor_new = [None for i in range(tensor_order)]\n",
    "\n",
    "        ######################################\n",
    "        # left-to-right step\n",
    "        left_to_right_fiberlist = []\n",
    "        # list row_idx: list of (tensor_order-1) of lists of left indices\n",
    "        row_idx = [[()]]\n",
    "        for k in range(tensor_order - 1):\n",
    "            (next_row_idx, fibers_list) = left_right_ttcross_step(\n",
    "                input_tensor, k, rank, row_idx, col_idx\n",
    "            )\n",
    "            # update row indices\n",
    "            left_to_right_fiberlist.extend(fibers_list)\n",
    "            row_idx.append(next_row_idx)\n",
    "\n",
    "        # end left-to-right step\n",
    "        ###############################################\n",
    "\n",
    "        ###############################################\n",
    "        # right-to-left step\n",
    "        right_to_left_fiberlist = []\n",
    "        # list col_idx: list (tensor_order-1) of lists of right indices\n",
    "        col_idx = [None] * tensor_order\n",
    "        col_idx[-1] = [()]\n",
    "        for k in range(tensor_order, 1, -1):\n",
    "            (next_col_idx, fibers_list, Q_skeleton) = right_left_ttcross_step(\n",
    "                input_tensor, k, rank, row_idx, col_idx\n",
    "            )\n",
    "            # update col indices\n",
    "            right_to_left_fiberlist.extend(fibers_list)\n",
    "            col_idx[k - 2] = next_col_idx\n",
    "\n",
    "            # Compute cores\n",
    "            try:\n",
    "                factor_new[k - 1] = tl.transpose(Q_skeleton)\n",
    "                factor_new[k - 1] = tl.reshape(\n",
    "                    factor_new[k - 1], (rank[k - 1], tensor_shape[k - 1], rank[k])\n",
    "                )\n",
    "            except:\n",
    "                # The rank should not be larger than the input tensor's size\n",
    "                raise (\n",
    "                    ValueError(\n",
    "                        \"The rank is too large compared to the size of the tensor. Try with small rank.\"\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Add the last core\n",
    "        idx = (slice(None, None, None),) + tuple(zip(*col_idx[0]))\n",
    "\n",
    "        core = input_tensor[idx]\n",
    "        core = tl.reshape(core, (tensor_shape[0], 1, rank[1]))\n",
    "        core = tl.transpose(core, (1, 0, 2))\n",
    "\n",
    "        factor_new[0] = core\n",
    "\n",
    "        # end right-to-left step\n",
    "        ################################################\n",
    "\n",
    "        # check the error for while-loop\n",
    "        error = tl.norm(tt_to_tensor(factor_old) - tt_to_tensor(factor_new), 2)\n",
    "        threshold = tol * tl.norm(tt_to_tensor(factor_new), 2)\n",
    "\n",
    "    # check convergence\n",
    "    if iter >= n_iter_max:\n",
    "        raise ValueError(\"Maximum number of iterations reached.\")\n",
    "    if tl.norm(tt_to_tensor(factor_old) - tt_to_tensor(factor_new), 2) > tol * tl.norm(\n",
    "        tt_to_tensor(factor_new), 2\n",
    "    ):\n",
    "        raise ValueError(\"Low Rank Approximation algorithm did not converge.\")\n",
    "\n",
    "    return factor_new\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def left_right_ttcross_step(input_tensor, k, rank, row_idx, col_idx):\n",
    "    \"\"\"Compute the next (right) core's row indices by QR decomposition.\n",
    "\n",
    "    For the current Tensor train core, we use the row indices and col indices to extract the entries from the input tensor\n",
    "    and compute the next core's row indices by QR and max volume algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    k: int\n",
    "            the actual sweep iteration\n",
    "    rank: list of int\n",
    "            list of upper ranks (tensor_order)\n",
    "    row_idx: list of list of int\n",
    "            list of (tensor_order-1) of lists of left indices\n",
    "    col_idx: list of list of int\n",
    "            list of (tensor_order-1) of lists of right indices\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    next_row_idx : list of int\n",
    "            the list of new row indices,\n",
    "    fibers_list : list of slice\n",
    "            the used fibers,\n",
    "    Q_skeleton : matrix\n",
    "            approximation of Q as product of Q and inverse of its maximum volume submatrix\n",
    "    \"\"\"\n",
    "\n",
    "    tensor_shape = tl.shape(input_tensor)\n",
    "    tensor_order = tl.ndim(input_tensor)\n",
    "    fibers_list = []\n",
    "\n",
    "    # Extract fibers according to the row and col indices\n",
    "    for i in range(rank[k]):\n",
    "        for j in range(rank[k + 1]):\n",
    "            fiber = row_idx[k][i] + (slice(None, None, None),) + col_idx[k][j]\n",
    "            fibers_list.append(fiber)\n",
    "    if k == 0:  # Is[k] will be empty\n",
    "        idx = (slice(None, None, None),) + tuple(zip(*col_idx[k]))\n",
    "    else:\n",
    "        idx = [[] for i in range(tensor_order)]\n",
    "        for lidx in row_idx[k]:\n",
    "            for ridx in col_idx[k]:\n",
    "                for j, jj in enumerate(lidx):\n",
    "                    idx[j].append(jj)\n",
    "                for j, jj in enumerate(ridx):\n",
    "                    idx[len(lidx) + 1 + j].append(jj)\n",
    "        idx[k] = slice(None, None, None)\n",
    "        idx = tuple(idx)\n",
    "\n",
    "    # Extract the core\n",
    "    core = input_tensor[idx]\n",
    "    # shape the core as a 3-tensor_order cube\n",
    "    if k == 0:\n",
    "        core = tl.reshape(core, (tensor_shape[k], rank[k], rank[k + 1]))\n",
    "        core = tl.transpose(core, (1, 0, 2))\n",
    "    else:\n",
    "        core = tl.reshape(core, (rank[k], rank[k + 1], tensor_shape[k]))\n",
    "        core = tl.transpose(core, (0, 2, 1))\n",
    "\n",
    "    # merge r_k and n_k, get a matrix\n",
    "    core = tl.reshape(core, (rank[k] * tensor_shape[k], rank[k + 1]))\n",
    "\n",
    "    # Compute QR decomposition\n",
    "    (Q, R) = tl.qr(core)\n",
    "\n",
    "    # Maxvol\n",
    "    (I, _) = maxvol(Q)\n",
    "\n",
    "    # Retrive indices in folded tensor\n",
    "    new_idx = [\n",
    "        np.unravel_index(idx, [rank[k], tensor_shape[k]]) for idx in I\n",
    "    ]  # First retrive idx in folded core\n",
    "    next_row_idx = [\n",
    "        row_idx[k][ic[0]] + (ic[1],) for ic in new_idx\n",
    "    ]  # Then reconstruct the idx in the tensor\n",
    "\n",
    "    return (next_row_idx, fibers_list)\n",
    "\n",
    "\n",
    "def right_left_ttcross_step(input_tensor, k, rank, row_idx, col_idx):\n",
    "    \"\"\"Compute the next (left) core's col indices by QR decomposition.\n",
    "\n",
    "    For the current Tensor train core, we use the row indices and col indices to extract the entries from the input tensor\n",
    "    and compute the next core's col indices by QR and max volume algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    k: int\n",
    "            the actual sweep iteration\n",
    "    rank: list of int\n",
    "            list of upper rank (tensor_order)\n",
    "    row_idx: list of list of int\n",
    "            list of (tensor_order-1) of lists of left indices\n",
    "    col_idx: list of list of int\n",
    "            list of (tensor_order-1) of lists of right indices\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    next_col_idx : list of int\n",
    "            the list of new col indices,\n",
    "    fibers_list : list of slice\n",
    "            the used fibers,\n",
    "    Q_skeleton : matrix\n",
    "            approximation of Q as product of Q and inverse of its maximum volume submatrix\n",
    "    \"\"\"\n",
    "\n",
    "    tensor_shape = tl.shape(input_tensor)\n",
    "    tensor_order = tl.ndim(input_tensor)\n",
    "    fibers_list = []\n",
    "\n",
    "    # Extract fibers\n",
    "    for i in range(rank[k - 1]):\n",
    "        for j in range(rank[k]):\n",
    "            fiber = row_idx[k - 1][i] + (slice(None, None, None),) + col_idx[k - 1][j]\n",
    "            fibers_list.append(fiber)\n",
    "\n",
    "    if k == tensor_order:  # Is[k] will be empty\n",
    "        idx = tuple(zip(*row_idx[k - 1])) + (slice(None, None, None),)\n",
    "    else:\n",
    "        idx = [[] for i in range(tensor_order)]\n",
    "        for lidx in row_idx[k - 1]:\n",
    "            for ridx in col_idx[k - 1]:\n",
    "                for j, jj in enumerate(lidx):\n",
    "                    idx[j].append(jj)\n",
    "                for j, jj in enumerate(ridx):\n",
    "                    idx[len(lidx) + 1 + j].append(jj)\n",
    "        idx[k - 1] = slice(None, None, None)\n",
    "        idx = tuple(idx)\n",
    "\n",
    "    core = input_tensor[idx]\n",
    "    # shape the core as a 3-tensor_order cube\n",
    "    core = tl.reshape(core, (rank[k - 1], rank[k], tensor_shape[k - 1]))\n",
    "    core = tl.transpose(core, (0, 2, 1))\n",
    "    # merge n_{k-1} and r_k, get a matrix\n",
    "    core = tl.reshape(core, (rank[k - 1], tensor_shape[k - 1] * rank[k]))\n",
    "    core = tl.transpose(core)\n",
    "\n",
    "    # Compute QR decomposition\n",
    "    (Q, R) = tl.qr(core)\n",
    "    # Maxvol\n",
    "    (J, Q_inv) = maxvol(Q)\n",
    "    Q_inv = tl.tensor(Q_inv)\n",
    "    Q_skeleton = tl.dot(Q, Q_inv)\n",
    "\n",
    "    # Retrive indices in folded tensor\n",
    "    new_idx = [\n",
    "        np.unravel_index(idx, [tensor_shape[k - 1], rank[k]]) for idx in J\n",
    "    ]  # First retrive idx in folded core\n",
    "    next_col_idx = [\n",
    "        (jc[0],) + col_idx[k - 1][jc[1]] for jc in new_idx\n",
    "    ]  # Then reconstruct the idx in the tensor\n",
    "\n",
    "    return (next_col_idx, fibers_list, Q_skeleton)\n",
    "\n",
    "\n",
    "def maxvol(A):\n",
    "    \"\"\"Find the rxr submatrix of maximal volume in A(nxr), n>=r\n",
    "\n",
    "    We want to decompose matrix A as `A = A[:,J] * (A[I,J])^-1 * A[I,:]`.\n",
    "    This algorithm helps us find this submatrix A[I,J] from A, which has the largest determinant.\n",
    "    We greedily find vector of max norm, and subtract its projection from the rest of rows.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    A: matrix\n",
    "        The matrix to find maximal volume\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    row_idx: list of int\n",
    "        is the list or rows of A forming the matrix with maximal volume,\n",
    "    A_inv: matrix\n",
    "        is the inverse of the matrix with maximal volume.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    S. A. Goreinov, I. V. Oseledets, D. V. Savostyanov, E. E. Tyrtyshnikov, N. L. Zamarashkin.\n",
    "    How to find a good submatrix.Goreinov, S. A., et al.\n",
    "    Matrix Methods: Theory, Algorithms and Applications: Dedicated to the Memory of Gene Golub. 2010. 247-256.\n",
    "\n",
    "    Ali Çivril, Malik Magdon-Ismail\n",
    "    On selecting a maximum volume sub-matrix of a matrix and related problems\n",
    "    Theoretical Computer Science. Volume 410, Issues 47–49, 6 November 2009, Pages 4801-4811\n",
    "    \"\"\"\n",
    "\n",
    "    (n, r) = tl.shape(A)\n",
    "\n",
    "    # The index of row of the submatrix\n",
    "    row_idx = tl.zeros(r, dtype=tl.int64)\n",
    "\n",
    "    # Rest of rows / unselected rows\n",
    "    rest_of_rows = tl.tensor(list(range(n)), dtype=tl.int64)\n",
    "\n",
    "    # Find r rows iteratively\n",
    "    i = 0\n",
    "    A_new = A\n",
    "    while i < r:\n",
    "        mask = list(range(tl.shape(A_new)[0]))\n",
    "        # Compute the square of norm of each row\n",
    "        rows_norms = tl.sum(A_new**2, axis=1)\n",
    "\n",
    "        # If there is only one row of A left, let's just return it.\n",
    "        if tl.shape(rows_norms) == ():\n",
    "            row_idx[i] = rest_of_rows\n",
    "            break\n",
    "\n",
    "        # If a row is 0, we delete it.\n",
    "        if any(rows_norms == 0):\n",
    "            zero_idx = tl.argmin(rows_norms, axis=0)\n",
    "            mask.pop(zero_idx)\n",
    "            rest_of_rows = rest_of_rows[mask]\n",
    "            A_new = A_new[mask, :]\n",
    "            continue\n",
    "\n",
    "        # Find the row of max norm\n",
    "        max_row_idx = tl.argmax(rows_norms, axis=0)\n",
    "        max_row = A[rest_of_rows[max_row_idx], :]\n",
    "\n",
    "        # Compute the projection of max_row to other rows\n",
    "        # projection a to b is computed as: <a,b> / sqrt(|a|*|b|)\n",
    "        projection = tl.dot(A_new, tl.transpose(max_row))\n",
    "        normalization = tl.sqrt(rows_norms[max_row_idx] * rows_norms)\n",
    "        # make sure normalization vector is of the same shape of projection\n",
    "        normalization = tl.reshape(normalization, tl.shape(projection))\n",
    "        projection = projection / normalization\n",
    "\n",
    "        # Subtract the projection from A_new:  b <- b - a * projection\n",
    "        A_new = A_new - A_new * tl.reshape(projection, (tl.shape(A_new)[0], 1))\n",
    "\n",
    "        # Delete the selected row\n",
    "        mask.pop(tl.to_numpy(max_row_idx))\n",
    "        A_new = A_new[mask, :]\n",
    "\n",
    "        # update the row_idx and rest_of_rows\n",
    "        row_idx = tl.index_update(row_idx, i, rest_of_rows[max_row_idx])\n",
    "        rest_of_rows = rest_of_rows[tl.tensor(mask, dtype=tl.int64)]\n",
    "        i = i + 1\n",
    "\n",
    "    row_idx = tl.tensor(row_idx, dtype=tl.int64)\n",
    "    inverse = tl.solve(\n",
    "        A[row_idx, :], tl.eye(tl.shape(A[row_idx, :])[0], **tl.context(A))\n",
    "    )\n",
    "    row_idx = tl.to_numpy(row_idx)\n",
    "\n",
    "    return row_idx, inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200bcb48-8b49-4998-a10c-1acbdde45fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spingnn",
   "language": "python",
   "name": "spingnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
