{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4210a62",
   "metadata": {},
   "source": [
    "### Maxvol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dc6bf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladimir/anaconda3/envs/spingnn/lib/python3.10/site-packages/torchtt/_dmrg.py:19: UserWarning: \u001b[33m\n",
      "C++ implementation not available. Using pure Python.\n",
      "\u001b[0m\n",
      "  warnings.warn(\"\\x1B[33m\\nC++ implementation not available. Using pure Python.\\n\\033[0m\")\n",
      "/home/vladimir/anaconda3/envs/spingnn/lib/python3.10/site-packages/torchtt/_amen.py:21: UserWarning: \u001b[33m\n",
      "C++ implementation not available. Using pure Python.\n",
      "\u001b[0m\n",
      "  warnings.warn(\n",
      "/home/vladimir/anaconda3/envs/spingnn/lib/python3.10/site-packages/torchtt/solvers.py:21: UserWarning: \u001b[33m\n",
      "C++ implementation not available. Using pure Python.\n",
      "\u001b[0m\n",
      "  warnings.warn(\n",
      "/home/vladimir/anaconda3/envs/spingnn/lib/python3.10/site-packages/torchtt/cpp.py:12: UserWarning: \u001b[33m\n",
      "C++ implementation not available. Using pure Python.\n",
      "\u001b[0m\n",
      "  warnings.warn(\"\\x1B[33m\\nC++ implementation not available. Using pure Python.\\n\\033[0m\")\n",
      "/home/vladimir/anaconda3/envs/spingnn/lib/python3.10/site-packages/torchtt/__init__.py:34: UserWarning: \u001b[33m\n",
      "C++ implementation not available. Using pure Python.\n",
      "\u001b[0m\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implements the cross approximation methods (DMRG).\n",
    "\n",
    "\"\"\"\n",
    "import torch as tn\n",
    "import numpy as np\n",
    "import torchtt\n",
    "import datetime\n",
    "from torchtt._decomposition import QR, SVD, rank_chop, lr_orthogonal, rl_orthogonal\n",
    "from torchtt._iterative_solvers import BiCGSTAB_reset, gmres_restart\n",
    "import opt_einsum as oe\n",
    "import sys\n",
    "\n",
    "\n",
    "def _LU(M):\n",
    "    \"\"\"\n",
    "    Perform an LU decomposition and returns L, U and a permutation vector P. \n",
    "\n",
    "    Args:\n",
    "        M (torch.tensor): [description]\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.tensor,torch.tensor,torch.tensor]: L, U, P\n",
    "    \"\"\"\n",
    "    LU, P = tn.linalg.lu_factor(M)\n",
    "    P, L, U = tn.lu_unpack(LU, P)  # P transpose or not transpose?\n",
    "    P = P@tn.reshape(tn.arange(P.shape[1],\n",
    "                     dtype=P.dtype, device=P.device), [-1, 1])\n",
    "    # P = tn.reshape(tn.arange(P.shape[1],dtype=P.dtype,device=P.device),[1,-1]) @ P\n",
    "\n",
    "    return L, U, tn.squeeze(P).to(tn.int64)\n",
    "\n",
    "\n",
    "def _max_matrix(M):\n",
    "\n",
    "    values, indices = M.flatten().topk(1)\n",
    "    try:\n",
    "        indices = [tn.unravel_index(i, M.shape) for i in indices]\n",
    "    except:\n",
    "        indices = [np.unravel_index(i, M.shape) for i in indices]\n",
    "\n",
    "    return values, indices\n",
    "\n",
    "\n",
    "def _maxvol(M):\n",
    "    \"\"\"\n",
    "    Maxvol\n",
    "\n",
    "    Args:\n",
    "        M (torch.tensor): input matrix.\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: indices of tha maxvol submatrix.\n",
    "    \"\"\"\n",
    "\n",
    "    if M.shape[1] >= M.shape[0]:\n",
    "        # more cols than row -> return all the row indices\n",
    "        idx = tn.tensor(range(M.shape[0]), dtype=tn.int64)\n",
    "        return idx\n",
    "    else:\n",
    "        L, U, P = _LU(M)\n",
    "        # Good initial approximation with LU decomp\n",
    "        idx = P[:M.shape[1]]\n",
    "\n",
    "    # First ~A\n",
    "    Msub = M[idx, :]\n",
    "    \n",
    "    # Get A ~A^{-1} = (I B)\n",
    "    Mat = tn.linalg.solve(Msub.T, M.T).t()\n",
    "\n",
    "    for i in range(100):\n",
    "        val_max, idx_max = _max_matrix(tn.abs(Mat))\n",
    "        # Get B[i j] = max ~A\n",
    "        idx_max = idx_max[0]\n",
    "        if val_max <= 1+5e-2:\n",
    "            idx = tn.sort(idx)[0]\n",
    "            return idx\n",
    "        # Schur complement I guess to cheap change the matrix A ~A^{-1} \n",
    "        # after i row of matrix ~A is swapped with j row of matrix A\n",
    "        Mat += tn.outer(Mat[:, idx_max[1]], Mat[idx[idx_max[1]]] -\n",
    "                        Mat[idx_max[0], :])/Mat[idx_max[0], idx_max[1]]\n",
    "        \n",
    "        # Record the index swap\n",
    "        idx[idx_max[1]] = idx_max[0]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa22157b",
   "metadata": {},
   "source": [
    "### Function interpolate\n",
    "\n",
    "* Univariate interpoaltion:\n",
    "\n",
    "    Let $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ be a function and $\\mathsf{x}\\in\\mathbb{R}^{N_1\\times\\cdots\\times N_d}$ be a tensor with a known TT approximation.\n",
    "    \n",
    "    The goal is to determine the TT approximation of $\\mathsf{y}_{i_1...i_d}$ = $f(\\mathsf{x}_{i_1...i_d})$ within a prescribed relative accuracy `eps`. \n",
    "\n",
    "* Multivariate interpolation\n",
    "\n",
    "    Let $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ be a function and $\\mathsf{x}^{(1)},...,\\mathsf{x}^{(d)}\\in\\mathbb{R}^{N_1\\times\\cdots\\times N_d}$ be tensors with a known TT approximation. \n",
    "    \n",
    "    The goal is to determine the TT approximation of $\\mathsf{y}_{i_1...i_d}=f(\\mathsf{x}_{i_1...i_d}^{(1)},...,\\mathsf{x}^{(d)})_{i_1...i_d}$ within a prescribed relative accuracy `eps`.\n",
    "    \n",
    "    \n",
    "Example:\n",
    "\n",
    "        * Univariate interpolation:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            func = lambda t: torch.log(t)\n",
    "            y = tntt.interpolate.function_interpolate(func, x, 1e-9) # the tensor x is chosen such that y has an afforbable low rank structure\n",
    "\n",
    "        * Multivariate interpolation:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            xs = tntt.meshgrid([tn.arange(0,n,dtype=torch.float64) for n in N])\n",
    "            func = lambda x: 1/(2+tn.sum(x,1).to(dtype=torch.float64))\n",
    "            z = tntt.interpolate.function_interpolate(func, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84518ea3-6340-494b-af42-dfdec04d896d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 10, 10]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = [10, 10, 10]\n",
    "\n",
    "xs = torchtt.meshgrid([tn.arange(0,n,dtype=tn.float64) for n in N])\n",
    "\n",
    "xs[0].N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55233ff2-1380-420e-b900-c8fa0e799b93",
   "metadata": {},
   "source": [
    "If a list is passed as x, the function handle takes as argument a \n",
    "$M\\times d$ torch.tensor and every of the M lines \n",
    "corresponds to an evaluation of the function f at a certain tensor entry. \n",
    "The function handle returns a torch tensor of length M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3db30ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_interpolate(function, x, eps=1e-9, start_tens=None, nswp=20, kick=2, dtype=tn.float64, rmax=sys.maxsize, verbose=False):\n",
    "    \"\"\"\n",
    "    Appication of a nonlinear function on a tensor in the TT format (using DMRG). Two cases are distinguished:\n",
    "\n",
    "    * Univariate interpoaltion:\n",
    "\n",
    "    Let :math:`f:\\\\mathbb{R}\\\\rightarrow\\\\mathbb{R}` be a function and :math:`\\\\mathsf{x}\\\\in\\\\mathbb{R}^{N_1\\\\times\\\\cdots\\\\times N_d}` be a tensor with a known TT approximation.\n",
    "    The goal is to determine the TT approximation of :math:`\\\\mathsf{y}_{i_1...i_d}=f(\\\\mathsf{x}_{i_1...i_d})` within a prescribed relative accuracy `eps`. \n",
    "\n",
    "    * Multivariate interpolation\n",
    "\n",
    "    Let :math:`f:\\\\mathbb{R}\\\\rightarrow\\\\mathbb{R}` be a function and :math:`\\\\mathsf{x}^{(1)},...,\\\\mathsf{x}^{(d)}\\\\in\\\\mathbb{R}^{N_1\\\\times\\\\cdots\\\\times N_d}` be tensors with a known TT approximation. The goal is to determine the TT approximation of :math:`\\\\mathsf{y}_{i_1...i_d}=f(\\\\mathsf{x}_{i_1...i_d}^{(1)},...,\\\\mathsf{x}^{(d)})_{i_1...i_d}` within a prescribed relative accuracy `eps`.\n",
    "\n",
    "\n",
    "    Example:\n",
    "\n",
    "        * Univariate interpolation:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            func = lambda t: torch.log(t)\n",
    "            y = tntt.interpolate.function_interpolate(func, x, 1e-9) # the tensor x is chosen such that y has an afforbable low rank structure\n",
    "\n",
    "        * Multivariate interpolation:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            xs = tntt.meshgrid([tn.arange(0,n,dtype=torch.float64) for n in N])\n",
    "            func = lambda x: 1/(2+tn.sum(x,1).to(dtype=torch.float64))\n",
    "            z = tntt.interpolate.function_interpolate(func, xs)\n",
    "\n",
    "\n",
    "    Args:\n",
    "        function (Callable): function handle. If the argument `x` is a `torchtt.TT` instance, \n",
    "            the the function handle has to be appliable elementwise on torch tensors.\n",
    "            If a list is passed as `x`, the function handle takes as argument a \n",
    "            :math:`M\\times d` torch.tensor and every of the :math:`M` lines \n",
    "            corresponds to an evaluation of the function :math:`f` at a certain tensor entry. \n",
    "            The function handle returns a torch tensor of length M.\n",
    "        x (torchtt.TT or list[torchtt.TT]): the argument/arguments of the function.\n",
    "        eps (float, optional): the relative accuracy. Defaults to 1e-9.\n",
    "            start_tens (torchtt.TT, optional): initial approximation of the output tensor \n",
    "            (None coresponds to random initialization). Defaults to None.\n",
    "        nswp (int, optional): number of iterations. Defaults to 20.\n",
    "        kick (int, optional): enrichment rank. Defaults to 2.\n",
    "        dtype (torch.dtype, optional): the dtype of the result. Defaults to tn.float64.\n",
    "        rmax (int, optional): the maximum rank. Defaults to the maximum possible integer.\n",
    "        verbose (bool, optional): display debug information to the console. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        torchtt.TT: the result.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(x, list) or isinstance(x, tuple):\n",
    "        # evaluate multivariable\n",
    "        eval_mv = True\n",
    "        N = x[0].N\n",
    "    else:\n",
    "        eval_mv = False\n",
    "        N = x.N\n",
    "    device = None\n",
    "\n",
    "    if not eval_mv and len(N) == 1:\n",
    "        return torchtt.TT(function(x.full())).to(device)\n",
    "\n",
    "    if eval_mv and len(N) == 1:\n",
    "        return torchtt.TT(function(x[0].full())).to(device)\n",
    "\n",
    "    d = len(N)\n",
    "\n",
    "    # random init of the tensor\n",
    "    if start_tens == None:\n",
    "        rank_init = 2\n",
    "        cores = torchtt.random(N, rank_init, dtype, device).cores\n",
    "        rank = [1]+[rank_init]*(d-1)+[1]\n",
    "    else:\n",
    "        rank = start_tens.R.copy()\n",
    "        cores = [c+0 for c in start_tens.cores]\n",
    "    # cores = (ones(N,dtype=dtype)).cores\n",
    "\n",
    "    cores, rank = rl_orthogonal(cores, rank, False)\n",
    "    cores, rank = lr_orthogonal(cores, rank, False)\n",
    "    Mats = []*(d+1)\n",
    "\n",
    "    Ps = [tn.ones((1, 1), dtype=dtype, device=device)]+(d-1) * \\\n",
    "        [None] + [tn.ones((1, 1), dtype=dtype, device=device)]\n",
    "    # ortho\n",
    "    Rm = tn.ones((1, 1), dtype=dtype, device=device)\n",
    "    Idx = [tn.zeros((1, 0), dtype=tn.int64)]+(d-1)*[None] + \\\n",
    "        [tn.zeros((0, 1), dtype=tn.int64)]\n",
    "    for k in range(d-1, 0, -1):\n",
    "\n",
    "        tmp = tn.einsum('ijk,kl->ijl', cores[k], Rm)\n",
    "        tmp = tn.reshape(tmp, [rank[k], -1]).t()\n",
    "        core, Rmat = QR(tmp)\n",
    "\n",
    "        rnew = min(N[k]*rank[k+1], rank[k])\n",
    "        Jk = _maxvol(core)\n",
    "        # print(Jk)\n",
    "        try:\n",
    "            tmp = tn.unravel_index(Jk[:rnew], (rank[k+1], N[k]))\n",
    "        except:\n",
    "            tmp = np.unravel_index(Jk[:rnew], (rank[k+1], N[k]))\n",
    "        # if k==d-1:\n",
    "        #    idx_new = tn.tensor(tmp[1].reshape([1,-1]))\n",
    "        # else:\n",
    "        idx_new = tn.tensor(\n",
    "            np.vstack((tmp[1].reshape([1, -1]), Idx[k+1][:, tmp[0]])))\n",
    "\n",
    "        Idx[k] = idx_new+0\n",
    "\n",
    "        Rm = core[Jk, :]\n",
    "\n",
    "        core = tn.linalg.solve(Rm.T, core.T)\n",
    "        Rm = (Rm@Rmat).t()\n",
    "        cores[k] = tn.reshape(core, [rnew, N[k], rank[k+1]])\n",
    "\n",
    "        core = tn.reshape(core, [-1, rank[k+1]]) @ Ps[k+1]\n",
    "\n",
    "        core = tn.reshape(core, [rank[k], -1]).t()\n",
    "        _, Ps[k] = QR(core)\n",
    "    cores[0] = tn.einsum('ijk,kl->ijl', cores[0], Rm)\n",
    "\n",
    "    # for p in Ps:\n",
    "    #     print(p)\n",
    "    # for i in Idx:\n",
    "    #     print(i)\n",
    "    # return\n",
    "    n_eval = 0\n",
    "\n",
    "    for swp in range(nswp):\n",
    "\n",
    "        max_err = 0.0\n",
    "        if verbose:\n",
    "            print('Sweep %d: ' % (swp+1))\n",
    "        # left to right\n",
    "        for k in range(d-1):\n",
    "            if verbose:\n",
    "                print('\\tLR supercore %d,%d' % (k+1, k+2))\n",
    "            I1 = tn.reshape(tn.kron(tn.kron(tn.ones(rank[k], dtype=tn.int64), tn.arange(N[k], dtype=tn.int64)), tn.kron(\n",
    "                tn.ones(N[k+1], dtype=tn.int64), tn.ones(rank[k+2], dtype=tn.int64))), [-1, 1])\n",
    "            I2 = tn.reshape(tn.kron(tn.kron(tn.ones(rank[k], dtype=tn.int64), tn.ones(N[k], dtype=tn.int64)), tn.kron(\n",
    "                tn.arange(N[k+1], dtype=tn.int64), tn.ones(rank[k+2], dtype=tn.int64))), [-1, 1])\n",
    "            I3 = Idx[k][tn.kron(tn.kron(tn.arange(rank[k], dtype=tn.int64), tn.ones(N[k], dtype=tn.int64)), tn.kron(\n",
    "                tn.ones(N[k+1], dtype=tn.int64), tn.ones(rank[k+2], dtype=tn.int64))), :]\n",
    "            I4 = Idx[k+2][:, tn.kron(tn.kron(tn.ones(rank[k], dtype=tn.int64), tn.ones(N[k], dtype=tn.int64)),\n",
    "                                     tn.kron(tn.ones(N[k+1], dtype=tn.int64), tn.arange(rank[k+2], dtype=tn.int64)))].t()\n",
    "\n",
    "            eval_index = tn.concat((I3, I1, I2, I4), 1)\n",
    "            eval_index = tn.reshape(eval_index, [-1, d]).to(dtype=tn.int64)\n",
    "\n",
    "            if verbose:\n",
    "                print('\\t\\tnumber evaluations', eval_index.shape[0])\n",
    "\n",
    "            if eval_mv:\n",
    "                ev = tn.zeros((eval_index.shape[0], 0), dtype=dtype)\n",
    "                for j in range(d):\n",
    "                    core = x[j].cores[0][0, eval_index[:, 0], :]\n",
    "                    for i in range(1, d):\n",
    "                        core = tn.einsum('ij,jil->il', core,\n",
    "                                         x[j].cores[i][:, eval_index[:, i], :])\n",
    "                    core = tn.reshape(core[..., 0], [-1, 1])\n",
    "                    ev = tn.hstack((ev, core))\n",
    "                supercore = tn.reshape(\n",
    "                    function(ev), [rank[k], N[k], N[k+1], rank[k+2]])\n",
    "                n_eval += core.shape[0]\n",
    "            else:\n",
    "                core = x.cores[0][0, eval_index[:, 0], :]\n",
    "                for i in range(1, d):\n",
    "                    core = tn.einsum('ij,jil->il', core,\n",
    "                                     x.cores[i][:, eval_index[:, i], :])\n",
    "                core = core[..., 0]\n",
    "                supercore = tn.reshape(\n",
    "                    function(core), [rank[k], N[k], N[k+1], rank[k+2]])\n",
    "                n_eval += core.shape[0]\n",
    "\n",
    "            # multiply with P_k left and right\n",
    "            supercore = tn.einsum('ij,jklm,mn->ikln',\n",
    "                                  Ps[k], supercore.to(dtype=dtype), Ps[k+2])\n",
    "            rank[k] = supercore.shape[0]\n",
    "            rank[k+2] = supercore.shape[3]\n",
    "            supercore = tn.reshape(\n",
    "                supercore, [supercore.shape[0]*supercore.shape[1], -1])\n",
    "\n",
    "            # split the super core with svd\n",
    "            U, S, V = SVD(supercore)\n",
    "            rnew = rank_chop(S.cpu().numpy(), tn.linalg.norm(\n",
    "                S).cpu().numpy()*eps/np.sqrt(d-1))+1\n",
    "            rnew = min(S.shape[0], rnew)\n",
    "            rnew = min(rmax, rnew)\n",
    "            U = U[:, :rnew]\n",
    "            S = S[:rnew]\n",
    "            V = V[:rnew, :]\n",
    "            # print('kkt new',tn.linalg.norm(supercore-U@tn.diag(S)@V))\n",
    "            # kick the rank\n",
    "            V = tn.diag(S) @ V\n",
    "            UK = tn.randn((U.shape[0], kick), dtype=dtype, device=device)\n",
    "            U, Rtemp = QR(tn.cat((U, UK), 1))\n",
    "            radd = Rtemp.shape[1] - rnew\n",
    "            if radd > 0:\n",
    "                V = tn.cat(\n",
    "                    (V, tn.zeros((radd, V.shape[1]), dtype=dtype, device=device)), 0)\n",
    "                V = Rtemp @ V\n",
    "\n",
    "            # print('kkt new',tn.linalg.norm(supercore-U@V))\n",
    "            # compute err (dx)\n",
    "            super_prev = tn.einsum('ijk,kmn->ijmn', cores[k], cores[k+1])\n",
    "            super_prev = tn.einsum(\n",
    "                'ij,jklm,mn->ikln', Ps[k], super_prev, Ps[k+2])\n",
    "            err = tn.linalg.norm(\n",
    "                supercore.flatten()-super_prev.flatten())/tn.linalg.norm(supercore)\n",
    "            max_err = max(max_err, err)\n",
    "            # update the rank\n",
    "            if verbose:\n",
    "                print('\\t\\trank updated %d -> %d, local error %e' %\n",
    "                      (rank[k+1], U.shape[1], err))\n",
    "            rank[k+1] = U.shape[1]\n",
    "\n",
    "            U = tn.linalg.solve(Ps[k], tn.reshape(U, [rank[k], -1]))\n",
    "            V = tn.linalg.solve(\n",
    "                Ps[k+2].t(), tn.reshape(V, [rank[k+1]*N[k+1], rank[k+2]]).t()).t()\n",
    "\n",
    "            # U = tn.einsum('ij,jkl->ikl',tn.linalg.inv(Ps[k]),tn.reshape(U,[rank[k],N[k],-1]))\n",
    "            # V = tn.einsum('ijk,kl->ijl',tn.reshape(V,[-1,N[k+1],rank[k+2]]),tn.linalg.inv(Ps[k+2]))\n",
    "\n",
    "            V = tn.reshape(V, [rank[k+1], -1])\n",
    "            U = tn.reshape(U, [-1, rank[k+1]])\n",
    "\n",
    "            # split cores\n",
    "            Qmat, Rmat = QR(U)\n",
    "            idx = _maxvol(Qmat)\n",
    "            Sub = Qmat[idx, :]\n",
    "            core = tn.linalg.solve(Sub.T, Qmat.T).t()\n",
    "            core_next = Sub@Rmat@V\n",
    "            cores[k] = tn.reshape(core, [rank[k], N[k], rank[k+1]])\n",
    "            cores[k+1] = tn.reshape(core_next, [rank[k+1], N[k+1], rank[k+2]])\n",
    "            # calc Ps\n",
    "            tmp = tn.einsum('ij,jkl->ikl', Ps[k], cores[k])\n",
    "            _, Ps[k+1] = QR(tn.reshape(tmp, [rank[k]*N[k], rank[k+1]]))\n",
    "\n",
    "            # calc Idx\n",
    "            try:\n",
    "                tmp = tn.unravel_index(idx[:rank[k+1]], (rank[k], N[k]))\n",
    "            except:\n",
    "                tmp = np.unravel_index(idx[:rank[k+1]], (rank[k], N[k]))\n",
    "            idx_new = tn.tensor(\n",
    "                np.hstack((Idx[k][tmp[0], :], tmp[1].reshape([-1, 1]))))\n",
    "            Idx[k+1] = idx_new+0\n",
    "\n",
    "        # right to left\n",
    "\n",
    "        for k in range(d-2, -1, -1):\n",
    "            if verbose:\n",
    "                print('\\tRL supercore %d,%d' % (k+1, k+2))\n",
    "            I1 = tn.reshape(tn.kron(tn.kron(tn.ones(rank[k], dtype=tn.int64), tn.arange(N[k], dtype=tn.int64)), tn.kron(\n",
    "                tn.ones(N[k+1], dtype=tn.int64), tn.ones(rank[k+2], dtype=tn.int64))), [-1, 1])\n",
    "            I2 = tn.reshape(tn.kron(tn.kron(tn.ones(rank[k], dtype=tn.int64), tn.ones(N[k], dtype=tn.int64)), tn.kron(\n",
    "                tn.arange(N[k+1], dtype=tn.int64), tn.ones(rank[k+2], dtype=tn.int64))), [-1, 1])\n",
    "            I3 = Idx[k][tn.kron(tn.kron(tn.arange(rank[k], dtype=tn.int64), tn.ones(N[k], dtype=tn.int64)), tn.kron(\n",
    "                tn.ones(N[k+1], dtype=tn.int64), tn.ones(rank[k+2], dtype=tn.int64))), :]\n",
    "            I4 = Idx[k+2][:, tn.kron(tn.kron(tn.ones(rank[k], dtype=tn.int64), tn.ones(N[k], dtype=tn.int64)),\n",
    "                                     tn.kron(tn.ones(N[k+1], dtype=tn.int64), tn.arange(rank[k+2], dtype=tn.int64)))].t()\n",
    "\n",
    "            eval_index = tn.concat((I3, I1, I2, I4), 1)\n",
    "            eval_index = tn.reshape(eval_index, [-1, d]).to(dtype=tn.int64)\n",
    "\n",
    "            if verbose:\n",
    "                print('\\t\\tnumber evaluations', eval_index.shape[0])\n",
    "\n",
    "            if eval_mv:\n",
    "                ev = tn.zeros((eval_index.shape[0], 0), dtype=dtype)\n",
    "                for j in range(d):\n",
    "                    core = x[j].cores[0][0, eval_index[:, 0], :]\n",
    "                    for i in range(1, d):\n",
    "                        core = tn.einsum('ij,jil->il', core,\n",
    "                                         x[j].cores[i][:, eval_index[:, i], :])\n",
    "                    core = tn.reshape(core[..., 0], [-1, 1])\n",
    "                    ev = tn.hstack((ev, core))\n",
    "                supercore = tn.reshape(\n",
    "                    function(ev), [rank[k], N[k], N[k+1], rank[k+2]])\n",
    "                n_eval += core.shape[0]\n",
    "            else:\n",
    "                core = x.cores[0][0, eval_index[:, 0], :]\n",
    "                for i in range(1, d):\n",
    "                    core = tn.einsum('ij,jil->il', core,\n",
    "                                     x.cores[i][:, eval_index[:, i], :])\n",
    "                core = core[..., 0]\n",
    "                supercore = tn.reshape(\n",
    "                    function(core), [rank[k], N[k], N[k+1], rank[k+2]])\n",
    "                n_eval += core.shape[0]\n",
    "\n",
    "            # multiply with P_k left and right\n",
    "            supercore = tn.einsum('ij,jklm,mn->ikln',\n",
    "                                  Ps[k], supercore.to(dtype=dtype), Ps[k+2])\n",
    "            rank[k] = supercore.shape[0]\n",
    "            rank[k+2] = supercore.shape[3]\n",
    "            supercore = tn.reshape(\n",
    "                supercore, [supercore.shape[0]*supercore.shape[1], -1])\n",
    "\n",
    "            # split the super core with svd\n",
    "            U, S, V = SVD(supercore)\n",
    "            rnew = rank_chop(S.cpu().numpy(), tn.linalg.norm(\n",
    "                S).cpu().numpy()*eps/np.sqrt(d-1))+1\n",
    "            rnew = min(S.shape[0], rnew)\n",
    "            rnew = min(rmax, rnew)\n",
    "            U = U[:, :rnew]\n",
    "            S = S[:rnew]\n",
    "            V = V[:rnew, :]\n",
    "            # print('kkt new',tn.linalg.norm(supercore-U@tn.diag(S)@V))\n",
    "\n",
    "            # kick the rank\n",
    "            # print('u before', U.shape)\n",
    "            U = U @ tn.diag(S)\n",
    "            VK = tn.randn((kick, V.shape[1]), dtype=dtype, device=device)\n",
    "            # print('V enrich', V.shape)\n",
    "            V, Rtemp = QR(tn.cat((V, VK), 0).t())\n",
    "            radd = Rtemp.shape[1] - rnew\n",
    "            # print('V after QR',V.shape,Rtemp.shape,radd)\n",
    "            if radd > 0:\n",
    "                U = tn.cat(\n",
    "                    (U, tn.zeros((U.shape[0], radd), dtype=dtype, device=device)), 1)\n",
    "                U = U @ Rtemp.T\n",
    "                V = V.t()\n",
    "\n",
    "            # print('kkt new',tn.linalg.norm(supercore-U@V))\n",
    "            # compute err (dx)\n",
    "            super_prev = tn.einsum('ijk,kmn->ijmn', cores[k], cores[k+1])\n",
    "            super_prev = tn.einsum(\n",
    "                'ij,jklm,mn->ikln', Ps[k], super_prev, Ps[k+2])\n",
    "            err = tn.linalg.norm(\n",
    "                supercore.flatten()-super_prev.flatten())/tn.linalg.norm(supercore)\n",
    "            max_err = max(max_err, err)\n",
    "            # update the rank\n",
    "            if verbose:\n",
    "                print('\\t\\trank updated %d -> %d, local error %e' %\n",
    "                      (rank[k+1], U.shape[1], err))\n",
    "            rank[k+1] = U.shape[1]\n",
    "\n",
    "            U = tn.linalg.solve(Ps[k], tn.reshape(U, [rank[k], -1]))\n",
    "            V = tn.linalg.solve(\n",
    "                Ps[k+2].t(), tn.reshape(V, [rank[k+1]*N[k+1], rank[k+2]]).t()).t()\n",
    "\n",
    "            # U = tn.einsum('ij,jkl->ikl',tn.linalg.inv(Ps[k]),tn.reshape(U,[rank[k],N[k],-1]))\n",
    "            # V = tn.einsum('ijk,kl->ijl',tn.reshape(V,[-1,N[k+1],rank[k+2]]),tn.linalg.inv(Ps[k+2]))\n",
    "\n",
    "            V = tn.reshape(V, [rank[k+1], -1])\n",
    "            U = tn.reshape(U, [-1, rank[k+1]])\n",
    "\n",
    "            # split cores\n",
    "            Qmat, Rmat = QR(V.T)\n",
    "            idx = _maxvol(Qmat)\n",
    "            Sub = Qmat[idx, :]\n",
    "            core_next = tn.linalg.solve(Sub.T, Qmat.T)\n",
    "            core = U@(Sub@Rmat).t()\n",
    "            cores[k] = tn.reshape(core, [rank[k], N[k], -1])\n",
    "            cores[k+1] = tn.reshape(core_next, [-1, N[k+1], rank[k+2]])\n",
    "\n",
    "            # calc Ps\n",
    "            tmp = tn.einsum('ijk,kl->ijl', cores[k+1], Ps[k+2])\n",
    "            _, tmp = QR(tn.reshape(tmp, [rank[k+1], -1]).t())\n",
    "            Ps[k+1] = tmp\n",
    "            # calc Idx\n",
    "            try:\n",
    "                tmp = tn.unravel_index(idx[:rank[k+1]], (N[k+1], rank[k+2]))\n",
    "            except:\n",
    "                tmp = np.unravel_index(idx[:rank[k+1]], (N[k+1], rank[k+2]))\n",
    "            idx_new = tn.tensor(\n",
    "                np.vstack((tmp[0].reshape([1, -1]), Idx[k+2][:, tmp[1]])))\n",
    "            Idx[k+1] = idx_new+0\n",
    "        # xxx = TT(cores)\n",
    "        # print('#            ',xxx[1,2,3,4])\n",
    "\n",
    "        # exit condition\n",
    "\n",
    "        if max_err < eps:\n",
    "            if verbose:\n",
    "                print('Max error %e < %e  ---->  DONE' % (max_err, eps))\n",
    "            break\n",
    "        else:\n",
    "            if verbose:\n",
    "                print('Max error %g' % (max_err))\n",
    "    if verbose:\n",
    "        print('number of function calls ', n_eval)\n",
    "        print()\n",
    "\n",
    "    return torchtt.TT(cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16fe770",
   "metadata": {},
   "source": [
    "### Dmrg cross\n",
    "\n",
    "I think in this case you can not work with indices anymore and have to do function calls.\n",
    "But probably function is on indixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb121ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmrg_cross(function, N, eps=1e-9, nswp=10, x_start=None, kick=2, dtype=tn.float64, device=None, eval_vect=True, rmax=sys.maxsize, verbose=False):\n",
    "    \"\"\"\n",
    "    Approximate a tensor in the TT format given that the individual entries are given using a function.\n",
    "    The function is given as a function handle taking as arguments a matrix of integer indices.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            func = lambda I: 1/(2+I[:,0]+I[:,1]+I[:,2]+I[:,3]).to(dtype=torch.float64)\n",
    "            N = [20]*4\n",
    "            x = torchtt.interpolate.dmrg_cross(func, N, eps = 1e-7)\n",
    "\n",
    "\n",
    "    Args:\n",
    "        function (Callable): function handle.\n",
    "        N (list[int]): the shape of the tensor.\n",
    "        eps (float, optional): the relative accuracy. Defaults to 1e-9.\n",
    "        nswp (int, optional): number of iterations. Defaults to 20.\n",
    "        x_start (torchtt.TT, optional): initial approximation of the output tensor \n",
    "        (None coresponds to random initialization). Defaults to None.\n",
    "        kick (int, optional): enrichment rank. Defaults to 2.\n",
    "        dtype (torch.dtype, optional): the dtype of the result. Defaults to tn.float64.\n",
    "        device (torch.device, optional): the device where the approximation will be stored. Defaults to None.\n",
    "        eval_vect (bool, optional): not yet implemented. Defaults to True.\n",
    "        rmax (int, optional): the maximum rank. Defaults to the maximum possible integer.\n",
    "        verbose (bool, optional): display debug information to the console. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        torchtt.TT: the result.\n",
    "\n",
    "    \"\"\"\n",
    "    # store the computed values\n",
    "    computed_vals = dict()\n",
    "\n",
    "    d = len(N)\n",
    "\n",
    "    # random init of the tensor\n",
    "    if x_start == None:\n",
    "        rank_init = 2\n",
    "        cores = torchtt.random(N, rank_init, dtype, device).cores\n",
    "        rank = [1]+[rank_init]*(d-1)+[1]\n",
    "    else:\n",
    "        rank = x_start.R.copy()\n",
    "        cores = [c+0 for c in x_start.cores]\n",
    "    # cores = (ones(N,dtype=dtype)).cores\n",
    "\n",
    "    cores, rank = lr_orthogonal(cores, rank, False)\n",
    "\n",
    "    Mats = []*(d+1)\n",
    "\n",
    "    Ps = [tn.ones((1, 1), dtype=dtype, device=device)]+(d-1) * \\\n",
    "        [None] + [tn.ones((1, 1), dtype=dtype, device=device)]\n",
    "    # ortho construct Ps (equivalent to Vk) from original paper to get Bk = A Vk^-1 Cheaply\n",
    "    Rm = tn.ones((1, 1), dtype=dtype, device=device)\n",
    "    Idx = [tn.zeros((1, 0), dtype=tn.int64)]+(d-1)*[None] + \\\n",
    "        [tn.zeros((0, 1), dtype=tn.int64)]\n",
    "    for k in range(d-1, 0, -1):\n",
    "\n",
    "        tmp = tn.einsum('ijk,kl->ijl', cores[k], Rm)\n",
    "        tmp = tn.reshape(tmp, [rank[k], -1]).t()\n",
    "        core, Rmat = QR(tmp)\n",
    "\n",
    "        rnew = min(N[k]*rank[k+1], rank[k])\n",
    "        Jk = _maxvol(core)\n",
    "        # print(Jk)\n",
    "        try:\n",
    "            tmp = tn.unravel_index(Jk[:rnew], (rank[k+1], N[k]))\n",
    "        except:\n",
    "            tmp = np.unravel_index(Jk[:rnew], (rank[k+1], N[k]))\n",
    "        # if k==d-1:\n",
    "        #    idx_new = tn.tensor(tmp[1].reshape([1,-1]))\n",
    "        # else:\n",
    "        idx_new = tn.tensor(\n",
    "            np.vstack((tmp[1].reshape([1, -1]), Idx[k+1][:, tmp[0]])))\n",
    "\n",
    "        Idx[k] = idx_new+0\n",
    "\n",
    "        Rm = core[Jk, :]\n",
    "\n",
    "        core = tn.linalg.solve(Rm.T, core.T)\n",
    "        # core = tn.linalg.solve(Rm,core.T)\n",
    "        Rm = (Rm@Rmat).t()\n",
    "        # core = core.t()\n",
    "        cores[k] = tn.reshape(core, [rnew, N[k], rank[k+1]])\n",
    "        core = tn.reshape(core, [-1, rank[k+1]]) @ Ps[k+1]\n",
    "        core = tn.reshape(core, [rank[k], -1]).t()\n",
    "        _, Ps[k] = QR(core)\n",
    "    cores[0] = tn.einsum('ijk,kl->ijl', cores[0], Rm)\n",
    "\n",
    "    # for p in Ps:\n",
    "    #     print(p)\n",
    "    # for i in Idx:\n",
    "    #     print(i)\n",
    "    # return\n",
    "    n_eval = 0\n",
    "\n",
    "    for swp in range(nswp):\n",
    "\n",
    "        max_err = 0.0\n",
    "        if verbose:\n",
    "            print('Sweep %d: ' % (swp+1))\n",
    "        # left to right\n",
    "        for k in range(d-1):\n",
    "            if verbose:\n",
    "                print('\\tLR supercore %d,%d' % (k+1, k+2))\n",
    "            I1 = tn.reshape(tn.kron(tn.kron(tn.ones(rank[k], dtype=tn.int64), tn.arange(N[k], dtype=tn.int64)), tn.kron(\n",
    "                tn.ones(N[k+1], dtype=tn.int64), tn.ones(rank[k+2], dtype=tn.int64))), [-1, 1])\n",
    "            I2 = tn.reshape(tn.kron(tn.kron(tn.ones(rank[k], dtype=tn.int64), tn.ones(N[k], dtype=tn.int64)), tn.kron(\n",
    "                tn.arange(N[k+1], dtype=tn.int64), tn.ones(rank[k+2], dtype=tn.int64))), [-1, 1])\n",
    "            I3 = Idx[k][tn.kron(tn.kron(tn.arange(rank[k], dtype=tn.int64), tn.ones(N[k], dtype=tn.int64)), tn.kron(\n",
    "                tn.ones(N[k+1], dtype=tn.int64), tn.ones(rank[k+2], dtype=tn.int64))), :]\n",
    "            I4 = Idx[k+2][:, tn.kron(tn.kron(tn.ones(rank[k], dtype=tn.int64), tn.ones(N[k], dtype=tn.int64)),\n",
    "                                     tn.kron(tn.ones(N[k+1], dtype=tn.int64), tn.arange(rank[k+2], dtype=tn.int64)))].t()\n",
    "\n",
    "            eval_index = tn.concat((I3, I1, I2, I4), 1)\n",
    "\n",
    "            eval_index = tn.reshape(eval_index, [-1, d]).to(dtype=tn.int64)\n",
    "\n",
    "            if verbose:\n",
    "                print('\\t\\tnumber evaluations', eval_index.shape[0])\n",
    "\n",
    "            if eval_vect:\n",
    "                supercore = tn.reshape(function(eval_index), [\n",
    "                                       rank[k], N[k], N[k+1], rank[k+2]])\n",
    "                n_eval += eval_index.shape[0]\n",
    "            else:\n",
    "                supercore = tn.zeros(eval_index.shape[0], dtype=dtype, device=device)\n",
    "                for ind in range(eval_index.shape[0]):\n",
    "                    supercore[ind] = function(*eval_index[ind,:])\n",
    "                supercore = tn.reshape(supercore, [rank[k], N[k], N[k+1], rank[k+2]])\n",
    "                n_eval += eval_index.shape[0]\n",
    "\n",
    "            # multiply with P_k left and right\n",
    "            supercore = tn.einsum('ij,jklm,mn->ikln',\n",
    "                                  Ps[k], supercore.to(dtype=dtype), Ps[k+2])\n",
    "            rank[k] = supercore.shape[0]\n",
    "            rank[k+2] = supercore.shape[3]\n",
    "            supercore = tn.reshape(\n",
    "                supercore, [supercore.shape[0]*supercore.shape[1], -1])\n",
    "\n",
    "            # split the super core with svd\n",
    "            U, S, V = SVD(supercore)\n",
    "            rnew = rank_chop(S.cpu().numpy(), tn.linalg.norm(\n",
    "                S).cpu().numpy()*eps/np.sqrt(d-1))+1\n",
    "            rnew = min(S.shape[0], rnew)\n",
    "            rnew = min(rmax, rnew)\n",
    "            U = U[:, :rnew]\n",
    "            S = S[:rnew]\n",
    "            V = V[:rnew, :]\n",
    "            # print('kkt new',tn.linalg.norm(supercore-U@tn.diag(S)@V))\n",
    "            # kick the rank\n",
    "            V = tn.diag(S) @ V\n",
    "            UK = tn.randn((U.shape[0], kick), dtype=dtype, device=device)\n",
    "            U, Rtemp = QR(tn.cat((U, UK), 1))\n",
    "            radd = U.shape[1] - rnew\n",
    "            if radd > 0:\n",
    "                V = tn.cat(\n",
    "                    (V, tn.zeros((radd, V.shape[1]), dtype=dtype, device=device)), 0)\n",
    "                V = Rtemp @ V\n",
    "            # print('kkt new',tn.linalg.norm(supercore-U@V))\n",
    "            # compute err (dx)\n",
    "            super_prev = tn.einsum('ijk,kmn->ijmn', cores[k], cores[k+1])\n",
    "            super_prev = tn.einsum(\n",
    "                'ij,jklm,mn->ikln', Ps[k], super_prev, Ps[k+2])\n",
    "            err = tn.linalg.norm(\n",
    "                supercore.flatten()-super_prev.flatten())/tn.linalg.norm(supercore)\n",
    "            max_err = max(max_err, err)\n",
    "            # update the rank\n",
    "            if verbose:\n",
    "                print('\\t\\trank updated %d -> %d, local error %e' %\n",
    "                      (rank[k+1], U.shape[1], err))\n",
    "            rank[k+1] = U.shape[1]\n",
    "\n",
    "            U = tn.linalg.solve(Ps[k], tn.reshape(U, [rank[k], -1]))\n",
    "            V = tn.linalg.solve(\n",
    "                Ps[k+2].t(), tn.reshape(V, [rank[k+1]*N[k+1], rank[k+2]]).t()).t()\n",
    "\n",
    "            # U = tn.einsum('ij,jkl->ikl',tn.linalg.inv(Ps[k]),tn.reshape(U,[rank[k],N[k],-1]))\n",
    "            # V = tn.einsum('ijk,kl->ijl',tn.reshape(V,[-1,N[k+1],rank[k+2]]),tn.linalg.inv(Ps[k+2]))\n",
    "\n",
    "            V = tn.reshape(V, [rank[k+1], -1])\n",
    "            U = tn.reshape(U, [-1, rank[k+1]])\n",
    "\n",
    "            # split cores\n",
    "            Qmat, Rmat = QR(U)\n",
    "            idx = _maxvol(Qmat)\n",
    "            Sub = Qmat[idx, :]\n",
    "            core = tn.linalg.solve(Sub.T, Qmat.T).t()\n",
    "            core_next = Sub@Rmat@V\n",
    "            cores[k] = tn.reshape(core, [rank[k], N[k], rank[k+1]])\n",
    "            cores[k+1] = tn.reshape(core_next, [rank[k+1], N[k+1], rank[k+2]])\n",
    "            # calc Ps\n",
    "            tmp = tn.einsum('ij,jkl->ikl', Ps[k], cores[k])\n",
    "            _, Ps[k+1] = QR(tn.reshape(tmp, [rank[k]*N[k], rank[k+1]]))\n",
    "\n",
    "            # calc Idx\n",
    "            try:\n",
    "                tmp = tn.unravel_index(idx[:rank[k+1]], (rank[k], N[k]))\n",
    "            except:\n",
    "                tmp = np.unravel_index(idx[:rank[k+1]], (rank[k], N[k]))\n",
    "            idx_new = tn.tensor(\n",
    "                np.hstack((Idx[k][tmp[0], :], tmp[1].reshape([-1, 1]))))\n",
    "            Idx[k+1] = idx_new+0\n",
    "\n",
    "        # right to left\n",
    "\n",
    "        for k in range(d-2, -1, -1):\n",
    "            if verbose:\n",
    "                print('\\tRL supercore %d,%d' % (k+1, k+2))\n",
    "            I1 = tn.reshape(tn.kron(tn.kron(tn.ones(rank[k], dtype=tn.int64), tn.arange(N[k], dtype=tn.int64)), tn.kron(\n",
    "                tn.ones(N[k+1], dtype=tn.int64), tn.ones(rank[k+2], dtype=tn.int64))), [-1, 1])\n",
    "            I2 = tn.reshape(tn.kron(tn.kron(tn.ones(rank[k], dtype=tn.int64), tn.ones(N[k], dtype=tn.int64)), tn.kron(\n",
    "                tn.arange(N[k+1], dtype=tn.int64), tn.ones(rank[k+2], dtype=tn.int64))), [-1, 1])\n",
    "            I3 = Idx[k][tn.kron(tn.kron(tn.arange(rank[k], dtype=tn.int64), tn.ones(N[k], dtype=tn.int64)), tn.kron(\n",
    "                tn.ones(N[k+1], dtype=tn.int64), tn.ones(rank[k+2], dtype=tn.int64))), :]\n",
    "            I4 = Idx[k+2][:, tn.kron(tn.kron(tn.ones(rank[k], dtype=tn.int64), tn.ones(N[k], dtype=tn.int64)),\n",
    "                                     tn.kron(tn.ones(N[k+1], dtype=tn.int64), tn.arange(rank[k+2], dtype=tn.int64)))].t()\n",
    "\n",
    "            eval_index = tn.concat((I3, I1, I2, I4), 1)\n",
    "            eval_index = tn.reshape(eval_index, [-1, d]).to(dtype=tn.int64)\n",
    "\n",
    "            if verbose:\n",
    "                print('\\t\\tnumber evaluations', eval_index.shape[0])\n",
    "\n",
    "            if eval_vect:\n",
    "                supercore = tn.reshape(function(eval_index).to(dtype=dtype), [\n",
    "                                       rank[k], N[k], N[k+1], rank[k+2]])\n",
    "                n_eval += eval_index.shape[0]\n",
    "            else:\n",
    "                supercore = tn.zeros(eval_index.shape[0], dtype=dtype, device=device)\n",
    "                for ind in range(eval_index.shape[0]):\n",
    "                    supercore[ind] = function(*eval_index[ind,:])\n",
    "                supercore = tn.reshape(supercore, [rank[k], N[k], N[k+1], rank[k+2]])\n",
    "                n_eval += eval_index.shape[0]\n",
    "\n",
    "            # multiply with P_k left and right\n",
    "            supercore = tn.einsum('ij,jklm,mn->ikln',\n",
    "                                  Ps[k], supercore.to(dtype=dtype), Ps[k+2])\n",
    "            rank[k] = supercore.shape[0]\n",
    "            rank[k+2] = supercore.shape[3]\n",
    "            supercore = tn.reshape(\n",
    "                supercore, [supercore.shape[0]*supercore.shape[1], -1])\n",
    "\n",
    "            # split the super core with svd\n",
    "            U, S, V = SVD(supercore)\n",
    "            rnew = rank_chop(S.cpu().numpy(), tn.linalg.norm(\n",
    "                S).cpu().numpy()*eps/np.sqrt(d-1))+1\n",
    "            rnew = min(S.shape[0], rnew)\n",
    "            rnew = min(rmax, rnew)\n",
    "            U = U[:, :rnew]\n",
    "            S = S[:rnew]\n",
    "            V = V[:rnew, :]\n",
    "\n",
    "            # kick the rank\n",
    "            U = U @ tn.diag(S)\n",
    "            VK = tn.randn((kick, V.shape[1]), dtype=dtype, device=device)\n",
    "            V, Rtemp = QR(tn.cat((V, VK), 0).t())\n",
    "            radd = Rtemp.shape[1] - rnew\n",
    "            if radd > 0:\n",
    "                U = tn.cat(\n",
    "                    (U, tn.zeros((U.shape[0], radd), dtype=dtype, device=device)), 1)\n",
    "                U = U @ Rtemp.T\n",
    "                V = V.t()\n",
    "\n",
    "            # compute err (dx)\n",
    "            super_prev = tn.einsum('ijk,kmn->ijmn', cores[k], cores[k+1])\n",
    "            super_prev = tn.einsum(\n",
    "                'ij,jklm,mn->ikln', Ps[k], super_prev, Ps[k+2])\n",
    "            err = tn.linalg.norm(\n",
    "                supercore.flatten()-super_prev.flatten())/tn.linalg.norm(supercore)\n",
    "            max_err = max(max_err, err)\n",
    "            # update the rank\n",
    "            if verbose:\n",
    "                print('\\t\\trank updated %d -> %d, local error %e' %\n",
    "                      (rank[k+1], U.shape[1], err))\n",
    "            rank[k+1] = U.shape[1]\n",
    "\n",
    "            U = tn.linalg.solve(Ps[k], tn.reshape(U, [rank[k], -1]))\n",
    "            V = tn.linalg.solve(\n",
    "                Ps[k+2].t(), tn.reshape(V, [rank[k+1]*N[k+1], rank[k+2]]).t()).t()\n",
    "\n",
    "            # U = tn.einsum('ij,jkl->ikl',tn.linalg.inv(Ps[k]),tn.reshape(U,[rank[k],N[k],-1]))\n",
    "            # V = tn.einsum('ijk,kl->ijl',tn.reshape(V,[-1,N[k+1],rank[k+2]]),tn.linalg.inv(Ps[k+2]))\n",
    "\n",
    "            V = tn.reshape(V, [rank[k+1], -1])\n",
    "            U = tn.reshape(U, [-1, rank[k+1]])\n",
    "\n",
    "            # split cores\n",
    "            Qmat, Rmat = QR(V.T)\n",
    "            idx = _maxvol(Qmat)\n",
    "            Sub = Qmat[idx, :]\n",
    "            core_next = tn.linalg.solve(Sub.T, Qmat.T)\n",
    "            core = U@(Sub@Rmat).t()\n",
    "            cores[k] = tn.reshape(core, [rank[k], N[k], -1])\n",
    "            cores[k+1] = tn.reshape(core_next, [-1, N[k+1], rank[k+2]])\n",
    "\n",
    "            # calc Ps\n",
    "            tmp = tn.einsum('ijk,kl->ijl', cores[k+1], Ps[k+2])\n",
    "            _, tmp = QR(tn.reshape(tmp, [rank[k+1], -1]).t())\n",
    "            Ps[k+1] = tmp\n",
    "            # calc Idx\n",
    "            try:\n",
    "                tmp = tn.unravel_index(idx[:rank[k+1]], (N[k+1], rank[k+2]))\n",
    "            except:\n",
    "                tmp = np.unravel_index(idx[:rank[k+1]], (N[k+1], rank[k+2]))\n",
    "            idx_new = tn.tensor(\n",
    "                np.vstack((tmp[0].reshape([1, -1]), Idx[k+2][:, tmp[1]])))\n",
    "            Idx[k+1] = idx_new+0\n",
    "        # xxx = TT(cores)\n",
    "        # print('#            ',xxx[1,2,3,4])\n",
    "\n",
    "        # exit condition\n",
    "\n",
    "        if max_err < eps:\n",
    "            if verbose:\n",
    "                print('Max error %e < %e  ---->  DONE' % (max_err, eps))\n",
    "            break\n",
    "        else:\n",
    "            if verbose:\n",
    "                print('Max error %g' % (max_err))\n",
    "    if verbose:\n",
    "        print('number of function calls ', n_eval)\n",
    "        print()\n",
    "\n",
    "    return torchtt.TT(cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9d34bc",
   "metadata": {},
   "source": [
    "#### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6bbbf80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10, 2]), torch.Size([2, 10, 2]), torch.Size([2, 10, 1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores = torchtt.random([10, 10, 10], 2, tn.float64, 'cpu').cores\n",
    "\n",
    "cores[0].shape, cores[1].shape, cores[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eb5a89",
   "metadata": {},
   "source": [
    "#### Orthodonalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51bdf967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "torch.Size([1, 10, 2])\n",
      "torch.Size([2, 10, 2])\n",
      "torch.Size([2, 10, 1])\n",
      "[1, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "cores, rank = lr_orthogonal(cores, [1, 2, 1], False)\n",
    "\n",
    "print(len(cores))\n",
    "for core in cores:\n",
    "    print(core.shape)\n",
    "    \n",
    "print(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "978186ed-4f9f-4940-a255-677e8ab9cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = torchtt.random([10, 10, 10], 2, tn.float64, 'cpu')\n",
    "\n",
    "B.cores = cores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0f7558-85b9-48de-be19-00999f78d996",
   "metadata": {},
   "source": [
    "### Complute P (equivalent to V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bded363-955c-4f47-8e19-14f9f3fa4136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1.]], dtype=torch.float64), None, None, tensor([[1.]], dtype=torch.float64)]\n",
      "tensor([[1.]], dtype=torch.float64)\n",
      "[tensor([], size=(1, 0), dtype=torch.int64), None, None, tensor([], size=(0, 1), dtype=torch.int64)]\n",
      "(tensor([0, 0]), tensor([3, 8]))\n",
      "tensor([], size=(0, 2), dtype=torch.int64)\n",
      "[tensor([], size=(1, 0), dtype=torch.int64), None, tensor([[3, 8]]), tensor([], size=(0, 1), dtype=torch.int64)]\n"
     ]
    }
   ],
   "source": [
    "d = 3\n",
    "device = 'cpu'\n",
    "dtype = tn.float64\n",
    "N = [10, 10, 10]\n",
    "rank = [1, 2, 2, 1]\n",
    "\n",
    "# Just matrix with 1 one\n",
    "Ps = [tn.ones((1, 1), dtype=dtype, device=device)]+(d-1) * \\\n",
    "    [None] + [tn.ones((1, 1), dtype=dtype, device=device)]\n",
    "\n",
    "print(Ps)\n",
    "\n",
    "# Rm\n",
    "Rm = tn.ones((1, 1), dtype=dtype, device=device)\n",
    "print(Rm)\n",
    "\n",
    "# Idx\n",
    "Idx = [tn.zeros((1, 0), dtype=tn.int64)]+(d-1)*[None] + \\\n",
    "    [tn.zeros((0, 1), dtype=tn.int64)]\n",
    "\n",
    "print(Idx)\n",
    "\n",
    "# corek @ R\n",
    "tmp = tn.einsum('ijk,kl->ijl', cores[-1], Rm)\n",
    "# rank is [1, rk-1, rk], so rk-1 is fine\n",
    "tmp = tn.reshape(tmp, [rank[-2], -1]).t()\n",
    "\n",
    "# new core is Q from QR \n",
    "core, Rmat = QR(tmp)\n",
    "\n",
    "rnew = min(N[-1]*rank[-1], rank[-2])\n",
    "\n",
    "# Return all new rows\n",
    "Jk = _maxvol(core)\n",
    "\n",
    "# For some reason work with rank[k+1], N[k]\n",
    "# Unfold index to square matrix ( Jk // N , Jk % N ) (assume related to QR implementation)\n",
    "try:\n",
    "    tmp = tn.unravel_index(Jk[:rnew], (rank[-1], N[-1]))\n",
    "except:\n",
    "    tmp = np.unravel_index(Jk[:rnew], (rank[-1], N[-1]))\n",
    "\n",
    "print(tmp)\n",
    "print(Idx[d][:, tmp[0]])\n",
    "# idx_new = which index to choose from alphak+1 + Ind alpha k+1\n",
    "idx_new = tn.tensor(\n",
    "    np.vstack((tmp[1].reshape([1, -1]), Idx[d][:, tmp[0]])))\n",
    "\n",
    "#print(idx_new)\n",
    "\n",
    "Idx[d-1] = idx_new+0\n",
    "\n",
    "print(Idx)\n",
    "\n",
    "# New truncated Rm\n",
    "Rm = core[Jk, :]\n",
    "\n",
    "# Correspondingly trancate the core\n",
    "core = tn.linalg.solve(Rm.T, core.T)\n",
    "\n",
    "# Create new iteration of V\n",
    "Rm = (Rm@Rmat).t()\n",
    "\n",
    "# Save new core\n",
    "cores[-1] = tn.reshape(core, [rnew, N[-1], rank[-1]])\n",
    "# Compute new Ps\n",
    "core = tn.reshape(core, [-1, rank[-1]]) @ Ps[d]\n",
    "core = tn.reshape(core, [rank[-2], -1]).t()\n",
    "_, Ps[-2] = QR(core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "365373bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mats = []*(d+1)\n",
    "\n",
    "Ps = [tn.ones((1, 1), dtype=dtype, device=device)]+(d-1) * \\\n",
    "    [None] + [tn.ones((1, 1), dtype=dtype, device=device)]\n",
    "# ortho construct Ps (equivalent to Vk) from original paper to get Bk = A Vk^-1 Cheaply\n",
    "Rm = tn.ones((1, 1), dtype=dtype, device=device)\n",
    "Idx = [tn.zeros((1, 0), dtype=tn.int64)]+(d-1)*[None] + \\\n",
    "    [tn.zeros((0, 1), dtype=tn.int64)]\n",
    "for k in range(d-1, 0, -1):\n",
    "\n",
    "    tmp = tn.einsum('ijk,kl->ijl', cores[k], Rm)\n",
    "    tmp = tn.reshape(tmp, [rank[k], -1]).t()\n",
    "    core, Rmat = QR(tmp)\n",
    "\n",
    "    rnew = min(N[k]*rank[k+1], rank[k])\n",
    "    Jk = _maxvol(core)\n",
    "    # print(Jk)\n",
    "    try:\n",
    "        tmp = tn.unravel_index(Jk[:rnew], (rank[k+1], N[k]))\n",
    "    except:\n",
    "        tmp = np.unravel_index(Jk[:rnew], (rank[k+1], N[k]))\n",
    "    # if k==d-1:\n",
    "    #    idx_new = tn.tensor(tmp[1].reshape([1,-1]))\n",
    "    # else:\n",
    "    idx_new = tn.tensor(\n",
    "        np.vstack((tmp[1].reshape([1, -1]), Idx[k+1][:, tmp[0]])))\n",
    "\n",
    "    Idx[k] = idx_new+0\n",
    "\n",
    "    Rm = core[Jk, :]\n",
    "\n",
    "    core = tn.linalg.solve(Rm.T, core.T)\n",
    "    # core = tn.linalg.solve(Rm,core.T)\n",
    "    Rm = (Rm@Rmat).t()\n",
    "    # core = core.t()\n",
    "    cores[k] = tn.reshape(core, [rnew, N[k], rank[k+1]])\n",
    "    core = tn.reshape(core, [-1, rank[k+1]]) @ Ps[k+1]\n",
    "    core = tn.reshape(core, [rank[k], -1]).t()\n",
    "    _, Ps[k] = QR(core)\n",
    "cores[0] = tn.einsum('ijk,kl->ijl', cores[0], Rm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef2a237c-8676-452e-9343-4ac5b191f15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([], size=(1, 0), dtype=torch.int64),\n",
       " tensor([[0],\n",
       "         [1],\n",
       "         [5],\n",
       "         [7],\n",
       "         [8]]),\n",
       " tensor([[3, 8]]),\n",
       " tensor([], size=(0, 1), dtype=torch.int64)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 0\n",
    "function = lambda x1, x2, x3: x1 + x2 + x3\n",
    "eval_vect = False\n",
    "\n",
    "n_eval = 0\n",
    "eps = 1e-5\n",
    "rmax = 4\n",
    "kick = 2\n",
    "max_err = 1e-8\n",
    "#if verbose:\n",
    "#    print('\\tLR supercore %d,%d' % (k+1, k+2))\n",
    "\n",
    "# I1 = 1_k x i_k x 1_k+1 x 1_k+2 list of all possible index multiplication with i_k replaced by 1\n",
    "I1 = tn.reshape(tn.kron(tn.kron(tn.ones(rank[k], dtype=tn.int64), tn.arange(N[k], dtype=tn.int64)), tn.kron(\n",
    "    tn.ones(N[k+1], dtype=tn.int64), tn.ones(rank[k+2], dtype=tn.int64))), [-1, 1])\n",
    "# I2 = 1_k x 1_k x i_k+1 x 1_k+2\n",
    "I2 = tn.reshape(tn.kron(tn.kron(tn.ones(rank[k], dtype=tn.int64), tn.ones(N[k], dtype=tn.int64)), tn.kron(\n",
    "    tn.arange(N[k+1], dtype=tn.int64), tn.ones(rank[k+2], dtype=tn.int64))), [-1, 1])\n",
    "# I3 = Idx[k][alpha_k x 1_k x 1_k+1 x 1_k+2] preselected indexes \n",
    "I3 = Idx[k][tn.kron(tn.kron(tn.arange(rank[k], dtype=tn.int64), tn.ones(N[k], dtype=tn.int64)), tn.kron(\n",
    "    tn.ones(N[k+1], dtype=tn.int64), tn.ones(rank[k+2], dtype=tn.int64))), :]\n",
    "# I2 = Idx[k+2][1_k x 1_k x 1_k+1 x alpha_k+2] preselected indexes\n",
    "I4 = Idx[k+2][:, tn.kron(tn.kron(tn.ones(rank[k], dtype=tn.int64), tn.ones(N[k], dtype=tn.int64)),\n",
    "                         tn.kron(tn.ones(N[k+1], dtype=tn.int64), tn.arange(rank[k+2], dtype=tn.int64)))].t()\n",
    "\n",
    "\n",
    "\n",
    "eval_index = tn.concat((I3, I1, I2, I4), 1)\n",
    "\n",
    "eval_index = tn.reshape(eval_index, [-1, d]).to(dtype=tn.int64)\n",
    "\n",
    "#if verbose:\n",
    "#    print('\\t\\tnumber evaluations', eval_index.shape[0])\n",
    "\n",
    "# Do function calls\n",
    "if eval_vect:\n",
    "    supercore = tn.reshape(function(eval_index), [\n",
    "                           rank[k], N[k], N[k+1], rank[k+2]])\n",
    "    n_eval += eval_index.shape[0]\n",
    "else:\n",
    "    supercore = tn.zeros(eval_index.shape[0], dtype=dtype, device=device)\n",
    "    for ind in range(eval_index.shape[0]):\n",
    "        supercore[ind] = function(*eval_index[ind,:])\n",
    "    supercore = tn.reshape(supercore, [rank[k], N[k], N[k+1], rank[k+2]])\n",
    "    n_eval += eval_index.shape[0]\n",
    "\n",
    "# multiply with P_k left and right\n",
    "supercore = tn.einsum('ij,jklm,mn->ikln',\n",
    "                      Ps[k], supercore.to(dtype=dtype), Ps[k+2])\n",
    "rank[k] = supercore.shape[0]\n",
    "rank[k+2] = supercore.shape[3]\n",
    "supercore = tn.reshape(\n",
    "    supercore, [supercore.shape[0]*supercore.shape[1], -1])\n",
    "\n",
    "# split the super core with svd\n",
    "U, S, V = SVD(supercore)\n",
    "rnew = rank_chop(S.cpu().numpy(), tn.linalg.norm(\n",
    "    S).cpu().numpy()*eps/np.sqrt(d-1))+1\n",
    "rnew = min(S.shape[0], rnew)\n",
    "rnew = min(rmax, rnew)\n",
    "U = U[:, :rnew]\n",
    "S = S[:rnew]\n",
    "V = V[:rnew, :]\n",
    "# print('kkt new',tn.linalg.norm(supercore-U@tn.diag(S)@V))\n",
    "# kick the rank\n",
    "V = tn.diag(S) @ V\n",
    "UK = tn.randn((U.shape[0], kick), dtype=dtype, device=device)\n",
    "U, Rtemp = QR(tn.cat((U, UK), 1))\n",
    "radd = U.shape[1] - rnew\n",
    "if radd > 0:\n",
    "    V = tn.cat(\n",
    "        (V, tn.zeros((radd, V.shape[1]), dtype=dtype, device=device)), 0)\n",
    "    V = Rtemp @ V\n",
    "# print('kkt new',tn.linalg.norm(supercore-U@V))\n",
    "# compute err (dx)\n",
    "super_prev = tn.einsum('ijk,kmn->ijmn', cores[k], cores[k+1])\n",
    "super_prev = tn.einsum(\n",
    "    'ij,jklm,mn->ikln', Ps[k], super_prev, Ps[k+2])\n",
    "err = tn.linalg.norm(\n",
    "    supercore.flatten()-super_prev.flatten())/tn.linalg.norm(supercore)\n",
    "max_err = max(max_err, err)\n",
    "# update the rank\n",
    "#if verbose:\n",
    "#    print('\\t\\trank updated %d -> %d, local error %e' %\n",
    "#          (rank[k+1], U.shape[1], err))\n",
    "rank[k+1] = U.shape[1]\n",
    "\n",
    "U = tn.linalg.solve(Ps[k], tn.reshape(U, [rank[k], -1]))\n",
    "V = tn.linalg.solve(\n",
    "    Ps[k+2].t(), tn.reshape(V, [rank[k+1]*N[k+1], rank[k+2]]).t()).t()\n",
    "\n",
    "# U = tn.einsum('ij,jkl->ikl',tn.linalg.inv(Ps[k]),tn.reshape(U,[rank[k],N[k],-1]))\n",
    "# V = tn.einsum('ijk,kl->ijl',tn.reshape(V,[-1,N[k+1],rank[k+2]]),tn.linalg.inv(Ps[k+2]))\n",
    "\n",
    "V = tn.reshape(V, [rank[k+1], -1])\n",
    "U = tn.reshape(U, [-1, rank[k+1]])\n",
    "\n",
    "# split cores\n",
    "Qmat, Rmat = QR(U)\n",
    "idx = _maxvol(Qmat)\n",
    "Sub = Qmat[idx, :]\n",
    "core = tn.linalg.solve(Sub.T, Qmat.T).t()\n",
    "core_next = Sub@Rmat@V\n",
    "cores[k] = tn.reshape(core, [rank[k], N[k], rank[k+1]])\n",
    "cores[k+1] = tn.reshape(core_next, [rank[k+1], N[k+1], rank[k+2]])\n",
    "# calc Ps\n",
    "tmp = tn.einsum('ij,jkl->ikl', Ps[k], cores[k])\n",
    "_, Ps[k+1] = QR(tn.reshape(tmp, [rank[k]*N[k], rank[k+1]]))\n",
    "\n",
    "# calc Idx\n",
    "try:\n",
    "    tmp = tn.unravel_index(idx[:rank[k+1]], (rank[k], N[k]))\n",
    "except:\n",
    "    tmp = np.unravel_index(idx[:rank[k+1]], (rank[k], N[k]))\n",
    "idx_new = tn.tensor(\n",
    "    np.hstack((Idx[k][tmp[0], :], tmp[1].reshape([-1, 1]))))\n",
    "Idx[k+1] = idx_new+0\n",
    "\n",
    "Idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057db3f5-d9d1-494f-8fcb-f2b6416f7423",
   "metadata": {},
   "source": [
    "### Backward sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17919f36-d44c-41bf-a336-c77b0593532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# right to left\n",
    "\n",
    "for k in range(d-2, -1, -1):\n",
    "    if verbose:\n",
    "        print('\\tRL supercore %d,%d' % (k+1, k+2))\n",
    "    I1 = tn.reshape(tn.kron(tn.kron(tn.ones(rank[k], dtype=tn.int64), tn.arange(N[k], dtype=tn.int64)), tn.kron(\n",
    "        tn.ones(N[k+1], dtype=tn.int64), tn.ones(rank[k+2], dtype=tn.int64))), [-1, 1])\n",
    "    I2 = tn.reshape(tn.kron(tn.kron(tn.ones(rank[k], dtype=tn.int64), tn.ones(N[k], dtype=tn.int64)), tn.kron(\n",
    "        tn.arange(N[k+1], dtype=tn.int64), tn.ones(rank[k+2], dtype=tn.int64))), [-1, 1])\n",
    "    I3 = Idx[k][tn.kron(tn.kron(tn.arange(rank[k], dtype=tn.int64), tn.ones(N[k], dtype=tn.int64)), tn.kron(\n",
    "        tn.ones(N[k+1], dtype=tn.int64), tn.ones(rank[k+2], dtype=tn.int64))), :]\n",
    "    I4 = Idx[k+2][:, tn.kron(tn.kron(tn.ones(rank[k], dtype=tn.int64), tn.ones(N[k], dtype=tn.int64)),\n",
    "                             tn.kron(tn.ones(N[k+1], dtype=tn.int64), tn.arange(rank[k+2], dtype=tn.int64)))].t()\n",
    "\n",
    "    eval_index = tn.concat((I3, I1, I2, I4), 1)\n",
    "    eval_index = tn.reshape(eval_index, [-1, d]).to(dtype=tn.int64)\n",
    "\n",
    "    if verbose:\n",
    "        print('\\t\\tnumber evaluations', eval_index.shape[0])\n",
    "\n",
    "    if eval_vect:\n",
    "        supercore = tn.reshape(function(eval_index).to(dtype=dtype), [\n",
    "                               rank[k], N[k], N[k+1], rank[k+2]])\n",
    "        n_eval += eval_index.shape[0]\n",
    "    else:\n",
    "        supercore = tn.zeros(eval_index.shape[0], dtype=dtype, device=device)\n",
    "        for ind in range(eval_index.shape[0]):\n",
    "            supercore[ind] = function(*eval_index[ind,:])\n",
    "        supercore = tn.reshape(supercore, [rank[k], N[k], N[k+1], rank[k+2]])\n",
    "        n_eval += eval_index.shape[0]\n",
    "\n",
    "    # multiply with P_k left and right\n",
    "    supercore = tn.einsum('ij,jklm,mn->ikln',\n",
    "                          Ps[k], supercore.to(dtype=dtype), Ps[k+2])\n",
    "    rank[k] = supercore.shape[0]\n",
    "    rank[k+2] = supercore.shape[3]\n",
    "    supercore = tn.reshape(\n",
    "        supercore, [supercore.shape[0]*supercore.shape[1], -1])\n",
    "\n",
    "    # split the super core with svd\n",
    "    U, S, V = SVD(supercore)\n",
    "    rnew = rank_chop(S.cpu().numpy(), tn.linalg.norm(\n",
    "        S).cpu().numpy()*eps/np.sqrt(d-1))+1\n",
    "    rnew = min(S.shape[0], rnew)\n",
    "    rnew = min(rmax, rnew)\n",
    "    U = U[:, :rnew]\n",
    "    S = S[:rnew]\n",
    "    V = V[:rnew, :]\n",
    "\n",
    "    # kick the rank\n",
    "    U = U @ tn.diag(S)\n",
    "    VK = tn.randn((kick, V.shape[1]), dtype=dtype, device=device)\n",
    "    V, Rtemp = QR(tn.cat((V, VK), 0).t())\n",
    "    radd = Rtemp.shape[1] - rnew\n",
    "    if radd > 0:\n",
    "        U = tn.cat(\n",
    "            (U, tn.zeros((U.shape[0], radd), dtype=dtype, device=device)), 1)\n",
    "        U = U @ Rtemp.T\n",
    "        V = V.t()\n",
    "\n",
    "    # compute err (dx)\n",
    "    super_prev = tn.einsum('ijk,kmn->ijmn', cores[k], cores[k+1])\n",
    "    super_prev = tn.einsum(\n",
    "        'ij,jklm,mn->ikln', Ps[k], super_prev, Ps[k+2])\n",
    "    err = tn.linalg.norm(\n",
    "        supercore.flatten()-super_prev.flatten())/tn.linalg.norm(supercore)\n",
    "    max_err = max(max_err, err)\n",
    "    # update the rank\n",
    "    if verbose:\n",
    "        print('\\t\\trank updated %d -> %d, local error %e' %\n",
    "              (rank[k+1], U.shape[1], err))\n",
    "    rank[k+1] = U.shape[1]\n",
    "\n",
    "    U = tn.linalg.solve(Ps[k], tn.reshape(U, [rank[k], -1]))\n",
    "    V = tn.linalg.solve(\n",
    "        Ps[k+2].t(), tn.reshape(V, [rank[k+1]*N[k+1], rank[k+2]]).t()).t()\n",
    "\n",
    "    # U = tn.einsum('ij,jkl->ikl',tn.linalg.inv(Ps[k]),tn.reshape(U,[rank[k],N[k],-1]))\n",
    "    # V = tn.einsum('ijk,kl->ijl',tn.reshape(V,[-1,N[k+1],rank[k+2]]),tn.linalg.inv(Ps[k+2]))\n",
    "\n",
    "    V = tn.reshape(V, [rank[k+1], -1])\n",
    "    U = tn.reshape(U, [-1, rank[k+1]])\n",
    "\n",
    "    # split cores\n",
    "    Qmat, Rmat = QR(V.T)\n",
    "    idx = _maxvol(Qmat)\n",
    "    Sub = Qmat[idx, :]\n",
    "    core_next = tn.linalg.solve(Sub.T, Qmat.T)\n",
    "    core = U@(Sub@Rmat).t()\n",
    "    cores[k] = tn.reshape(core, [rank[k], N[k], -1])\n",
    "    cores[k+1] = tn.reshape(core_next, [-1, N[k+1], rank[k+2]])\n",
    "\n",
    "    # calc Ps\n",
    "    tmp = tn.einsum('ijk,kl->ijl', cores[k+1], Ps[k+2])\n",
    "    _, tmp = QR(tn.reshape(tmp, [rank[k+1], -1]).t())\n",
    "    Ps[k+1] = tmp\n",
    "    # calc Idx\n",
    "    try:\n",
    "        tmp = tn.unravel_index(idx[:rank[k+1]], (N[k+1], rank[k+2]))\n",
    "    except:\n",
    "        tmp = np.unravel_index(idx[:rank[k+1]], (N[k+1], rank[k+2]))\n",
    "    idx_new = tn.tensor(\n",
    "        np.vstack((tmp[0].reshape([1, -1]), Idx[k+2][:, tmp[1]])))\n",
    "    Idx[k+1] = idx_new+0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73bf1ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/temporary/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/torchtt/_dmrg.py:19: UserWarning: \u001b[33m\n",
      "C++ implementation not available. Using pure Python.\n",
      "\u001b[0m\n",
      "  warnings.warn(\"\\x1B[33m\\nC++ implementation not available. Using pure Python.\\n\\033[0m\")\n",
      "/Users/temporary/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/torchtt/_amen.py:21: UserWarning: \u001b[33m\n",
      "C++ implementation not available. Using pure Python.\n",
      "\u001b[0m\n",
      "  warnings.warn(\n",
      "/Users/temporary/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/torchtt/solvers.py:21: UserWarning: \u001b[33m\n",
      "C++ implementation not available. Using pure Python.\n",
      "\u001b[0m\n",
      "  warnings.warn(\n",
      "/Users/temporary/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/torchtt/cpp.py:12: UserWarning: \u001b[33m\n",
      "C++ implementation not available. Using pure Python.\n",
      "\u001b[0m\n",
      "  warnings.warn(\"\\x1B[33m\\nC++ implementation not available. Using pure Python.\\n\\033[0m\")\n",
      "/Users/temporary/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/torchtt/__init__.py:34: UserWarning: \u001b[33m\n",
      "C++ implementation not available. Using pure Python.\n",
      "\u001b[0m\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torchtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a89758c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0984, dtype=torch.float64)\n",
      "tensor(0.1870, dtype=torch.float64)\n",
      "tensor(0.4310, dtype=torch.float64)\n",
      "tensor(1.4508, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torchtt\n",
    "import torch as tn\n",
    "from torchtt._decomposition import lr_orthogonal, rl_orthogonal\n",
    "d = 4\n",
    "tn.manual_seed(32)\n",
    "\n",
    "random_tt = torchtt.random([10, 10, 10, 10], 2, tn.float64, 'cpu')\n",
    "cores = random_tt.cores\n",
    "ranks = random_tt.R\n",
    "vectors = [tn.rand(10, dtype = tn.float64) for i in range(d)]\n",
    "\n",
    "for i in range(d):\n",
    "    print(cores[i][0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a50f456e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10, 2]),\n",
       " torch.Size([2, 10, 2]),\n",
       " torch.Size([2, 10, 2]),\n",
       " torch.Size([2, 10, 1]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores[0].shape, cores[1].shape, cores[2].shape, cores[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6e071a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-125.3333], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_out = tn.ones(1, dtype = tn.float64)\n",
    "for i in range(d-1, -1, -1):\n",
    "    u_out = tn.einsum('abc,b,c->a', cores[i], vectors[i], u_out)\n",
    "    \n",
    "u_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b07b6b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-125.3333], dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3f18dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0287, dtype=torch.float64)\n",
      "tensor(-0.1107, dtype=torch.float64)\n",
      "tensor(-0.1985, dtype=torch.float64)\n",
      "tensor(96.4381, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "cores, R = lr_orthogonal(cores, ranks, is_ttm=False)\n",
    "\n",
    "random_tt.cores = cores\n",
    "\n",
    "for i in range(d):\n",
    "    print(cores[i][0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6673224f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000],\n",
       "        [0.0000, 1.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores[0].flatten(0, 1).T @ cores[0].flatten(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c32b6c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 2.0817e-17],\n",
       "        [2.0817e-17, 1.0000e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores[1].flatten(0, 1).T @ cores[1].flatten(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de099b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00, -6.9389e-17],\n",
       "        [-6.9389e-17,  1.0000e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores[2].flatten(0, 1).T @ cores[2].flatten(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17f108be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[74083.6800]], dtype=torch.float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores[3].flatten(0, 1).T @ cores[3].flatten(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f8e02a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-125.3333], dtype=torch.float64)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_out = tn.ones(1, dtype = tn.float64)\n",
    "for i in range(d-1, -1, -1):\n",
    "    u_out = tn.einsum('abc,b,c->a', cores[i], vectors[i], u_out)\n",
    "    \n",
    "u_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6d8ea404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.2268, dtype=torch.float64)\n",
      "tensor(-0.1423, dtype=torch.float64)\n",
      "tensor(-0.1413, dtype=torch.float64)\n",
      "tensor(-0.4976, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "cores, R = rl_orthogonal(cores, ranks, is_ttm=False)\n",
    "\n",
    "random_tt.cores = cores\n",
    "\n",
    "for i in range(d):\n",
    "    print(cores[i][0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "349cc90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-125.3333], dtype=torch.float64)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_out = tn.ones(1, dtype = tn.float64)\n",
    "for i in range(d-1, -1, -1):\n",
    "    u_out = tn.einsum('abc,b,c->a', cores[i], vectors[i], u_out)\n",
    "    \n",
    "u_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_mkl",
   "language": "python",
   "name": "torch_mkl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
