{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6d35d75-0a83-48a3-97b3-07411515442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e3nn.o3 import spherical_harmonics\n",
    "from e3nn.o3 import wigner_3j\n",
    "from torch import Tensor\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d5682ec-579f-4e4f-a03e-d059270c5083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100, 9])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = torch.randn((10, 100, 3))\n",
    "r_ang = r/torch.linalg.vector_norm(r, dim = -1).unsqueeze(-1)\n",
    "\n",
    "\n",
    "Y_1 = spherical_harmonics('0e', r_ang, normalize = 'component')\n",
    "Y_2 = spherical_harmonics('1o', r_ang, normalize = 'component')\n",
    "Y_3 = spherical_harmonics('2e', r_ang, normalize = 'component')\n",
    "\n",
    "features = torch.concatenate([Y_1, Y_2, Y_3], dim = -1)\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84e0fda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 100, 1]), torch.Size([10, 100, 3]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_1.shape, Y_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84984a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1a4fccd-d3e3-4fda-bffc-b5c1bc82b095",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1, l2, l3 = 1, 2, 3\n",
    "m1, m2, m3 = l1, l2, l3 # m1 = 0, m2 = 0, m3 = 0\n",
    "C123 = wigner_3j(1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5cc496a-9b93-4e74-85ba-ca1642e5d80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2928)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C123_000 = C123[m1, m2, m3]\n",
    "\n",
    "C123_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4598f711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 7])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C123.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "096e842c-9505-4fb0-a35e-5b989f732638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class weigner_3j_img_to_real():\n",
    "\n",
    "\n",
    "    def __init__(self, l):\n",
    "        self.l = l\n",
    "\n",
    "    def __call__(self):\n",
    "        matrix = torch.zeros((2*self.l + 1, 2*self.l + 1), dtype = torch.complex64)\n",
    "\n",
    "        mult = 1\n",
    "        for i in range(2*self.l + 1):\n",
    "            \n",
    "            if i < self.l:\n",
    "                matrix[i, i] = 1.0j/2**(1/2.)\n",
    "                matrix[2*self.l + 1 - i - 1, i] = 1/2**(1/2.)\n",
    "            elif i == self.l:\n",
    "                matrix[i, i] = 1.\n",
    "            else:\n",
    "                matrix[i, i] = (-1.0)*mult/2**(1/2.)\n",
    "                matrix[2*self.l + 1 - i - 1, i] = (-1.0j)*mult/2**(1/2.)\n",
    "                mult *= -1\n",
    "        \n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5841bc65-2103-46f7-8b22-bac392e8a753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000+1.0000j,  0.0000+0.0000j,  0.0000-1.0000j],\n",
       "        [ 0.0000+0.0000j,  1.4142+0.0000j,  0.0000+0.0000j],\n",
       "        [ 1.0000+0.0000j,  0.0000+0.0000j, -1.0000+0.0000j]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weigner_3j_img_to_real(1)()*2**(1/2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65b1c83c-fac0-47f7-8237-623ed87fd857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tri_ineq(l1, l2, l3):\n",
    "    #print(max([l1, l2, l3]), min([l1 + l2, l2 + l3, l1 + l3]))\n",
    "    return max([l1, l2, l3]) <= min([l1 + l2, l2 + l3, l1 + l3])\n",
    "\n",
    "class order_3_equvariant_tensor():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Basically transformation using\n",
    "            C (l1 l2 l3)\n",
    "              (m1 m2 m3)\"\"\"\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def __call__(self, l1, l2, l3, n1, n2, n3):\n",
    "        self.l1 = l1; self.l2 = l2; self.l3 = l3\n",
    "        self.n1 = n1; self.n2 = n2; self.n3 = n3\n",
    "        \n",
    "        weight = torch.zeros([n1, n2, n3])\n",
    "        if tri_ineq(l1, l2, l3):\n",
    "            W = nn.Parameter(nn.init.kaiming_uniform_(weight))\n",
    "            symbol_3j = wigner_3j(l1, l2, l3)\n",
    "            return symbol_3j.view(*symbol_3j.shape, 1, 1, 1)*W.view(1, 1, 1, *weight.shape)\n",
    "        else:\n",
    "            return torch.zeros((2*l1 + 1, 2*l2 + 1, 2*l3 + 1, n1, n2, n3))\n",
    "        \n",
    "        \n",
    "class order_2_equivariant_tensor():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Makes the 2nd order tensor in a way that\n",
    "           each lm is multiplied by coefficient c, no angular momentum mixing\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, l1, l2, n1, n2):\n",
    "        self.l1 = l1; self.l2 = l2\n",
    "        self.n1 = n1; self.n2 = n2\n",
    "        \n",
    "        weight = torch.zeros([n1, n2])\n",
    "        if l1 == l2: # same as tri_ineq(l1, l2, 0)\n",
    "            W = nn.Parameter(nn.init.kaiming_uniform_(weight))\n",
    "            return W.view(*weight.shape)\n",
    "        else:\n",
    "            return torch.zeros((n1, n2))\n",
    "\n",
    "        \n",
    "class order_1_equivariant_tensor():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Makes the 1nd order tensor in a way that\n",
    "           each lm is multiplied by coefficient c, no angular momentum mixing\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, l1, n1):\n",
    "        self.l1 = l1;\n",
    "        self.n1 = n1;\n",
    "        \n",
    "        weight = torch.zeros([n1])\n",
    "        W = nn.Parameter(nn.init.uniform(weight))\n",
    "        return torch.ones((2*l1 + 1, 1))*W.view(1, *weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b288318e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "tensor(-0.3470, grad_fn=<SumBackward0>)\n",
      "torch.Size([3, 5, 7, 4, 6, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p2/mwhbpbfx4dx636dcf50t7_rr0000gn/T/ipykernel_7802/4165673985.py:61: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  W = nn.Parameter(nn.init.uniform(weight))\n"
     ]
    }
   ],
   "source": [
    "print(order_1_equivariant_tensor()(2, 3).shape)\n",
    "print(order_1_equivariant_tensor()(2, 10).shape)\n",
    "\n",
    "print(order_2_equivariant_tensor()(2, 2, 3, 4).shape)\n",
    "print(order_2_equivariant_tensor()(2, 2, 3, 4).shape)\n",
    "\n",
    "\n",
    "print(order_3_equvariant_tensor()(1, 2, 3, 4, 6, 8).sum())\n",
    "print(order_3_equvariant_tensor()(1, 2, 3, 4, 6, 8).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccb3a36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_ineq(0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8346b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 7])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C123.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb0d2d",
   "metadata": {},
   "source": [
    "### Loading qm9 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50ba7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference example\n",
    "from nequip.data import dataset_from_config\n",
    "from nequip.utils import Config\n",
    "#from nequip.utils.misc import get_default_device_name\n",
    "#from nequip.utils.config import _GLOBAL_ALL_ASKED_FOR_KEYS\n",
    "\n",
    "from nequip.model import model_from_config\n",
    "\n",
    "\n",
    "default_config = dict(\n",
    "    root=\"./\",\n",
    "    tensorboard=False,\n",
    "    wandb=False,\n",
    "    model_builders=[\n",
    "        \"SimpleIrrepsConfig\",\n",
    "        \"EnergyModel\",\n",
    "        \"PerSpeciesRescale\",\n",
    "        \"StressForceOutput\",\n",
    "        \"RescaleEnergyEtc\",\n",
    "    ],\n",
    "    dataset_statistics_stride=1,\n",
    "    device='cuda',\n",
    "    default_dtype=\"float64\",\n",
    "    model_dtype=\"float32\",\n",
    "    allow_tf32=True,\n",
    "    verbose=\"INFO\",\n",
    "    model_debug_mode=False,\n",
    "    equivariance_test=False,\n",
    "    grad_anomaly_mode=False,\n",
    "    gpu_oom_offload=False,\n",
    "    append=False,\n",
    "    warn_unused=False,\n",
    "    _jit_bailout_depth=2,  # avoid 20 iters of pain, see https://github.com/pytorch/pytorch/issues/52286\n",
    "    # Quote from eelison in PyTorch slack:\n",
    "    # https://pytorch.slack.com/archives/CDZD1FANA/p1644259272007529?thread_ts=1644064449.039479&cid=CDZD1FANA\n",
    "    # > Right now the default behavior is to specialize twice on static shapes and then on dynamic shapes.\n",
    "    # > To reduce warmup time you can do something like setFusionStrartegy({{FusionBehavior::DYNAMIC, 3}})\n",
    "    # > ... Although we would wouldn't really expect to recompile a dynamic shape fusion in a model,\n",
    "    # > provided broadcasting patterns remain fixed\n",
    "    # We default to DYNAMIC alone because the number of edges is always dynamic,\n",
    "    # even if the number of atoms is fixed:\n",
    "    _jit_fusion_strategy=[(\"DYNAMIC\", 3)],\n",
    "    # Due to what appear to be ongoing bugs with nvFuser, we default to NNC (fuser1) for now:\n",
    "    # TODO: still default to NNC on CPU regardless even if change this for GPU\n",
    "    # TODO: default for ROCm?\n",
    "    _jit_fuser=\"fuser1\",\n",
    ")\n",
    "\n",
    "# All default_config keys are valid / requested\n",
    "#_GLOBAL_ALL_ASKED_FOR_KEYS.update(default_config.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d99679c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtomicData(atom_types=[19, 1], cell=[3, 3], edge_cell_shift=[340, 3], edge_index=[2, 340], pbc=[3], pos=[19, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config.from_file('./configs/example_SpinGNNPlus.yaml', defaults=default_config)\n",
    "    \n",
    "\n",
    "dataset = dataset_from_config(config, prefix=\"dataset\")\n",
    "\n",
    "validation_dataset = None\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dc2da30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_jit_bailout_depth': 2, '_jit_fusion_strategy': [('DYNAMIC', 3)], '_jit_fuser': 'fuser1', 'root': 'results/qm9', 'tensorboard': False, 'wandb': False, 'model_builders': ['allegro.model.SpinGNNPlus', 'PerSpeciesRescale', 'ParaStressForceSpinForceOutput', 'RescaleEnergyEtc'], 'dataset_statistics_stride': 1, 'device': 'cuda', 'default_dtype': 'float32', 'model_dtype': 'float32', 'allow_tf32': True, 'verbose': 'debug', 'model_debug_mode': False, 'equivariance_test': False, 'grad_anomaly_mode': False, 'gpu_oom_offload': False, 'append': True, 'warn_unused': False, 'run_name': 'example', 'seed': 123456, 'dataset_seed': 123456, 'r_max': 6.0, 'avg_num_neighbors': 'auto', 'BesselBasis_trainable': True, 'PolynomialCutoff_p': 6, 'l_max': 2, 'parity': 'o3_full', 'num_layers': 2, 'env_embed_multiplicity': 64, 'embed_initial_edge': True, 'two_body_latent_mlp_latent_dimensions': [128, 256, 512, 1024], 'two_body_latent_mlp_nonlinearity': 'silu', 'two_body_latent_mlp_initialization': 'uniform', 'latent_mlp_latent_dimensions': [1024, 1024, 1024], 'latent_mlp_nonlinearity': 'silu', 'latent_mlp_initialization': 'uniform', 'latent_resnet': True, 'env_embed_mlp_latent_dimensions': [], 'env_embed_mlp_nonlinearity': None, 'env_embed_mlp_initialization': 'uniform', 'edge_eng_mlp_latent_dimensions': [128], 'edge_eng_mlp_nonlinearity': None, 'edge_eng_mlp_initialization': 'uniform', 'dataset': 'ase', 'dataset_file_name': './data/qm9.xyz', 'ase_args': {'format': 'extxyz'}, 'chemical_symbols': ['H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F'], 'wandb_project': 'qm9', 'n_train': 950, 'n_val': 50, 'batch_size': 5, 'max_epochs': 1000000, 'learning_rate': 0.001, 'train_val_split': 'random', 'shuffle': True, 'metrics_key': 'validation_loss', 'use_ema': True, 'ema_decay': 0.99, 'ema_use_num_updates': True, 'loss_coeffs': {'forces': 1.0, 'total_energy': [1.0, 'PerAtomMSELoss']}, 'optimizer_name': 'Adam', 'optimizer_params': {'amsgrad': False, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0}, 'lr_scheduler_name': 'ReduceLROnPlateau', 'lr_scheduler_patience': 50, 'lr_scheduler_factor': 0.5, 'early_stopping_upper_bounds': {'cumulative_wall': 604800.0}, 'early_stopping_lower_bounds': {'LR': 1e-05}, 'early_stopping_patiences': {'validation_loss': 100}, 'dataset_AtomicData_options': {'r_max': 6.0}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e10fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "from nequip.data import AtomicData, AtomicDataDict\n",
    "from torch.nn.functional import one_hot\n",
    "from e3nn.nn import FullyConnectedNet\n",
    "from allegro import with_edge_spin_length\n",
    "from allegro import _keys\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "data = [AtomicData.to_AtomicDataDict(dataset[i]) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2ec18ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf29945a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([494, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 2\n",
    "\n",
    "num_types = len(config['chemical_symbols'])\n",
    "\n",
    "\n",
    "atom_types_embed = data[i]['atom_types'][data[i]['edge_index'][0]]*num_types + data[i]['atom_types'][data[i]['edge_index'][1]]\n",
    "\n",
    "atom_types_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9591ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nequip.data import AtomicDataDict, AtomicDataset\n",
    "from nequip.nn.embedding import (\n",
    "    OneHotAtomEncoding,\n",
    "    SphericalHarmonicEdgeAttrs,\n",
    "    RadialBasisEdgeEncoding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5935fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e3nn import o3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6c2c373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1x0ee+1x1oe+1x2ee"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o3.Irreps.spherical_harmonics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cad5833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([494, 8]) torch.Size([494, 9])\n"
     ]
    }
   ],
   "source": [
    "from nequip.data import AtomicDataDict, AtomicDataset\n",
    "from nequip.nn.embedding import (\n",
    "    OneHotAtomEncoding,\n",
    "    SphericalHarmonicEdgeAttrs,\n",
    "    RadialBasisEdgeEncoding,\n",
    ")\n",
    "from e3nn import o3\n",
    "\n",
    "L = 2\n",
    "\n",
    "irreps_edge_sh = o3.Irreps.spherical_harmonics(L)\n",
    "\n",
    "rbe = RadialBasisEdgeEncoding(basis_kwargs={'r_max': config['r_max'], \n",
    "                                            'num_basis': 8},\n",
    "                              cutoff_kwargs={'r_max': config['r_max']},\n",
    "                              out_field=AtomicDataDict.EDGE_EMBEDDING_KEY,\n",
    "                              )\n",
    "\n",
    "sh = SphericalHarmonicEdgeAttrs(irreps_edge_sh=irreps_edge_sh)\n",
    "\n",
    "data = [rbe(data[i]) for i in range(len(data))]\n",
    "\n",
    "data = [sh(data[i]) for i in range(len(data))]\n",
    "\n",
    "\n",
    "print(data[i]['edge_embedding'].shape, data[i]['edge_attrs'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbb5715",
   "metadata": {},
   "source": [
    "## Feature Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "910c7aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 494]) torch.Size([494, 3, 4]) torch.Size([494, 9, 4])\n",
      "torch.Size([23, 9, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch_runstats.scatter import scatter\n",
    "\n",
    "N_rad = 8\n",
    "\n",
    "N_spec_rank = 4\n",
    "N_rad_rank = 4\n",
    "\n",
    "Q = data[i]['edge_embedding']\n",
    "\n",
    "A = torch.randn(L + 1, N_spec_rank, num_types**2)\n",
    "B = torch.randn(L + 1, N_rad_rank, N_rad, N_spec_rank)\n",
    "\n",
    "\n",
    "a = A[:, :, atom_types_embed].squeeze(-1)\n",
    "\n",
    "b = torch.einsum('Lrnk,LkE,En->ELr', B, a, Q)\n",
    "\n",
    "Y = data[i]['edge_attrs']\n",
    "\n",
    "#print(data[i]['edge_attrs'][:, slices])\n",
    "F = torch.concat([torch.einsum('Em,En->Emn', Y[:, slices],\n",
    "                               b[:, l]) for l, slices in enumerate(irreps_edge_sh.slices())], dim = -2)\n",
    "\n",
    "print(a.shape, b.shape, F.shape)\n",
    "\n",
    "\n",
    "species = data[i][AtomicDataDict.ATOM_TYPE_KEY].squeeze(-1)\n",
    "edge_center = data[i][AtomicDataDict.EDGE_INDEX_KEY][0]\n",
    "edge_neighbor = data[i][AtomicDataDict.EDGE_INDEX_KEY][1]\n",
    "\n",
    "center_species = species[edge_center]\n",
    "neighbor_species = species[edge_neighbor]\n",
    "\n",
    "F = scatter(F, edge_center, dim=0, dim_size=len(species))\n",
    "\n",
    "print(F.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e317052e",
   "metadata": {},
   "source": [
    "### Order 2 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cea8590",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = 4\n",
    "n2 = 32\n",
    "L1 = L2 = range(L + 1)\n",
    "\n",
    "u_in = F.clone()\n",
    "T_2 = [order_2_equivariant_tensor()(l, l, n1, n2) for l in L1]\n",
    "\n",
    "\n",
    "u_out = torch.zeros((u_in.shape[0], u_in.shape[1], n2))\n",
    "for i, slices in enumerate(irreps_edge_sh.slices()):\n",
    "    u_out[:, slices, :] = torch.einsum('ij,Nmi->Nmj', T_2[i], u_in[:, slices, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9901d2a",
   "metadata": {},
   "source": [
    "### Order 3 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30d0a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n3 = 32\n",
    "L3 = range(L + 1)\n",
    "\n",
    "T_3 = [[[order_3_equvariant_tensor()(l3, l1, l2, n3, n1, n2) for l2 in L2] for l1 in L1] for l3 in L3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61d10c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-63.2352, grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_in = u_out\n",
    "\n",
    "#T_2_tmp = torch.zeros((u_in.shape[0], u_in.shape[1], n2))\n",
    "\n",
    "T_2_tmp = [[None for l1 in L1] for l3 in L3]\n",
    "\n",
    "u_out = torch.zeros((u_in.shape[0], u_in.shape[1], n3))\n",
    "\n",
    "\n",
    "v = F.clone()\n",
    "for l2, slices in enumerate(irreps_edge_sh.slices()):\n",
    "    if l2 == 0:\n",
    "        for l3 in L3:\n",
    "            for l1 in L1:\n",
    "                T_2_tmp[l3][l1] = torch.einsum('abcijk,Nck->Nabij', T_3[l3][l1][l2], u_in[:, slices, :])\n",
    "    else:\n",
    "        for l3 in L3:\n",
    "            for l1 in L1:\n",
    "                T_2_tmp[l3][l1] += torch.einsum('abcijk,Nck->Nabij', T_3[l3][l1][l2], u_in[:, slices, :])\n",
    "    \n",
    "    \n",
    "for l3 in L3:    \n",
    "    for l1, slices in enumerate(irreps_edge_sh.slices()):\n",
    "        u_out[:, irreps_edge_sh.slices()[l3], :] += torch.einsum('Nabij,Nbj->Nai', T_2_tmp[l3][l1], v[:, slices, :])\n",
    "        \n",
    "        \n",
    "u_out.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fc2cce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(51.1680, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_out.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cdd85c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 9, 32])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6be116a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4300, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([ T_2[i][j].max() for i in [0, 1, 2] for j in [0, 1, 2] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4374b9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_2[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85f0d658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 9, 32])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3033c5b8",
   "metadata": {},
   "source": [
    "### Equivariance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebf78042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1x0ee+1x1oe+1x2ee\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0796)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([[1, 12, 34]]*10)\n",
    "\n",
    "edge_vec_plus = x / torch.linalg.norm(x, dim = 1).unsqueeze(-1)\n",
    "edge_vec_minus = -x / torch.linalg.norm(x, dim = 1).unsqueeze(-1)\n",
    "\n",
    "irreps_sh = o3.Irreps('1x0e + 1x1o + 1x2e') #o3.Irreps.spherical_harmonics(lmax=2)\n",
    "irreps_sh_r = o3.Irreps('1x1o')\n",
    "print(irreps_sh)\n",
    "\n",
    "\n",
    "alpha, beta, gamma = o3.rand_angles(100)\n",
    "\n",
    "rot_matrix = irreps_sh.D_from_angles(alpha[0], beta[0], gamma[0])\n",
    "rot_matrix_r = irreps_sh_r.D_from_angles(alpha[0], beta[0], gamma[0])\n",
    "\n",
    "\n",
    "sh_plus = o3.spherical_harmonics(irreps_sh, edge_vec_plus, normalize=True)\n",
    "\n",
    "sh_plus_of_rot =  o3.spherical_harmonics(irreps_sh, edge_vec_plus @ rot_matrix_r, \n",
    "                                        normalize=True)\n",
    "\n",
    "sh_plus_rot = o3.spherical_harmonics(irreps_sh, edge_vec_plus, \n",
    "                                        normalize=True) @ rot_matrix\n",
    "\n",
    "sh_plus_of_rot_rot = o3.spherical_harmonics(irreps_sh, edge_vec_plus @ rot_matrix_r, \n",
    "                                        normalize=True) @ rot_matrix.T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sh_minus = o3.spherical_harmonics(irreps_sh, edge_vec_minus, normalize=True)\n",
    "# normalize=True ensure that x is divided by |x| before computing the sh\n",
    "\n",
    "sh_plus.pow(2).mean()  # should be close to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1585ddf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2821, -0.4627,  0.1386, -0.0735,  0.1557, -0.2935, -0.2393, -0.0466,\n",
       "        -0.4776])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sh_plus_rot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f02e0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2821,  0.0135,  0.1626,  0.4606,  0.0286,  0.0101, -0.2107,  0.3426,\n",
      "         0.4850])\n",
      "tensor([ 0.2821,  0.0135,  0.1626,  0.4606,  0.0286,  0.0101, -0.2107,  0.3426,\n",
      "         0.4850])\n"
     ]
    }
   ],
   "source": [
    "print(sh_plus[0])\n",
    "\n",
    "print(sh_plus_of_rot_rot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5912c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_0(data, i):\n",
    "    \n",
    "\n",
    "    from nequip.data import AtomicDataDict, AtomicDataset\n",
    "    from nequip.nn.embedding import (\n",
    "        OneHotAtomEncoding,\n",
    "        SphericalHarmonicEdgeAttrs,\n",
    "        RadialBasisEdgeEncoding,\n",
    "    )\n",
    "    from e3nn import o3\n",
    "\n",
    "    \n",
    "    torch.manual_seed(32)\n",
    "    \n",
    "    L = 2\n",
    "\n",
    "    irreps_edge_sh = o3.Irreps.spherical_harmonics(L)\n",
    "\n",
    "    rbe = RadialBasisEdgeEncoding(basis_kwargs={'r_max': config['r_max'], \n",
    "                                                'num_basis': 8},\n",
    "                                  cutoff_kwargs={'r_max': config['r_max']},\n",
    "                                  out_field=AtomicDataDict.EDGE_EMBEDDING_KEY,\n",
    "                                  )\n",
    "\n",
    "    sh = SphericalHarmonicEdgeAttrs(irreps_edge_sh=irreps_edge_sh)\n",
    "\n",
    "    data = [rbe(data[i]) for i in range(len(data))]\n",
    "\n",
    "    data = [sh(data[i]) for i in range(len(data))]\n",
    "\n",
    "\n",
    "    #print(data[i]['edge_embedding'].shape, data[i]['edge_attrs'].shape)\n",
    "    \n",
    "    \n",
    "    from torch_runstats.scatter import scatter\n",
    "\n",
    "    N_rad = 8\n",
    "\n",
    "    N_spec_rank = 4\n",
    "    N_rad_rank = 4\n",
    "\n",
    "    Q = data[i]['edge_embedding']\n",
    "\n",
    "    A = torch.randn(L + 1, N_spec_rank, num_types**2)\n",
    "    B = torch.randn(L + 1, N_rad_rank, N_rad, N_spec_rank)\n",
    "\n",
    "\n",
    "    a = A[:, :, atom_types_embed].squeeze(-1)\n",
    "\n",
    "    b = torch.einsum('Lrnk,LkE,En->ELr', B, a, Q)\n",
    "\n",
    "    #print(data[i]['pos'][2])\n",
    "    Y = data[i]['edge_attrs']\n",
    "    #print(Y[2])\n",
    "    #print(data[i]['edge_attrs'][:, slices])\n",
    "    F = torch.concat([torch.einsum('Em,En->Emn', Y[:, slices],\n",
    "                                   b[:, l]) for l, slices in enumerate(irreps_edge_sh.slices())], dim = -2)\n",
    "    \n",
    "    print(F[2, :, 0])\n",
    "    #F = Y.unsqueeze(-1)\n",
    "    #print(a.shape, b.shape, F.shape)\n",
    "\n",
    "\n",
    "    species = data[i][AtomicDataDict.ATOM_TYPE_KEY].squeeze(-1)\n",
    "    edge_center = data[i][AtomicDataDict.EDGE_INDEX_KEY][0]\n",
    "    edge_neighbor = data[i][AtomicDataDict.EDGE_INDEX_KEY][1]\n",
    "\n",
    "    center_species = species[edge_center]\n",
    "    neighbor_species = species[edge_neighbor]\n",
    "\n",
    "    F = scatter(F, edge_center, dim=0, dim_size=len(species))\n",
    "\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65864472",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "from nequip.data import AtomicData, AtomicDataDict\n",
    "from torch.nn.functional import one_hot\n",
    "from e3nn.nn import FullyConnectedNet\n",
    "from allegro import with_edge_spin_length\n",
    "from allegro import _keys\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "data = [AtomicData.to_AtomicDataDict(dataset[i]) for i in range(10)]\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "data_new = [{key: torch.clone(el[key]) for key in el} for el in data]\n",
    "\n",
    "irreps_sh = o3.Irreps('1x0e + 1x1o + 1x2e') #o3.Irreps.spherical_harmonics(lmax=2)\n",
    "irreps_sh_r = o3.Irreps('1x1o')\n",
    "\n",
    "alpha, beta, gamma = o3.rand_angles(100)\n",
    "\n",
    "rot_matrix = irreps_sh.D_from_angles(alpha[0], beta[0], gamma[0])\n",
    "rot_matrix_r = irreps_sh_r.D_from_angles(alpha[0], beta[0], gamma[0])\n",
    "\n",
    "\n",
    "for i, el in enumerate(data_new):\n",
    "    data_new[i]['pos'] = data_new[i]['pos'] @ rot_matrix_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b4bc72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1360,  0.6565, -0.1762,  0.0175, -0.0983,  0.9870,  0.9094,  0.0264,\n",
      "         1.8374], grad_fn=<SelectBackward0>)\n",
      "tensor([-0.1360, -0.0921, -0.0290,  0.6730,  0.5290, -0.0228,  1.1326,  0.1664,\n",
      "        -1.8966], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "F = stage_0(data, 2)\n",
    "\n",
    "F_new = stage_0(data_new, 2)\n",
    "\n",
    "\n",
    "F_new_rot = torch.einsum('Njn,jk->Nkn', F_new, rot_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0a722e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(F, F_new_rot, atol=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42f6446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_1(F):\n",
    "    \n",
    "    torch.manual_seed(32)\n",
    "    \n",
    "    n1 = 4\n",
    "    n2 = 32\n",
    "    L1 = L2 = range(L + 1)\n",
    "\n",
    "    u_in = F.clone()\n",
    "    T_2 = [order_2_equivariant_tensor()(l, l, n1, n2) for l in L1]\n",
    "\n",
    "\n",
    "    u_out = torch.zeros((u_in.shape[0], u_in.shape[1], n2))\n",
    "    for i, slices in enumerate(irreps_edge_sh.slices()):\n",
    "        u_out[:, slices, :] = torch.einsum('ij,Nmi->Nmj', T_2[i], u_in[:, slices, :])\n",
    "        \n",
    "    return u_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18aefcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_out = stage_1(F)\n",
    "\n",
    "u_out_new = stage_1(F_new)\n",
    "\n",
    "u_out_new_rot = torch.einsum('Njn,jk->Nkn', u_out_new, rot_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d5f76f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(u_out, u_out_new_rot, atol=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "08375d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_2(u_in, F):\n",
    "    \n",
    "    torch.manual_seed(32)\n",
    "    \n",
    "    n1 = 4\n",
    "    n2 = 32\n",
    "    L1 = L2 = range(L + 1)\n",
    "    \n",
    "    n3 = 32\n",
    "    L3 = range(L + 1)\n",
    "\n",
    "    T_3 = [[[order_3_equvariant_tensor()(l3, l1, l2, n3, n1, n2) for l2 in L2] for l1 in L1] for l3 in L3]\n",
    "\n",
    "    #T_2_tmp = torch.zeros((u_in.shape[0], u_in.shape[1], n2))\n",
    "\n",
    "    T_2_tmp = [[None for l1 in L1] for l3 in L3]\n",
    "\n",
    "    u_out = torch.zeros((u_in.shape[0], u_in.shape[1], n3))\n",
    "\n",
    "\n",
    "    v = F.clone()\n",
    "    for l2, slices in enumerate(irreps_edge_sh.slices()):\n",
    "        if l2 == 0:\n",
    "            for l3 in L3:\n",
    "                for l1 in L1:\n",
    "                    T_2_tmp[l3][l1] = torch.einsum('abcijk,Nck->Nabij', T_3[l3][l1][l2], u_in[:, slices, :])\n",
    "        else:\n",
    "            for l3 in L3:\n",
    "                for l1 in L1:\n",
    "                    T_2_tmp[l3][l1] += torch.einsum('abcijk,Nck->Nabij', T_3[l3][l1][l2], u_in[:, slices, :])\n",
    "\n",
    "\n",
    "    for l3 in L3:    \n",
    "        for l1, slices in enumerate(irreps_edge_sh.slices()):\n",
    "            u_out[:, irreps_edge_sh.slices()[l3], :] += torch.einsum('Nabij,Nbj->Nai', T_2_tmp[l3][l1], v[:, slices, :])\n",
    "\n",
    "            \n",
    "    return u_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c3d8fe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_out_2 = stage_2(u_out, F)\n",
    "\n",
    "u_out_2_new = stage_2(u_out_new, F_new)\n",
    "\n",
    "u_out_2_new_rot = torch.einsum('Njn,jk->Nkn', u_out_2_new, rot_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e979ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(u_out_2, u_out_2_new_rot, atol=1e-03)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a7b4a2",
   "metadata": {},
   "source": [
    "### Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "73216f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tri_ineq(l1, l2, l3):\n",
    "    #print(max([l1, l2, l3]), min([l1 + l2, l2 + l3, l1 + l3]))\n",
    "    return max([l1, l2, l3]) <= min([l1 + l2, l2 + l3, l1 + l3])\n",
    "\n",
    "class order_3_equvariant_tensor():\n",
    "    \n",
    "    def __init__(self, l1, l2, l3, n1, n2, n3):\n",
    "        \"\"\" Basically transformation using\n",
    "            C (l1 l2 l3)\n",
    "              (m1 m2 m3)\"\"\"\n",
    "        self.l1 = l1; self.l2 = l2; self.l3 = l3\n",
    "        self.n1 = n1; self.n2 = n2; self.n3 = n3\n",
    "    \n",
    "        weight = torch.zeros([n1, n2, n3])\n",
    "        self.weight = nn.Parameter(nn.init.kaiming_uniform_(weight))\n",
    "        \n",
    "        \n",
    "        self.symbol_3j = wigner_3j(l1, l2, l3)\n",
    "        \n",
    "        \n",
    "    def __call__(self):\n",
    "        \n",
    "        if tri_ineq(l1, l2, l3):\n",
    "            return self.symbol_3j.view(*symbol_3j.shape, 1, 1, 1)*self.weight.view(1, 1, 1, *weight.shape)\n",
    "        else:\n",
    "            return torch.zeros((2*l1 + 1, 2*l2 + 1, 2*l3 + 1, n1, n2, n3))\n",
    "        \n",
    "        \n",
    "def contract_2_tensors(tensor_1, tensor_2):\n",
    "    weight_new = (tensor_1.weight.flatten(end_dim = -2) @ tensor_2.weight.flatten(start_dim = 1)).view(tensor_1.n1, tensor_1.n2, tensor_2.n2,\n",
    "                                                                                                       tensor_2.n3)\n",
    "    \n",
    "    \n",
    "    return weight_new, tensor_1.symbol_3j, tensor_2.symbol_3j\n",
    "    \n",
    "        \n",
    "class order_2_equivariant_tensor():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Makes the 2nd order tensor in a way that\n",
    "           each lm is multiplied by coefficient c, no angular momentum mixing\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, l1, l2, n1, n2):\n",
    "        self.l1 = l1; self.l2 = l2\n",
    "        self.n1 = n1; self.n2 = n2\n",
    "        \n",
    "        weight = torch.zeros([n1, n2])\n",
    "        if l1 == l2: # same as tri_ineq(l1, l2, 0)\n",
    "            W = nn.Parameter(nn.init.kaiming_uniform_(weight))\n",
    "            return W.view(*weight.shape)\n",
    "        else:\n",
    "            return torch.zeros((n1, n2))\n",
    "\n",
    "        \n",
    "class order_1_equivariant_tensor():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Makes the 1nd order tensor in a way that\n",
    "           each lm is multiplied by coefficient c, no angular momentum mixing\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, l1, n1):\n",
    "        self.l1 = l1;\n",
    "        self.n1 = n1;\n",
    "        \n",
    "        weight = torch.zeros([n1])\n",
    "        W = nn.Parameter(nn.init.uniform(weight))\n",
    "        return torch.ones((2*l1 + 1, 1))*W.view(1, *weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a5ab629f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8, 8])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_1 = order_3_equvariant_tensor(0, 0, 0, 4, 8, 32)\n",
    "tensor_2 = order_3_equvariant_tensor(0, 0, 0, 32, 8, 8)\n",
    "\n",
    "tensor_4th_order_weight = contract_2_tensors(tensor_1, tensor_2)[0]\n",
    "\n",
    "tensor_4th_order_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "af401041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "505"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([max([l1, l2, l3]) <= min([l1 + l2, l2 + l3, l1 + l3]) for l1 in range(10) for l2 in range(10) for l3 in range(10)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d851f285",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLiftingConvolution\u001b[39;00m(\u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, group, in_channels, out_channels, kernel_size, padding):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "class InterpolativeLiftingKernel(LiftingKernelBase):\n",
    "\n",
    "    def __init__(self, group, kernel_size, in_channels, out_channels):\n",
    "        super().__init__(group, kernel_size, in_channels, out_channels)\n",
    "\n",
    "        # Create and initialise a set of weights, we will interpolate these\n",
    "        # to create our transformed spatial kernels.\n",
    "        self.weight = torch.nn.Parameter(torch.zeros((\n",
    "            self.out_channels,\n",
    "            self.in_channels,\n",
    "            self.kernel_size,\n",
    "            self.kernel_size\n",
    "        ), device=self.group.identity.device))\n",
    "\n",
    "        # Initialize weights using kaiming uniform intialisation.\n",
    "        torch.nn.init.kaiming_uniform_(self.weight.data, a=math.sqrt(5))\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\" Sample convolution kernels for a given number of group elements\n",
    "\n",
    "        should return:\n",
    "        :return kernels: filter bank extending over all input channels,\n",
    "            containing kernels transformed for all output group elements.\n",
    "        \"\"\"\n",
    "        # First, we fold the output channel dim into the input channel dim;\n",
    "        # this allows us to transform the entire filter bank in one go using the\n",
    "        # torch grid_sample function.\n",
    "\n",
    "        ## YOUR CODE STARTS HERE ##\n",
    "        weight = self.weight.view(\n",
    "            self.out_channels * self.in_channels,\n",
    "            self.kernel_size,\n",
    "            self.kernel_size\n",
    "        )\n",
    "        ## AND ENDS HERE ##\n",
    "\n",
    "        # Sample the transformed kernels.\n",
    "        transformed_weight = []\n",
    "        for spatial_grid_idx in range(self.group.elements().numel()):\n",
    "            transformed_weight.append(\n",
    "                bilinear_interpolation(weight, self.transformed_grid_R2[:, spatial_grid_idx, :, :])\n",
    "            )\n",
    "        transformed_weight = torch.stack(transformed_weight)\n",
    "\n",
    "        # Separate input and output channels.\n",
    "        transformed_weight = transformed_weight.view(\n",
    "            self.group.elements().numel(),\n",
    "            self.out_channels,\n",
    "            self.in_channels,\n",
    "            self.kernel_size,\n",
    "            self.kernel_size\n",
    "        )\n",
    "\n",
    "        # Put out channel dimension before group dimension. We do this\n",
    "        # to be able to use pytorched Conv2D. Details below!\n",
    "        transformed_weight = transformed_weight.transpose(0, 1)\n",
    "\n",
    "        return transformed_weight\n",
    "\n",
    "\n",
    "class LiftingConvolution(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, group, in_channels, out_channels, kernel_size, padding):\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel = InterpolativeLiftingKernel(\n",
    "            group=group,\n",
    "            kernel_size=kernel_size,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels\n",
    "        )\n",
    "\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Perform lifting convolution\n",
    "\n",
    "        @param x: Input sample [batch_dim, in_channels, spatial_dim_1,\n",
    "            spatial_dim_2]\n",
    "        @return: Function on a homogeneous space of the group\n",
    "            [batch_dim, out_channels, num_group_elements, spatial_dim_1,\n",
    "            spatial_dim_2]\n",
    "        \"\"\"\n",
    "\n",
    "        # Obtain convolution kernels transformed under the group.\n",
    "\n",
    "        ## YOUR CODE STARTS HERE ##\n",
    "        conv_kernels = self.kernel.sample()\n",
    "        ## AND ENDS HERE ##\n",
    "\n",
    "        # Apply lifting convolution. Note that using a reshape we can fold the\n",
    "        # group dimension of the kernel into the output channel dimension. We\n",
    "        # treat every transformed kernel as an additional output channel. This\n",
    "        # way we can use pytorch's conv2d function!\n",
    "\n",
    "        # Question: Do you see why we (can) do this?\n",
    "\n",
    "        ## YOUR CODE STARTS HERE ##\n",
    "        x = torch.nn.functional.conv2d(\n",
    "            input=x,\n",
    "            weight=conv_kernels.reshape(\n",
    "                self.kernel.out_channels * self.kernel.group.elements().numel(),\n",
    "                self.kernel.in_channels,\n",
    "                self.kernel.kernel_size,\n",
    "                self.kernel.kernel_size\n",
    "            ),\n",
    "            padding=self.padding\n",
    "        )\n",
    "        ## AND ENDS HERE ##\n",
    "\n",
    "        # Reshape [batch_dim, in_channels * num_group_elements, spatial_dim_1,\n",
    "        # spatial_dim_2] into [batch_dim, in_channels, num_group_elements,\n",
    "        # spatial_dim_1, spatial_dim_2], separating channel and group\n",
    "        # dimensions.\n",
    "        x = x.view(\n",
    "            -1,\n",
    "            self.kernel.out_channels,\n",
    "            self.kernel.group.elements().numel(),\n",
    "            x.shape[-1],\n",
    "            x.shape[-2]\n",
    "        )\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fea33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            torch.nn.init.uniform_(self.bias, -bound, bound)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x, y = input.shape\n",
    "        if y != self.in_features:\n",
    "            print(f'Wrong Input Features. Please use tensor with {self.in_features} Input Features')\n",
    "            return 0\n",
    "        output = input.matmul(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias\n",
    "        ret = output\n",
    "        return ret\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_mkl",
   "language": "python",
   "name": "torch_mkl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
