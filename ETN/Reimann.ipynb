{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c08d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Manifold gradient module.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch as tn\n",
    "from torchtt._decomposition import mat_to_tt, to_tt, lr_orthogonal, round_tt, rl_orthogonal\n",
    "from torchtt import TT\n",
    "from torchtt.errors import *\n",
    "\n",
    "def _delta2cores(tt_cores, R, Sds, is_ttm = False, ortho = None):\n",
    "    \"\"\"\n",
    "    Convert the detla notation to TT.\n",
    "    Implements Algorithm 5.1 from \"AUTOMATIC DIFFERENTIATION FOR RIEMANNIAN OPTIMIZATION ON LOW-RANK MATRIX AND TENSOR-TRAIN MANIFOLDS\".\n",
    "\n",
    "    Args:\n",
    "        tt_cores (list[torch.tensor]): the TT cores.\n",
    "        R (list[int]): the rank of the tensor.\n",
    "        Sds (list[torch.tensor]): deltas.\n",
    "        # if the number of dimentions of tt format and approximated array match\n",
    "        is_ttm (bool, optional): is TT matrix or not. Defaults to False.\n",
    "        ortho (list[list[torch.tensor]], optional): the left and right orthogonal cores of tt_cores. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list[torch.tensor]: the resulting TT cores.\n",
    "    \"\"\"\n",
    "    \n",
    "    if ortho == None:\n",
    "        l_cores,_  = lr_orthogonal(tt_cores, R, is_ttm)\n",
    "        r_cores,_  = rl_orthogonal(tt_cores, R, is_ttm)\n",
    "    else:\n",
    "        l_cores = ortho[0]\n",
    "        r_cores = ortho[1]\n",
    "    \n",
    "    # first\n",
    "    cores_new = [tn.cat((Sds[0],l_cores[0]),2 if not is_ttm else 3)]\n",
    "    # 2...d-1\n",
    "    for k in range(1,len(tt_cores)-1):\n",
    "        up = tn.cat((r_cores[k],tn.zeros((r_cores[k].shape),dtype = l_cores[0].dtype, device = l_cores[0].device)),2 if not is_ttm else 3)\n",
    "        down = tn.cat((Sds[k],l_cores[k]),2 if not is_ttm else 3)\n",
    "        cores_new.append(tn.cat((up,down),0))\n",
    "    # last\n",
    "    cores_new.append(tn.cat((r_cores[-1],Sds[-1]),0))\n",
    "    \n",
    "    return cores_new\n",
    "\n",
    "def riemannian_gradient(x,func):\n",
    "    \"\"\"\n",
    "    Compute the Riemannian gradient using AD.\n",
    "\n",
    "    Args:\n",
    "        x (torchtt.TT): the point on the manifold where the gradient is computed.\n",
    "        func ([type]): function that has to be differentiated. The function takes as only argument `torchtt.TT` instances.\n",
    "\n",
    "    Returns:\n",
    "        torchtt.TT: the gradient projected on the tangent space of x.\n",
    "    \"\"\"\n",
    "\n",
    "    l_cores,_  = lr_orthogonal(x.cores, x.R, x.is_ttm)\n",
    "    r_cores,_  = rl_orthogonal(l_cores, x.R, x.is_ttm)\n",
    "    \n",
    "    is_ttm = x.is_ttm\n",
    "\n",
    "    \n",
    "    R = x.R\n",
    "    d = len(x.N)\n",
    "    \n",
    "    Rs = [ r_cores[0] ]\n",
    "    Rs += [ x.cores[i]*0 for i in range(1,d)]\n",
    "    \n",
    "    # AD part\n",
    "    for i in range(d):\n",
    "        Rs[i].requires_grad_(True)\n",
    "    Ghats = _delta2cores(x.cores, R, Rs, is_ttm = is_ttm,ortho = [l_cores,r_cores])\n",
    "    fval = func(TT(Ghats))\n",
    "    fval.backward() \n",
    "\n",
    "    # Sds = tape.gradient(fval, Rs)\n",
    "    Sds = [r.grad for r in Rs]\n",
    "    # print('Sds ',Sds)\n",
    "  \n",
    "    \n",
    "    # compute Sdeltas\n",
    "    for k in range(d-1):\n",
    "        D = tn.reshape(Sds[k],[-1,R[k+1]])\n",
    "        UL = tn.reshape(l_cores[k],[-1,R[k+1]])\n",
    "        D = D - UL @ (UL.T @ D)\n",
    "        Sds[k] = tn.reshape(D,l_cores[k].shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # print([tf.einsum('ijk,ijl->kl',l_cores[i],Sds[i]).numpy() for i in range(d-1)])\n",
    "    # delta to TT\n",
    "    grad_cores = _delta2cores(x.cores, R, Sds, is_ttm,ortho = [l_cores,r_cores])\n",
    "    return TT(grad_cores)\n",
    "        \n",
    "def riemannian_projection(Xspace,z):\n",
    "    \"\"\"\n",
    "    Project the tensor z onto the tangent space defined at xspace\n",
    "\n",
    "    Args:\n",
    "        Xspace (torchtt.TT): the target where the tensor should be projected.\n",
    "        z (torchtt.TT): the tensor that should be projected.\n",
    "\n",
    "    Raises:\n",
    "        IncompatibleTypes: Both must be of same type.\n",
    "\n",
    "    Returns:\n",
    "        torchtt.TT: the projection.\n",
    "    \"\"\"\n",
    "\n",
    "    if Xspace.is_ttm != z.is_ttm:\n",
    "        raise IncompatibleTypes('Both must be of same type.')\n",
    "       \n",
    "    is_ttm = Xspace.is_ttm\n",
    "     \n",
    "    l_cores,R  = lr_orthogonal(Xspace.cores, Xspace.R, Xspace.is_ttm)\n",
    "    r_cores,_  = rl_orthogonal(l_cores, R, Xspace.is_ttm)\n",
    "    \n",
    "    d = len(Xspace.N)\n",
    "\n",
    "    N = Xspace.N\n",
    "    \n",
    "    # Pleft = [tf.ones((1,1,1),dtype=Xspace.cores[0].dtype)]\n",
    "    Pleft = []\n",
    "    tmp = tn.ones((1,1),dtype=Xspace.cores[0].dtype, device = Xspace.cores[0].device)\n",
    "    for k in range(d-1):\n",
    "        if is_ttm:\n",
    "            tmp = tn.einsum('rs,rijR,sijS->RS',tmp,l_cores[k],z.cores[k]) # size rk x sk\n",
    "        else:\n",
    "            tmp = tn.einsum('rs,riR,siS->RS',tmp,l_cores[k],z.cores[k]) # size rk x sk\n",
    "        Pleft.append(tmp)\n",
    "        \n",
    "   \n",
    "    \n",
    "    Pright = []\n",
    "    tmp = tn.ones((1,1), dtype = Xspace.cores[0].dtype, device = Xspace.cores[0].device)\n",
    "    for k in range(d-1,0,-1):\n",
    "        if is_ttm:\n",
    "            tmp = tn.einsum('RS,rijR,sijS->rs',tmp,r_cores[k],z.cores[k]) # size rk x sk\n",
    "        else:\n",
    "            tmp = tn.einsum('RS,riR,siS->rs',tmp,r_cores[k],z.cores[k]) # size rk x sk\n",
    "        Pright.append(tmp)\n",
    "    Pright = Pright[::-1]\n",
    "    \n",
    "    \n",
    "    # compute elements of the tangent space\n",
    "    Sds = []\n",
    "    for k in range(d):\n",
    "  \n",
    "        if k==0:\n",
    "            L = tn.ones((1,1),dtype=Xspace.cores[0].dtype, device = Xspace.cores[0].device)\n",
    "        else:\n",
    "            L = Pleft[k-1]\n",
    "        if k==d-1:\n",
    "            if is_ttm:\n",
    "                Sds.append(tn.einsum('rs,sjiS->rjiS',L,z.cores[k]))   \n",
    "            else:\n",
    "                Sds.append(tn.einsum('rs,siS->riS',L,z.cores[k]))           \n",
    "        else:\n",
    "            R = Pright[k]\n",
    "            if is_ttm:\n",
    "                tmp1 = tn.einsum('rs,sijS->rijS',L,z.cores[k])\n",
    "                tmp2 = tn.einsum('rijR,RS->rijS',l_cores[k],tn.einsum('rs,rijR,sijS->RS',L,l_cores[k],z.cores[k]))\n",
    "                Sds.append(tn.einsum('rijS,RS->rijR',tmp1-tmp2,R))\n",
    "            else:\n",
    "                tmp1 = tn.einsum('rs,siS->riS',L,z.cores[k])\n",
    "                tmp2 = tn.einsum('riR,RS->riS',l_cores[k],tn.einsum('rs,riR,siS->RS',L,l_cores[k],z.cores[k]))\n",
    "                Sds.append(tn.einsum('riS,RS->riR',tmp1-tmp2,R))  \n",
    "        \n",
    "    # convert Sds to TT\n",
    "    grad_cores = _delta2cores(Xspace.cores, R, Sds, Xspace.is_ttm,ortho = [l_cores,r_cores])\n",
    "\n",
    "    return TT(grad_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff210cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tn\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torchtt as tntt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6234da10",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [10,11,12,13,14]\n",
    "Rt = [1,3,4,5,6,1]\n",
    "Rx = [1,6,6,6,6,1]\n",
    "target = tntt.randn(N,Rt).round(0)\n",
    "func = lambda x: 0.5*(x-target).norm(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c9b91",
   "metadata": {},
   "source": [
    "## Rank reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c583e",
   "metadata": {},
   "source": [
    "### Riemann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "639e470b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value  244833.65627034567\n",
      "Value  188978.66681149497\n",
      "Value  67445.76635865941\n",
      "Value  12268.368453471641\n",
      "Value  3199.2757887970793\n",
      "Value  152.5318579530042\n",
      "Value  0.012728872401420278\n",
      "Value  8.501112842678166e-15\n",
      "Value  1.2381654377069425e-23\n",
      "Value  6.727284734463914e-24\n",
      "Value  4.765436867627989e-24\n",
      "Value  4.3669895779394166e-24\n",
      "Value  6.045341163657253e-24\n",
      "Value  5.906954016533238e-24\n",
      "Value  4.837892670979434e-24\n",
      "Value  3.557827855807675e-24\n",
      "Value  3.512077168862015e-24\n",
      "Value  4.1102357213128275e-24\n",
      "Value  4.7845678809676356e-24\n",
      "Value  3.574841193177573e-24\n"
     ]
    }
   ],
   "source": [
    "x0 = tntt.randn(N,Rx)\n",
    "x =x0.clone()\n",
    "for i in range(20):\n",
    "    # compute riemannian gradient using AD    \n",
    "    gr = tntt.manifold.riemannian_gradient(x,func)\n",
    "    \n",
    "    #stepsize length\n",
    "    alpha = 1.0\n",
    "    \n",
    "    # update step\n",
    "    x = (x-alpha*gr).round(0,Rx)    \n",
    "    print('Value ' , func(x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcdfcb0",
   "metadata": {},
   "source": [
    "### Classical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e843870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(246470.4807, dtype=torch.float64)\n",
      "tensor(246132.1552, dtype=torch.float64)\n",
      "tensor(242583.9335, dtype=torch.float64)\n",
      "tensor(138157.3856, dtype=torch.float64)\n",
      "tensor(15593.6928, dtype=torch.float64)\n",
      "tensor(3666.3339, dtype=torch.float64)\n",
      "tensor(2173.1221, dtype=torch.float64)\n",
      "tensor(1188.6520, dtype=torch.float64)\n",
      "tensor(601.5732, dtype=torch.float64)\n",
      "tensor(295.4684, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y = x0.detach().clone()\n",
    "\n",
    "for i in range(1000):\n",
    "    tntt.grad.watch(y)\n",
    "    fval = func(y)\n",
    "    deriv = tntt.grad.grad(fval,y)    \n",
    "    alpha = 0.00001 # for stability\n",
    "    y = tntt.TT([y.cores[i].detach()-alpha*deriv[i] for i in range(len(deriv))])\n",
    "    if (i+1)%100 == 0: print(func(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ae6bf",
   "metadata": {},
   "source": [
    "## Tensor completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e2ab89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 3, 2, 1]\n",
      "Riemannian gradient descent\n",
      "\n",
      "Iteration  100 loss value 2.669319e+02 error 5.536844e-02 tensor norm 6.433788e+06\n",
      "Iteration  200 loss value 1.755638e+02 error 4.436142e-02 tensor norm 6.557243e+06\n",
      "Iteration  300 loss value 8.471260e+01 error 3.680423e-02 tensor norm 6.567885e+06\n",
      "Iteration  400 loss value 8.314812e+00 error 2.808722e-02 tensor norm 6.575853e+06\n",
      "Iteration  500 loss value 1.522958e+00 error 2.557929e-02 tensor norm 6.576202e+06\n",
      "Iteration  600 loss value 5.109728e-01 error 2.490470e-02 tensor norm 6.576524e+06\n",
      "Iteration  700 loss value 2.870835e-01 error 2.460458e-02 tensor norm 6.576592e+06\n",
      "Iteration  800 loss value 1.892848e-01 error 2.443801e-02 tensor norm 6.576565e+06\n",
      "Iteration  900 loss value 1.423169e-01 error 2.434341e-02 tensor norm 6.576523e+06\n",
      "Iteration 1000 loss value 1.186208e-01 error 2.428746e-02 tensor norm 6.576488e+06\n",
      "Iteration 1100 loss value 1.061103e-01 error 2.425224e-02 tensor norm 6.576462e+06\n",
      "Iteration 1200 loss value 9.920543e-02 error 2.422840e-02 tensor norm 6.576445e+06\n",
      "Iteration 1300 loss value 9.518781e-02 error 2.421104e-02 tensor norm 6.576432e+06\n",
      "Iteration 1400 loss value 9.269262e-02 error 2.419757e-02 tensor norm 6.576423e+06\n",
      "Iteration 1500 loss value 9.102476e-02 error 2.418660e-02 tensor norm 6.576417e+06\n",
      "Iteration 1600 loss value 8.982593e-02 error 2.417736e-02 tensor norm 6.576412e+06\n",
      "Iteration 1700 loss value 8.890807e-02 error 2.416941e-02 tensor norm 6.576408e+06\n",
      "Iteration 1800 loss value 8.816960e-02 error 2.416246e-02 tensor norm 6.576404e+06\n",
      "Iteration 1900 loss value 8.755347e-02 error 2.415633e-02 tensor norm 6.576401e+06\n",
      "Iteration 2000 loss value 8.702605e-02 error 2.415090e-02 tensor norm 6.576399e+06\n",
      "Iteration 2100 loss value 8.656644e-02 error 2.414607e-02 tensor norm 6.576397e+06\n",
      "Iteration 2200 loss value 8.616095e-02 error 2.414175e-02 tensor norm 6.576394e+06\n",
      "Iteration 2300 loss value 8.580013e-02 error 2.413791e-02 tensor norm 6.576392e+06\n",
      "Iteration 2400 loss value 8.547718e-02 error 2.413447e-02 tensor norm 6.576390e+06\n",
      "Iteration 2500 loss value 8.518699e-02 error 2.413140e-02 tensor norm 6.576388e+06\n",
      "Iteration 2600 loss value 8.492562e-02 error 2.412867e-02 tensor norm 6.576387e+06\n",
      "Iteration 2700 loss value 8.468989e-02 error 2.412624e-02 tensor norm 6.576385e+06\n",
      "Iteration 2800 loss value 8.447721e-02 error 2.412407e-02 tensor norm 6.576383e+06\n",
      "Iteration 2900 loss value 8.428538e-02 error 2.412215e-02 tensor norm 6.576382e+06\n",
      "Iteration 3000 loss value 8.411249e-02 error 2.412045e-02 tensor norm 6.576381e+06\n",
      "Iteration 3100 loss value 8.395687e-02 error 2.411895e-02 tensor norm 6.576379e+06\n",
      "Iteration 3200 loss value 8.381701e-02 error 2.411762e-02 tensor norm 6.576378e+06\n",
      "Iteration 3300 loss value 8.369153e-02 error 2.411645e-02 tensor norm 6.576377e+06\n",
      "Iteration 3400 loss value 8.357916e-02 error 2.411542e-02 tensor norm 6.576376e+06\n",
      "Iteration 3500 loss value 8.347874e-02 error 2.411452e-02 tensor norm 6.576375e+06\n",
      "Iteration 3600 loss value 8.338918e-02 error 2.411373e-02 tensor norm 6.576375e+06\n",
      "Iteration 3700 loss value 8.330945e-02 error 2.411305e-02 tensor norm 6.576374e+06\n",
      "Iteration 3800 loss value 8.323863e-02 error 2.411245e-02 tensor norm 6.576373e+06\n",
      "Iteration 3900 loss value 8.317583e-02 error 2.411193e-02 tensor norm 6.576372e+06\n",
      "Iteration 4000 loss value 8.312024e-02 error 2.411148e-02 tensor norm 6.576372e+06\n",
      "Iteration 4100 loss value 8.307111e-02 error 2.411110e-02 tensor norm 6.576371e+06\n",
      "Iteration 4200 loss value 8.302777e-02 error 2.411077e-02 tensor norm 6.576370e+06\n",
      "Iteration 4300 loss value 8.298958e-02 error 2.411048e-02 tensor norm 6.576370e+06\n",
      "Iteration 4400 loss value 8.295597e-02 error 2.411024e-02 tensor norm 6.576369e+06\n",
      "Iteration 4500 loss value 8.292642e-02 error 2.411003e-02 tensor norm 6.576369e+06\n",
      "Iteration 4600 loss value 8.290047e-02 error 2.410986e-02 tensor norm 6.576368e+06\n",
      "Iteration 4700 loss value 8.287769e-02 error 2.410972e-02 tensor norm 6.576368e+06\n",
      "Iteration 4800 loss value 8.285771e-02 error 2.410960e-02 tensor norm 6.576367e+06\n",
      "Iteration 4900 loss value 8.284018e-02 error 2.410950e-02 tensor norm 6.576366e+06\n",
      "Iteration 5000 loss value 8.282481e-02 error 2.410942e-02 tensor norm 6.576366e+06\n",
      "Iteration 5100 loss value 8.281133e-02 error 2.410935e-02 tensor norm 6.576365e+06\n",
      "Iteration 5200 loss value 8.279950e-02 error 2.410930e-02 tensor norm 6.576365e+06\n",
      "Iteration 5300 loss value 8.278912e-02 error 2.410926e-02 tensor norm 6.576364e+06\n",
      "Iteration 5400 loss value 8.277999e-02 error 2.410924e-02 tensor norm 6.576363e+06\n",
      "Iteration 5500 loss value 8.277196e-02 error 2.410922e-02 tensor norm 6.576363e+06\n",
      "Iteration 5600 loss value 8.276489e-02 error 2.410921e-02 tensor norm 6.576362e+06\n",
      "Iteration 5700 loss value 8.275865e-02 error 2.410920e-02 tensor norm 6.576361e+06\n",
      "Iteration 5800 loss value 8.275312e-02 error 2.410920e-02 tensor norm 6.576361e+06\n",
      "Iteration 5900 loss value 8.274823e-02 error 2.410921e-02 tensor norm 6.576360e+06\n",
      "Iteration 6000 loss value 8.274387e-02 error 2.410922e-02 tensor norm 6.576359e+06\n",
      "Iteration 6100 loss value 8.273999e-02 error 2.410924e-02 tensor norm 6.576358e+06\n",
      "Iteration 6200 loss value 8.273651e-02 error 2.410926e-02 tensor norm 6.576358e+06\n",
      "Iteration 6300 loss value 8.273338e-02 error 2.410928e-02 tensor norm 6.576357e+06\n",
      "Iteration 6400 loss value 8.273056e-02 error 2.410930e-02 tensor norm 6.576356e+06\n",
      "Iteration 6500 loss value 8.272800e-02 error 2.410933e-02 tensor norm 6.576355e+06\n",
      "Iteration 6600 loss value 8.272567e-02 error 2.410935e-02 tensor norm 6.576355e+06\n",
      "Iteration 6700 loss value 8.272353e-02 error 2.410938e-02 tensor norm 6.576354e+06\n",
      "Iteration 6800 loss value 8.272156e-02 error 2.410941e-02 tensor norm 6.576353e+06\n",
      "Iteration 6900 loss value 8.271974e-02 error 2.410945e-02 tensor norm 6.576352e+06\n",
      "Iteration 7000 loss value 8.271804e-02 error 2.410948e-02 tensor norm 6.576351e+06\n",
      "Iteration 7100 loss value 8.271645e-02 error 2.410951e-02 tensor norm 6.576350e+06\n",
      "Iteration 7200 loss value 8.271495e-02 error 2.410955e-02 tensor norm 6.576349e+06\n",
      "Iteration 7300 loss value 8.271354e-02 error 2.410958e-02 tensor norm 6.576348e+06\n",
      "Iteration 7400 loss value 8.271219e-02 error 2.410962e-02 tensor norm 6.576348e+06\n",
      "Iteration 7500 loss value 8.271090e-02 error 2.410965e-02 tensor norm 6.576347e+06\n",
      "Iteration 7600 loss value 8.270966e-02 error 2.410969e-02 tensor norm 6.576346e+06\n",
      "Iteration 7700 loss value 8.270846e-02 error 2.410973e-02 tensor norm 6.576345e+06\n",
      "Iteration 7800 loss value 8.270730e-02 error 2.410976e-02 tensor norm 6.576344e+06\n",
      "Iteration 7900 loss value 8.270617e-02 error 2.410980e-02 tensor norm 6.576343e+06\n",
      "Iteration 8000 loss value 8.270507e-02 error 2.410984e-02 tensor norm 6.576342e+06\n",
      "Iteration 8100 loss value 8.270399e-02 error 2.410988e-02 tensor norm 6.576341e+06\n",
      "Iteration 8200 loss value 8.270293e-02 error 2.410992e-02 tensor norm 6.576340e+06\n",
      "Iteration 8300 loss value 8.270188e-02 error 2.410995e-02 tensor norm 6.576339e+06\n",
      "Iteration 8400 loss value 8.270085e-02 error 2.410999e-02 tensor norm 6.576338e+06\n",
      "Iteration 8500 loss value 8.269984e-02 error 2.411003e-02 tensor norm 6.576337e+06\n",
      "Iteration 8600 loss value 8.269883e-02 error 2.411007e-02 tensor norm 6.576336e+06\n",
      "Iteration 8700 loss value 8.269783e-02 error 2.411011e-02 tensor norm 6.576335e+06\n",
      "Iteration 8800 loss value 8.269684e-02 error 2.411015e-02 tensor norm 6.576334e+06\n",
      "Iteration 8900 loss value 8.269585e-02 error 2.411018e-02 tensor norm 6.576333e+06\n",
      "Iteration 9000 loss value 8.269486e-02 error 2.411022e-02 tensor norm 6.576332e+06\n",
      "Iteration 9100 loss value 8.269388e-02 error 2.411026e-02 tensor norm 6.576331e+06\n",
      "Iteration 9200 loss value 8.269291e-02 error 2.411030e-02 tensor norm 6.576330e+06\n",
      "Iteration 9300 loss value 8.269193e-02 error 2.411034e-02 tensor norm 6.576329e+06\n",
      "Iteration 9400 loss value 8.269096e-02 error 2.411038e-02 tensor norm 6.576328e+06\n",
      "Iteration 9500 loss value 8.268999e-02 error 2.411042e-02 tensor norm 6.576327e+06\n",
      "Iteration 9600 loss value 8.268901e-02 error 2.411046e-02 tensor norm 6.576326e+06\n",
      "Iteration 9700 loss value 8.268804e-02 error 2.411050e-02 tensor norm 6.576325e+06\n",
      "Iteration 9800 loss value 8.268707e-02 error 2.411054e-02 tensor norm 6.576324e+06\n",
      "Iteration 9900 loss value 8.268610e-02 error 2.411058e-02 tensor norm 6.576323e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10000 loss value 8.268513e-02 error 2.411061e-02 tensor norm 6.576322e+06\n",
      "\n",
      "Time elapsed 0:03:33.777151\n",
      "Number of observations 15000, tensor shape [25, 25, 25, 25], percentage of entries observed 3.8400\n",
      "Number of unknowns 1000, number of observations 15000, DoF/observations 0.066667\n",
      "Rank after rounding TT with sizes and ranks:\n",
      "N = [25, 25, 25, 25]\n",
      "R = [1, 4, 4, 4, 1]\n",
      "\n",
      "Device: cpu, dtype: torch.float64\n",
      "#entries 1000 compression 0.00256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 25\n",
    "target = tntt.randn([N]*4,[1,2,3,3,1])\n",
    "Xs = tntt.meshgrid([tn.linspace(0,1,N, dtype = tn.float64)]*4)\n",
    "target = Xs[0]+1+Xs[1]+Xs[2]+Xs[3]+Xs[0]*Xs[1]+Xs[1]*Xs[2]+tntt.TT(tn.sin(Xs[0].full()))\n",
    "target = target.round(1e-10)\n",
    "print(target.R)\n",
    "\n",
    "M = 15000 # number of observations \n",
    "indices = tn.randint(0,N,(M,4))\n",
    "\n",
    "# observations are considered to be noisy\n",
    "sigma_noise = 0.00001\n",
    "obs = tn.normal(target.apply_mask(indices), sigma_noise)\n",
    "\n",
    "# define the loss function\n",
    "loss = lambda x: (x.apply_mask(indices)-obs).norm()**2\n",
    "\n",
    "#%% Manifold learning\n",
    "print('Riemannian gradient descent\\n')\n",
    "# starting point\n",
    "x = tntt.randn([N]*4,[1,4,4,4,1])\n",
    "\n",
    "tme = datetime.datetime.now()\n",
    "# iterations\n",
    "for i in range(10000):\n",
    "    # manifold gradient \n",
    "    gr = tntt.manifold.riemannian_gradient(x,loss)\n",
    "\n",
    "    step_size = 1.0\n",
    "    R = x.R\n",
    "    # step update\n",
    "    x = (x - step_size * gr).round(0,R)\n",
    "\n",
    "    # compute loss value\n",
    "    if (i+1)%100 == 0:\n",
    "        loss_value = loss(x)\n",
    "        print('Iteration %4d loss value %e error %e tensor norm %e'%(i+1,loss_value.numpy(),(x-target).norm()/target.norm(), x.norm()**2))\n",
    "\n",
    "tme = datetime.datetime.now() - tme\n",
    "print('')\n",
    "print('Time elapsed',tme)\n",
    "print('Number of observations %d, tensor shape %s, percentage of entries observed %6.4f'%(M,str(x.N),100*M/np.prod(x.N)))\n",
    "print('Number of unknowns %d, number of observations %d, DoF/observations %.6f'%(tntt.numel(x),M,tntt.numel(x)/M))\n",
    "\n",
    "print('Rank after rounding',x.round(1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1455ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e3nn.o3 import wigner_3j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "463f7701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.1667, 0.0000],\n",
       "         [0.0000, 0.0000, 0.1667]],\n",
       "\n",
       "        [[0.1667, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.1667]],\n",
       "\n",
       "        [[0.1667, 0.0000, 0.0000],\n",
       "         [0.0000, 0.1667, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(wigner_3j(1, 1, 1) @ wigner_3j(1, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f375a9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (7) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwigner_3j\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwigner_3j\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (7) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "wigner_3j(1, 2, 3) @ wigner_3j(1, 2, 3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd358278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_mkl",
   "language": "python",
   "name": "torch_mkl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
