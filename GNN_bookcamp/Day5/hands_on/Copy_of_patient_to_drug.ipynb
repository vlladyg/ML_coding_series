{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Patient-to-Drug Link Prediction: A Walkthrough**\n",
        "This notebook demonstrates how to perform a **link prediction task** using a **biological knowledge graph**. The goal is to predict whether a specific drug is suitable for a given patient, based on their features and the relationships in the graph. Download the data [here](https://drive.google.com/drive/folders/12fFDPScwcvYjykKm_olKKF_-JXN6d__P?usp=drive_link)\n",
        "\n",
        "---\n",
        "\n",
        "## **Objective**\n",
        "- Learn how to set up a **biological knowledge graph** where nodes represent entities (patients, drugs, etc.) and edges represent relationships (e.g., drug efficacy).\n",
        "- Build a **Graph Neural Network (GNN)** for link prediction.\n",
        "- Train the model to predict the presence of edges between specific pairs of nodes (patient-drug pairs).\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Introduction to Link Prediction**\n",
        "### **What is Link Prediction?**\n",
        "Link prediction is the task of predicting whether a link (edge) exists between two nodes in a graph. It is widely used in:\n",
        "- **Biology**: Predicting interactions (e.g., protein-protein interactions).\n",
        "- **Recommender Systems**: Suggesting products or friends.\n",
        "- **Drug Discovery**: Identifying new drug-target interactions.\n",
        "\n",
        "### **Biological Knowledge Graph**\n",
        "In this case:\n",
        "- **Nodes**: Represent patients and drugs.\n",
        "- **Edges**: Represent known relationships, such as whether a drug is prescribed for a patient.\n",
        "- **Features**:\n",
        "  - **Patients**: Demographic, clinical, or genetic information.\n",
        "  - **Drugs**: Molecular properties or existing indications.\n",
        "\n",
        "### **Goal**\n",
        "Predict whether a drug (node) is suitable for a patient (node) by learning from the structure of the biological knowledge graph.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "0ZYFLinW_M4o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxgd_Ubm-uZu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65314e5e-586b-470d-cf9f-fd5a3f2d233d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: psycopg2 in /usr/local/lib/python3.10/dist-packages (2.9.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 1. Install required libraries\n",
        "!pip install torch torch-geometric pandas psycopg2\n",
        "\n",
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, SAGEConv\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from torch_geometric.data import Data\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **2. Data Setup**\n",
        "In this section, we will:\n",
        "1. Construct a **biological knowledge graph** with nodes for patients and drugs.\n",
        "2. Split the edges into training, validation, and test sets for link prediction.\n",
        "\n",
        "---\n",
        "\n",
        "### **Graph Construction**\n",
        "We create a synthetic graph where:\n",
        "- **Nodes**:\n",
        "  - Patients: Represented by patient-specific features (e.g., age, symptoms).\n",
        "  - Drugs: Represented by drug-specific features (e.g., molecular descriptors).\n",
        "- **Edges**: Represent known relationships between patients and drugs.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "BIgbt0Eo_Q8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load patient and prescription data\n",
        "patients = pd.read_csv(\"/content/drive/Shareddrives/Bootcamp/Bootcamp 8 - GNN/Module 5/csvs/patients.csv\")\n",
        "prescriptions = pd.read_csv(\"/content/drive/Shareddrives/Bootcamp/Bootcamp 8 - GNN/Module 5/csvs/prescriptions.csv\")\n",
        "print(patients.head())\n",
        "print(prescriptions.head())\n",
        "\n",
        "# Step 1: Preprocess Patient Features\n",
        "# Calculate age and select relevant columns\n",
        "patients['age'] = 2024 - pd.to_datetime(patients['anchor_year']).dt.year\n",
        "patient_features = patients[['subject_id', 'gender', 'age']].copy()\n",
        "\n",
        "# Encode categorical features (e.g., gender)\n",
        "patient_features['gender'] = patient_features['gender'].map({'M': 0, 'F': 1})\n",
        "patient_features_tensor = torch.tensor(patient_features[['age', 'gender']].values, dtype=torch.float)\n",
        "\n",
        "# Step 2: Preprocess Drug Features\n",
        "# Extract and one-hot encode drug form\n",
        "drug_features = prescriptions[['drug', 'form_rx']].drop_duplicates()\n",
        "drug_features_encoded = pd.get_dummies(drug_features, columns=['form_rx'])\n",
        "drug_features_tensor = torch.tensor(drug_features_encoded.iloc[:, 1:].values, dtype=torch.float)\n",
        "\n",
        "# Step 3: Align Dimensions\n",
        "# Determine the maximum feature length between patients and drugs\n",
        "max_features = max(patient_features_tensor.size(1), drug_features_tensor.size(1))\n",
        "\n",
        "# Pad the smaller tensor to match dimensions\n",
        "if patient_features_tensor.size(1) < max_features:\n",
        "    padding = max_features - patient_features_tensor.size(1)\n",
        "    patient_features_tensor = torch.nn.functional.pad(patient_features_tensor, (0, padding))\n",
        "elif drug_features_tensor.size(1) < max_features:\n",
        "    padding = max_features - drug_features_tensor.size(1)\n",
        "    drug_features_tensor = torch.nn.functional.pad(drug_features_tensor, (0, padding))\n",
        "\n",
        "# Step 4: Create Edges\n",
        "edges = prescriptions[['subject_id', 'drug']].drop_duplicates()\n",
        "\n",
        "# Map subject_id to zero-based indices\n",
        "edges['subject_id'] = edges['subject_id'].map(lambda x: x - 1)\n",
        "\n",
        "# Map drugs to indices in the drug_features DataFrame\n",
        "drug_index_map = {drug: idx for idx, drug in enumerate(drug_features['drug'])}\n",
        "edges['drug'] = edges['drug'].map(drug_index_map)\n",
        "\n",
        "# Convert edges to PyTorch Geometric format\n",
        "edge_index = torch.tensor(edges.dropna().values.T, dtype=torch.long)\n",
        "\n",
        "x = torch.cat([patient_features_tensor, drug_features_tensor], dim=0)\n",
        "\n",
        "# Total number of nodes\n",
        "num_patients = patient_features_tensor.size(0)\n",
        "num_drugs = drug_features_tensor.size(0)\n",
        "num_nodes = num_patients + num_drugs\n",
        "\n",
        "# Step 5: Create Edge Index\n",
        "# Prepare edges by mapping `subject_id` and `drug` to their respective node indices\n",
        "edges = prescriptions[['subject_id', 'drug']].drop_duplicates()\n",
        "\n",
        "# Map `subject_id` to zero-based indices\n",
        "patient_ids = patients['subject_id'].unique()\n",
        "patient_index_map = {patient_id: idx for idx, patient_id in enumerate(patient_ids)}\n",
        "edges['subject_id'] = edges['subject_id'].map(patient_index_map)\n",
        "\n",
        "\n",
        "# Map `drug` to indices, starting after the patients\n",
        "unique_drugs = edges['drug'].unique()\n",
        "drug_index_map = {drug: idx + num_patients for idx, drug in enumerate(unique_drugs)}\n",
        "edges['drug'] = edges['drug'].map(drug_index_map)\n",
        "\n",
        "\n",
        "# Drop NaN values in the mapping\n",
        "edges = edges.dropna()\n",
        "\n",
        "# Convert edges to PyTorch Geometric format\n",
        "edge_index = torch.tensor(edges.values.T, dtype=torch.long)\n",
        "\n",
        "# Step 6: Validate Edge Indices\n",
        "# Ensure all indices in `edge_index` are within the range of the node feature matrix\n",
        "if edge_index.max() >= num_nodes:\n",
        "    raise ValueError(\n",
        "        f\"Edge indices out of bounds: max index {edge_index.max()} exceeds total nodes {num_nodes}.\"\n",
        "    )\n",
        "\n",
        "# Step 7: Create the Graph\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "\n",
        "# Display Graph Information\n",
        "print(\"Graph created successfully!\")\n",
        "print(f\"Number of nodes: {data.num_nodes}\")\n",
        "print(f\"Number of edges: {data.num_edges}\")\n",
        "print(f\"Feature dimension: {data.x.size(1)}\")\n"
      ],
      "metadata": {
        "id": "Xb11nYyHAwc8",
        "outputId": "6a7fa2b8-6172-4e1d-dd92-566c438451a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   subject_id gender  anchor_age  anchor_year anchor_year_group         dod\n",
            "0    10014729      F          21         2125       2011 - 2013         NaN\n",
            "1    10003400      F          72         2134       2011 - 2013  2137-09-02\n",
            "2    10002428      F          80         2155       2011 - 2013         NaN\n",
            "3    10032725      F          38         2143       2011 - 2013  2143-03-30\n",
            "4    10027445      F          48         2142       2011 - 2013  2146-02-09\n",
            "   subject_id   hadm_id  pharmacy_id poe_id  poe_seq order_provider_id  \\\n",
            "0    10027602  28166872     27168639    NaN      NaN               NaN   \n",
            "1    10027602  28166872     40720238    NaN      NaN               NaN   \n",
            "2    10027602  28166872     62845687    NaN      NaN               NaN   \n",
            "3    10027602  28166872     24340150    NaN      NaN               NaN   \n",
            "4    10027602  28166872     14435820    NaN      NaN               NaN   \n",
            "\n",
            "             starttime stoptime drug_type              drug  ...  gsn ndc  \\\n",
            "0  2201-10-30 12:00:00      NaN      MAIN  Fentanyl Citrate  ...  NaN NaN   \n",
            "1  2201-10-30 12:00:00      NaN      MAIN  Fentanyl Citrate  ...  NaN NaN   \n",
            "2  2201-10-31 12:00:00      NaN      MAIN         Lorazepam  ...  NaN NaN   \n",
            "3  2201-10-30 12:00:00      NaN      MAIN         Midazolam  ...  NaN NaN   \n",
            "4  2201-10-30 12:00:00      NaN      MAIN         Midazolam  ...  NaN NaN   \n",
            "\n",
            "   prod_strength form_rx dose_val_rx dose_unit_rx form_val_disp  \\\n",
            "0            NaN     NaN         NaN          NaN           NaN   \n",
            "1            NaN     NaN         NaN          NaN           NaN   \n",
            "2            NaN     NaN         NaN          NaN           NaN   \n",
            "3            NaN     NaN         NaN          NaN           NaN   \n",
            "4            NaN     NaN         NaN          NaN           NaN   \n",
            "\n",
            "  form_unit_disp doses_per_24_hrs  route  \n",
            "0            NaN              NaN    NaN  \n",
            "1            NaN              NaN    NaN  \n",
            "2            NaN              NaN    NaN  \n",
            "3            NaN              NaN    NaN  \n",
            "4            NaN              NaN    NaN  \n",
            "\n",
            "[5 rows x 21 columns]\n",
            "Graph created successfully!\n",
            "Number of nodes: 731\n",
            "Number of edges: 5571\n",
            "Feature dimension: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Edge Splitting for Link Prediction**\n",
        "For link prediction, we split the edges into:\n",
        "- **Training edges**: Used to train the model.\n",
        "- **Validation edges**: Used to tune hyperparameters.\n",
        "- **Test edges**: Used to evaluate the model's performance.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "6kn-KfBM_Tdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split edges into train, validation, and test sets\n",
        "def split_edges(edge_index, val_ratio=0.1, test_ratio=0.1):\n",
        "    # Get all edges\n",
        "    edges = edge_index.t().numpy()  # Shape: (num_edges, 2)\n",
        "    num_edges = edges.shape[0]\n",
        "\n",
        "    # Shuffle edges\n",
        "    indices = np.random.permutation(num_edges)\n",
        "    edges = edges[indices]\n",
        "\n",
        "    # Calculate split sizes\n",
        "    num_val = int(num_edges * val_ratio)\n",
        "    num_test = int(num_edges * test_ratio)\n",
        "    num_train = num_edges - num_val - num_test\n",
        "\n",
        "    # Split edges\n",
        "    train_edges = edges[:num_train]\n",
        "    val_edges = edges[num_train:num_train + num_val]\n",
        "    test_edges = edges[num_train + num_val:]\n",
        "\n",
        "    # Convert back to PyTorch tensors\n",
        "    train_edges = torch.tensor(train_edges, dtype=torch.long).t()\n",
        "    val_edges = torch.tensor(val_edges, dtype=torch.long).t()\n",
        "    test_edges = torch.tensor(test_edges, dtype=torch.long).t()\n",
        "\n",
        "    return train_edges, val_edges, test_edges\n",
        "\n",
        "val_ratio = 0.1\n",
        "test_ratio = 0.1\n",
        "train_edges, val_edges, test_edges = split_edges(data.edge_index, val_ratio, test_ratio)\n",
        "\n",
        "\n",
        "print(\"Train/Test split complete.\")\n",
        "print(f\"Train edges: {train_edges.size(1)}\")\n",
        "print(f\"Validation edges: {val_edges.size(1)}\")\n",
        "print(f\"Test edges: {test_edges.size(1)}\")\n"
      ],
      "metadata": {
        "id": "P67lB0O5_XMq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d607b9c8-0963-44e3-d520-4c9a2688dce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Test split complete.\n",
            "Train edges: 4457\n",
            "Validation edges: 557\n",
            "Test edges: 557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **3. Graph Neural Network for Link Prediction**\n",
        "We will use a GNN-based encoder to generate node embeddings and a decoder to predict the existence of edges.\n",
        "\n",
        "### **Model Architecture**\n",
        "1. **Encoder**: A GNN (e.g., GCN or GraphSAGE) to learn node embeddings.\n",
        "2. **Decoder**: A dot product layer to predict edge existence.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ETcKnhjf_Y-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinkPredictor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinkPredictor, self).__init__()\n",
        "\n",
        "    def forward(self, z, edge_index):\n",
        "        # Extract embeddings for the source and target nodes of each edge\n",
        "        row, col = edge_index\n",
        "        # Compute the dot product between the embeddings\n",
        "        scores = (z[row] * z[col]).sum(dim=-1)\n",
        "        return scores\n",
        "# 3. Define GNN Encoder\n",
        "class GNNEncoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(GNNEncoder, self).__init__()\n",
        "        self.conv1 = SAGEConv(input_dim, hidden_dim)\n",
        "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Define the encoder and link predictor\n",
        "input_dim = data.x.size(1)\n",
        "hidden_dim = 64\n",
        "\n",
        "encoder = GNNEncoder(input_dim, hidden_dim)\n",
        "link_predictor = LinkPredictor()\n",
        "\n",
        "print(\"Model initialized.\")\n"
      ],
      "metadata": {
        "id": "G2zcCe-z_agT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7820ae29-4657-4203-d3bf-11fe7cfb21d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **4. Training and Evaluation**\n",
        "### **Training**\n",
        "We optimize the model using a binary cross-entropy loss for the link prediction task.\n",
        "\n",
        "### **Evaluation**\n",
        "Evaluate the model on the validation and test sets using metrics such as accuracy or ROC-AUC.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "S_7wsVCz_cH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Training and evaluation\n",
        "from torch_geometric.utils import negative_sampling\n",
        "# Function to sample negative edges\n",
        "def sample_negative_edges(edge_index, num_nodes, num_samples):\n",
        "    neg_edge_index = negative_sampling(\n",
        "        edge_index=edge_index,\n",
        "        num_nodes=num_nodes,\n",
        "        num_neg_samples=num_samples\n",
        "    )\n",
        "    return neg_edge_index\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(encoder.parameters()) + list(link_predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "# Training Function\n",
        "def train():\n",
        "    encoder.train()\n",
        "    link_predictor.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Encode node embeddings\n",
        "    z = encoder(data.x, train_edges)\n",
        "\n",
        "    # Sample negative edges\n",
        "    num_train_edges = train_edges.size(1)\n",
        "    neg_edges = sample_negative_edges(train_edges, data.x.size(0), num_train_edges)\n",
        "\n",
        "    # Predict edge probabilities\n",
        "    pos_pred = link_predictor(z, train_edges)\n",
        "    neg_pred = link_predictor(z, neg_edges)\n",
        "\n",
        "    # Create labels for positive and negative edges\n",
        "    pos_label = torch.ones(pos_pred.size(0), dtype=torch.float)\n",
        "    neg_label = torch.zeros(neg_pred.size(0), dtype=torch.float)\n",
        "\n",
        "    # Compute binary cross-entropy loss\n",
        "    loss = F.binary_cross_entropy_with_logits(\n",
        "        torch.cat([pos_pred, neg_pred]), torch.cat([pos_label, neg_label])\n",
        "    )\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Evaluation Function\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def evaluate(edge_index, neg_edge_index):\n",
        "    encoder.eval()\n",
        "    link_predictor.eval()\n",
        "\n",
        "    # Encode node embeddings\n",
        "    z = encoder(data.x, edge_index)\n",
        "\n",
        "    # Predict edge probabilities\n",
        "    pos_pred = link_predictor(z, edge_index)\n",
        "    neg_pred = link_predictor(z, neg_edge_index)\n",
        "\n",
        "    # Create labels\n",
        "    pos_label = torch.ones(pos_pred.size(0), dtype=torch.float)\n",
        "    neg_label = torch.zeros(neg_pred.size(0), dtype=torch.float)\n",
        "\n",
        "    # Compute ROC-AUC\n",
        "    pred = torch.cat([pos_pred, neg_pred]).sigmoid().cpu().detach().numpy()\n",
        "    label = torch.cat([pos_label, neg_label]).cpu().detach().numpy()\n",
        "    auc = roc_auc_score(label, pred)\n",
        "    return auc\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(50):\n",
        "    loss = train()\n",
        "\n",
        "    # Validation AUC\n",
        "    num_val_edges = val_edges.size(1)\n",
        "    val_neg_edges = sample_negative_edges(val_edges, data.x.size(0), num_val_edges)\n",
        "    val_auc = evaluate(val_edges, val_neg_edges)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}, Validation AUC: {val_auc:.4f}\")\n",
        "\n",
        "# Test AUC\n",
        "num_test_edges = test_edges.size(1)\n",
        "test_neg_edges = sample_negative_edges(test_edges, data.x.size(0), num_test_edges)\n",
        "test_auc = evaluate(test_edges, test_neg_edges)\n",
        "print(f\"Test ROC-AUC: {test_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "khGunrU6_dS9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4570e669-a7d5-48ef-b97f-a4018fd0ea23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2062.0457, Validation AUC: 0.6517\n",
            "Epoch 2, Loss: 717.3783, Validation AUC: 0.6499\n",
            "Epoch 3, Loss: 423.7495, Validation AUC: 0.6391\n",
            "Epoch 4, Loss: 696.3655, Validation AUC: 0.6364\n",
            "Epoch 5, Loss: 818.4996, Validation AUC: 0.6697\n",
            "Epoch 6, Loss: 586.0899, Validation AUC: 0.2123\n",
            "Epoch 7, Loss: 283.5254, Validation AUC: 0.9120\n",
            "Epoch 8, Loss: 177.8408, Validation AUC: 0.9084\n",
            "Epoch 9, Loss: 195.6187, Validation AUC: 0.8860\n",
            "Epoch 10, Loss: 245.7602, Validation AUC: 0.8824\n",
            "Epoch 11, Loss: 243.9477, Validation AUC: 0.0380\n",
            "Epoch 12, Loss: 232.1940, Validation AUC: 0.8896\n",
            "Epoch 13, Loss: 198.2712, Validation AUC: 0.8923\n",
            "Epoch 14, Loss: 188.8858, Validation AUC: 0.8815\n",
            "Epoch 15, Loss: 160.1725, Validation AUC: 0.8950\n",
            "Epoch 16, Loss: 125.3144, Validation AUC: 0.0467\n",
            "Epoch 17, Loss: 97.2902, Validation AUC: 0.7327\n",
            "Epoch 18, Loss: 75.6229, Validation AUC: 0.8311\n",
            "Epoch 19, Loss: 62.5485, Validation AUC: 0.0410\n",
            "Epoch 20, Loss: 57.1136, Validation AUC: 0.8959\n",
            "Epoch 21, Loss: 60.3896, Validation AUC: 0.9004\n",
            "Epoch 22, Loss: 66.0363, Validation AUC: 0.8959\n",
            "Epoch 23, Loss: 64.5916, Validation AUC: 0.8507\n",
            "Epoch 24, Loss: 53.8659, Validation AUC: 0.0505\n",
            "Epoch 25, Loss: 55.6016, Validation AUC: 0.0382\n",
            "Epoch 26, Loss: 42.4163, Validation AUC: 0.8241\n",
            "Epoch 27, Loss: 28.6317, Validation AUC: 0.8941\n",
            "Epoch 28, Loss: 28.2023, Validation AUC: 0.9075\n",
            "Epoch 29, Loss: 27.7944, Validation AUC: 0.8950\n",
            "Epoch 30, Loss: 25.3309, Validation AUC: 0.8878\n",
            "Epoch 31, Loss: 21.3254, Validation AUC: 0.8555\n",
            "Epoch 32, Loss: 18.5750, Validation AUC: 0.0486\n",
            "Epoch 33, Loss: 21.6250, Validation AUC: 0.8398\n",
            "Epoch 34, Loss: 18.2976, Validation AUC: 0.8228\n",
            "Epoch 35, Loss: 19.3362, Validation AUC: 0.9057\n",
            "Epoch 36, Loss: 17.9468, Validation AUC: 0.8968\n",
            "Epoch 37, Loss: 14.0656, Validation AUC: 0.8188\n",
            "Epoch 38, Loss: 9.6308, Validation AUC: 0.8701\n",
            "Epoch 39, Loss: 6.4144, Validation AUC: 0.8414\n",
            "Epoch 40, Loss: 5.8944, Validation AUC: 0.0650\n",
            "Epoch 41, Loss: 7.8003, Validation AUC: 0.8675\n",
            "Epoch 42, Loss: 8.8892, Validation AUC: 0.8447\n",
            "Epoch 43, Loss: 9.7138, Validation AUC: 0.8435\n",
            "Epoch 44, Loss: 8.8370, Validation AUC: 0.8294\n",
            "Epoch 45, Loss: 6.7425, Validation AUC: 0.8431\n",
            "Epoch 46, Loss: 4.6887, Validation AUC: 0.2656\n",
            "Epoch 47, Loss: 3.6005, Validation AUC: 0.0405\n",
            "Epoch 48, Loss: 3.4658, Validation AUC: 0.8268\n",
            "Epoch 49, Loss: 3.4404, Validation AUC: 0.8276\n",
            "Epoch 50, Loss: 3.7814, Validation AUC: 0.8155\n",
            "Test ROC-AUC: 0.8210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **5. Results and Conclusion**\n",
        "### **Test Performance**\n",
        "Evaluate the model on the test set.\n",
        "\n",
        "### **Insights**\n",
        "The model demonstrates how GNNs can effectively predict links (e.g., patient-drug associations) in a biological knowledge graph.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "mymVIlx2_fOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "num_test_edges = test_edges.size(1)  # Number of positive test edges\n",
        "test_neg_edges = sample_negative_edges(test_edges, data.x.size(0), num_test_edges)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_auc = evaluate(test_edges, test_neg_edges)\n",
        "\n",
        "print(f\"Test ROC-AUC: {test_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "F8B9RgbK_gty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc2e5737-2a3f-4cc7-d1ae-667fa768774e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test ROC-AUC: 0.8245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Check-in Questions:**\n",
        "\n",
        "Why is the SAGEConv layer used in the model, and how does it aggregate node information?\n",
        "\n",
        "What role do node embeddings play in the context of link prediction?\n",
        "\n",
        "How does the model leverage graph structure (nodes and edges) during training?"
      ],
      "metadata": {
        "id": "Kjzkr9yXCE8l"
      }
    }
  ]
}