{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Knowledge Graphs\n",
        "\n",
        "By: Thierno Diallo\n",
        "\n",
        "In this notebook we walkthorugth visualizations of relational data as Knowledge Graphs alongside implemnetations of Graph Convolutional Networks to classify entities (nodes) and predict relations between entities (edges). Please find linked the notebooks used for this walkthrough alongside the original paper on the topic:\n",
        "- Predicting Relations By Haoxin Li: https://github.com/mims-harvard/graphml-tutorials/blob/master/README.md\n",
        "- Classifying Nodes: https://colab.research.google.com/drive/1LJir3T6M6Omc2Vn2GV2cDW_GV2YfI53_?usp=sharing\n",
        "- Original Paper: https://arxiv.org/pdf/1703.06103"
      ],
      "metadata": {
        "id": "VMTs-nH0EjrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Node Classification on large Knowledge Graphs\n",
        "\n",
        "In this exercise, we look at the the **Cora Dataset** which serves as benchmark for graph-based learning.  It comprises 2,708 scientific publications categorized into one of seven classes:\n",
        "\n",
        "*  Case-Based\n",
        "*  Genetic Algorithms\n",
        "*  Neural Networks\n",
        "*  Probabilistic Methods\n",
        "*  Reinforcement Learning\n",
        "*  Rule Learning\n",
        "*  Theory\n",
        "\n",
        "Each publication is represented by a binary word vector of length 1,433, indicating the presence or absence of specific words from a predefined dictionary. The dataset includes a citation network with 5,429 links, where each directed edge denotes a citation from one paper to another and each paper represnts a node in our directed graph."
      ],
      "metadata": {
        "id": "1Gm-cuVzao0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading and Visualization"
      ],
      "metadata": {
        "id": "mPwUk33BkVVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dependencies\n",
        "# Check CUDA Version\n",
        "!python -c \"import torch; print(torch.version.cuda)\"\n",
        "\n",
        "# Install Pytorch Geometric\n",
        "!pip install torch torch_geometric matplotlib pandas networkx #torch-sparse"
      ],
      "metadata": {
        "id": "QznGMAYukcT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "from torch_geometric.utils import to_networkx\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import random"
      ],
      "metadata": {
        "id": "-r5dP7ANfDzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())"
      ],
      "metadata": {
        "id": "POfFUCTtzx9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Cora dataset\n",
        "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())\n",
        "\n",
        "# Get some basic info about the dataset\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "#TODO: There is only one graph in the dataset, use it as new data object\n",
        "data = TO DO\n",
        "print(f'Number of Citations: {data.num_nodes}')\n",
        "print(f'Number of Edges: {data.num_edges}')\n",
        "print(\"Citations Per Catagory:\\n\",pd.Series(data.y.numpy()).value_counts().sort_index())\n",
        "print(50*'=')\n",
        "\n",
        "# Convert our data into an undirected graph for plotting\n",
        "graph = to_networkx(data, to_undirected=True)\n",
        "\n",
        "#TODO: Experiement and visualze graphs with different number of nodes\n",
        "# Visualizes the first N nodes and their relationships\n",
        "N = 1000\n",
        "subset_nodes = list(range(N))\n",
        "subset_graph = graph.subgraph(subset_nodes)\n",
        "\n",
        "\n",
        "# Visualize the subgraph\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.title(f\"Visualization of the First {N} Citations\")\n",
        "pos = nx.spring_layout(subset_graph, seed=42)  # Positioning for better visualization\n",
        "nx.draw(\n",
        "    subset_graph,\n",
        "    pos,\n",
        "    node_size=25,\n",
        "    node_color=[data.y[node] for node in subset_nodes],\n",
        "    cmap=plt.cm.tab10,\n",
        "    font_size=8,\n",
        ")\n",
        "plt.show()\n",
        "#TODO: What are the benefits of representing data as a graph for machine learning tasks?\n",
        "#TODO: How does a citation network represent relationships between nodes (i.e how is this a knowledge graph)?"
      ],
      "metadata": {
        "id": "nURz8Ke8fIYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Node Classifcation **Without** Considering Knowledege Graph\n",
        "\n",
        "Here we consider a baseline Multi Layer Perceptron model that does not take into account the citations, but rather will take the node features as input and predict the class label of nodes. Here, we are not considering the graph structure of how nodes are related through citations to form baseline that we can compare our Graph Convolutional Network to. You can implement any such model of your choice below, however, **for the solution we will implement an MLP model with two fully connected models**."
      ],
      "metadata": {
        "id": "91nbl47a2l1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Extract Features and Labels\n",
        "x = TO DO # Node Features\n",
        "y = TO DO # Node Labels\n",
        "\n",
        "#TODO: Split data into training, validation, and test (hint loader already does this)\n",
        "train_mask = data.train_mask\n",
        "val_mask = TO DO\n",
        "test_mask = TO DO\n",
        "\n",
        "#TO DO: Create an MLP baseline model with 2 fully connected layers\n",
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "    super(MLP, self).__init__()\n",
        "    #TODO: Add layers\n",
        "\n",
        "  def forward(self, x):\n",
        "    #TODO: Complete Forward Propagation step\n",
        "    return x\n",
        "\n",
        "#TODO: Initialize MLP baseline model\n",
        "in_channels =  # Node Features\n",
        "hidden_channels = 64 # How many hidden layes dow we want. Feel free to modify.\n",
        "out_channels = # Number of classes/outputs\n",
        "baseline_model = MLP(in_channels, hidden_channels, out_channels)\n",
        "\n",
        "# Define Optimizer\n",
        "optimizer = torch.optim.Adam(baseline_model.parameters(), lr=0.01)\n",
        "# Define Loss Function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#TODO: Train Model\n",
        "def train(data, epochs=100):\n",
        "  #TODO: set up model for training\n",
        "  for epoch in range(epochs):\n",
        "    #TODO: Optimzer\n",
        "    #TODO: Forward Propagate\n",
        "    #TODO: Compute Loss\n",
        "    #TODO: Back propagate\n",
        "    #TODO: Update model weights\n",
        "\n",
        "    # Print Epoch, Loss, and Training Accuracy every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "      pred = out.argmax(dim=1)\n",
        "      correct = (pred[data.train_mask] == data.y[data.train_mask]).sum()\n",
        "      acc = int(correct) / int(data.train_mask.sum())\n",
        "      print(f\"Epoch {epoch+1:03d}, Loss: {loss:.4f}, Train Accuracy: {acc:.4f}\")\n",
        "\n",
        "train(data)\n"
      ],
      "metadata": {
        "id": "SLBESQSu25cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets Evaluate our model\n",
        "def evaluate_model(data):\n",
        "  baseline_model.eval()\n",
        "  with torch.no_grad():\n",
        "    out = baseline_model(data.x)\n",
        "    pred = out.argmax(dim=1)\n",
        "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
        "    acc = int(correct) / int(data.test_mask.sum())\n",
        "    print(f\"Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "pred = evaluate_model(data)"
      ],
      "metadata": {
        "id": "DMja1QqZ0GRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph Convolutional Network for Node Classification\n",
        "\n",
        "Now we will construct a Graph Neural Network that considers these relations and compare the test accuracy with that of the MLP. We will implment a simple 2 layer GCN for this walkthrough, however, feel free to modify the netwrok to improve evalution acuracy."
      ],
      "metadata": {
        "id": "WxgwZCmiwQUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv #GATConv\n",
        "\n",
        "#TO DO: Create an GCN with 2 convolutional layers. We want t keep the model\n",
        "# as simple as possible similar to our basiline to compare two simple mdoels.\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(42)\n",
        "\n",
        "        #TODO: Add layers\n",
        "        self.out = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        #TODO: First Message Passing Layer\n",
        "        TO DO\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        #TODO: Second Message Passing Layer\n",
        "        TO DO\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        # Output layer\n",
        "        x = F.softmax(self.out(x), dim=1)\n",
        "        return x\n",
        "\n",
        "model = GCN(hidden_channels=16)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "JOYz9WXwyn19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation"
      ],
      "metadata": {
        "id": "-U39NWXavuCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = GCN(hidden_channels=16)\n",
        "\n",
        "# Use GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "data = data.to(device)\n",
        "\n",
        "# Initialize Optimizer\n",
        "learning_rate = 0.01\n",
        "decay = 5e-4\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=learning_rate,\n",
        "                             weight_decay=decay)\n",
        "# Define loss function (CrossEntropyLoss for Classification Problems with\n",
        "# probability distributions)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "#TODO: FInish model training\n",
        "def train():\n",
        "      TO DO\n",
        "      # Use all data as input, because all nodes have node features\n",
        "      out = model(data.x, data.edge_index)\n",
        "      # Only use nodes with labels available for loss calculation --> mask\n",
        "      loss = TO DO\n",
        "      TO DO\n",
        "      return loss\n",
        "\n",
        "losses = []\n",
        "for epoch in range(0, 1001):\n",
        "    loss = train()\n",
        "    losses.append(loss)\n",
        "    if epoch % 100 == 0:\n",
        "      print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
      ],
      "metadata": {
        "id": "Sq13Fv8ZvZB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "def test():\n",
        "      model.eval()\n",
        "      out = model(data.x, data.edge_index)\n",
        "      # Use the class with highest probability.\n",
        "      pred = out.argmax(dim=1)\n",
        "      # Check against ground-truth labels.\n",
        "      test_correct = pred[data.test_mask] == data.y[data.test_mask]\n",
        "      # Derive ratio of correct predictions.\n",
        "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
        "      print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "test()\n",
        "\n",
        "#TODO: Why do you think we are seeing this increased test accuracy?\n",
        "#TODO: What are the advantages of considering node relations?"
      ],
      "metadata": {
        "id": "HWYk_uvb7KAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWuLUfGEv0hO"
      },
      "source": [
        "# Multi-relational Link Prediction on Knowledge Graphs\n",
        "\n",
        "Now that we have seen node classifcation with knowledge graphs, we will look at link (edge) predicition on knowledge graphs using Relational Graph Convolutional Neural Networks (RGCN) as proposed on the original paper. We will apply this architecture to a biological dataset where each node is a protein or drug and each edge represents the intractions between nodes. Here, we have multi-relational links as we do not have the same interaction between all nodes the time. We will explore this deeper when discussing the dataset.\n",
        "\n",
        "**Motivations:** In the biological world, different types of relation could exist between two entities. For example, a drug/chemical compound can act as a *target, enzyme, carrier* or *transporter* on proteins, forming 4 types of edges. Thus, it would not be ideal to represent these relations using the same edge embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Up Files And Download Dependencies"
      ],
      "metadata": {
        "id": "8J9UPE3QQO8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Mount GPU. Please make sure that you are connected to a runtime\n",
        "# GPU for this exercise otherwise it will take FOREVER to run and might not work\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available!\")\n",
        "    print(\"Device:\", torch.device(\"cuda:0\"))\n",
        "else:\n",
        "    print(\"GPU is not available. Using CPU instead.\")\n",
        "\n",
        "# Download torch dependencies\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "id": "ZP6nI5Rv1VrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOytcbb_v0hU"
      },
      "outputs": [],
      "source": [
        "# Import dependencies\n",
        "from torch_geometric.data import Data, GraphSAINTRandomWalkSampler, NeighborSampler, GraphSAINTEdgeSampler\n",
        "from torch_geometric.nn import RGCNConv, Node2Vec, FastRGCNConv\n",
        "from torch_geometric.utils import contains_isolated_nodes\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, f1_score, average_precision_score\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wa want to save our data in a data file similar to referenced github\n",
        "data_dir = '/content/data'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# List of data files to download\n",
        "data_files = [\n",
        "    'edge_index.pt',\n",
        "    'edge_attr.pt',\n",
        "    'edge_meta_type.pt',\n",
        "    'edge_type.pt',\n",
        "    'x.pt',\n",
        "    'y.pt',\n",
        "    'train_mask.pt',\n",
        "    'val_mask.pt',\n",
        "    'test_mask.pt'\n",
        "]\n",
        "\n",
        "# Base URL of the GitHub repository\n",
        "base_url = 'https://github.com/mims-harvard/graphml-tutorials/raw/master/02-KG/data/'\n",
        "\n",
        "# Download each file\n",
        "for file_name in data_files:\n",
        "    file_url = base_url + file_name\n",
        "    !wget -P {data_dir} {file_url}\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IvUCmfMv0MiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rlv-8WKuv0hV"
      },
      "source": [
        "### Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huyM58PCv0hV"
      },
      "outputs": [],
      "source": [
        "edge_index = torch.load('data/edge_index.pt')\n",
        "row, col = edge_index # row: first row, col: second row\n",
        "edge_attr = torch.load('data/edge_attr.pt')\n",
        "edge_meta_type = torch.load('data/edge_meta_type.pt')\n",
        "edge_type = torch.load('data/edge_type.pt')\n",
        "x = torch.load('data/x.pt')\n",
        "y = torch.load('data/y.pt')\n",
        "num_nodes = len(y) # total number of nodes in the graph\n",
        "\n",
        "train_mask = torch.load('data/train_mask.pt') # training mask of edges, split randomly 80%\n",
        "val_mask = torch.load('data/val_mask.pt') # validation mask of edges, split randomly 10%\n",
        "test_mask = torch.load('data/test_mask.pt') # test_mask of edges, split randomly 10%\n",
        "\n",
        "num_relations = edge_type.unique().size(0) # total number of edge types in the graph\n",
        "\n",
        "data = Data(edge_attr=edge_attr, edge_index=edge_index, edge_type=edge_type, edge_meta_type=edge_meta_type,\n",
        "            x=x, y=y, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4UXughMv0hW"
      },
      "source": [
        "- `edge_index` stores all the edges in the dataset in the form of a 2-D tensor. Each column represents an edge formed by two nodes and the number of columns indicate the total number of edges in the dataset. For example, the first column in `edge_index` is [0, 9052], which represents an edge between node 0 and node 9052.\n",
        "- `edge_attr` contains edge attributes calulated using `1.0 / torch_geometric.utils.degree(col, num_nodes)[col]`. This attribute is used for GraphSAINT sampler. Please see [this](https://github.com/rusty1s/pytorch_geometric/blob/master/examples/graph_saint.py) and [this](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html) for reference.\n",
        "- `edge_meta_type` helps to identify the meta edge type of each edge in `edge_index`. Because drug and protein edges are directional, we use edge meta types here to do negative sampling more easily.  There are 3 meta edges. `1` represents edges between a drug and a protein, where drug is the starting node and protein is the ending node. `2` represents edges between proteins and proteins. `3` represents edges between a protein and a drug where protein is the starting node and drug is the ending node.\n",
        "- `edge_type` stores the edge type for each edge in `edge_index`. The meaning of each number is shown in the next cell. See `edge_type_mapping`.\n",
        "- `x` stores the input embeddings/attributes of each node, with dimension of 128. It was learnt separately using [node2vec](https://arxiv.org/pdf/1607.00653.pdf). The main reason to use these embeddings is to decrease the input dimension for each node from 25455 to 128. Naively, one-hot-encoded embeddings are used to represent each node. Alternatively, one can use random Gaussian vectors as input embeddings/attributes. In applications where side feature information about nodes is available, x can be used to integrate that information into the model.\n",
        "- `y` stores the node type, where `0` represents a drug and `1` represents a protein."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egE7lekav0hW"
      },
      "outputs": [],
      "source": [
        "edge_type_mapping = {\n",
        "    0: 'target',\n",
        "    1: 'enzyme',\n",
        "    2: 'carrier',\n",
        "    3: 'transporter',\n",
        "    4: 'ppi',\n",
        "    5: 'target_rev',\n",
        "    6: 'enzyme_rev',\n",
        "    7: 'carrier_rev',\n",
        "    8: 'transporter_rev'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vh9VIhXv0hW"
      },
      "source": [
        "Here we have 9 different edge types. The last 4 edge types are the opposites of the first 4 edge types as we want our graph to be un-directional.\n",
        "e.g. Drug A **targets** Protein A is equivalent to Protein A is **targeted** by Drug A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx-zl5xnv0hX"
      },
      "outputs": [],
      "source": [
        "data_loader = GraphSAINTRandomWalkSampler(data, batch_size=128, walk_length=16, num_steps=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHCNduCQv0hX"
      },
      "source": [
        "We utilize [GraphSAINT Random Walk Sampler](https://arxiv.org/pdf/1907.04931.pdf) as it allows us to sample fully-connected sub-graphs for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK2U8CFmv0hX"
      },
      "source": [
        "### Constructing a GNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAuuC2rlv0hX"
      },
      "outputs": [],
      "source": [
        "class RGCN(torch.nn.Module):\n",
        "    def __init__(self, in_dim, h_dim, out_dim, num_rels):\n",
        "        super(RGCN, self).__init__()\n",
        "        self.num_rels = num_rels # Number of relations\n",
        "        # Create a two layers FastRGCNConv(in_channels, out_channels, num_relations) model\n",
        "        self.conv1 = TO DO\n",
        "        self.conv2 = TO DO\n",
        "        self.relu = nn.ReLU()\n",
        "        self.w_rels = nn.Parameter(torch.Tensor(num_rels, out_dim))\n",
        "        nn.init.xavier_uniform_(self.w_rels,\n",
        "                                gain=nn.init.calculate_gain('relu'))\n",
        "\n",
        "    def forward(self, x, edge_index, edge_type):\n",
        "        #TODO: Forward propagate\n",
        "        out = F.log_softmax(x2, dim=1)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Computes metrics like loss and probabilities for link prediction.\n",
        "def get_metrics(model, embed, edge_index, edge_type, labels):\n",
        "    probs = DistMult(embed, edge_index, edge_type, model)\n",
        "    loss = F.binary_cross_entropy(probs, labels)\n",
        "\n",
        "    probs = probs.cpu().detach().numpy()\n",
        "    labels = labels.cpu().detach().numpy()\n",
        "\n",
        "    return loss, probs, labels\n",
        "\n",
        "# Implements scoring function\n",
        "def DistMult(embed, edge_index, edge_type, model):\n",
        "    s = embed[edge_index[0, :]]\n",
        "    o = embed[edge_index[1, :]]\n",
        "    r = model.w_rels[edge_type]\n",
        "    scores = torch.sum(s * r * o, dim=1)\n",
        "\n",
        "    return torch.sigmoid(scores)\n",
        "\n",
        "# Creates a tensor of link labels (1 for positive edges, 0 for negative edges).\n",
        "def get_link_labels(edge_index_pos_len, edge_index_neg_len):\n",
        "    link_labels = torch.zeros(edge_index_pos_len + edge_index_neg_len).float().to(device)\n",
        "    link_labels[:int(edge_index_pos_len)] = 1.\n",
        "    return link_labels\n",
        "\n",
        "# Generates node embeddings by passing data through the RGCN model.\n",
        "def get_embeddings(data):\n",
        "    data = data.to(device)\n",
        "    x = data.x\n",
        "    edge_index_pos = data.edge_index\n",
        "    edge_type = torch.squeeze(data.edge_type)\n",
        "    embed = model(x, edge_index_pos, edge_type)\n",
        "\n",
        "    return embed\n",
        "\n",
        "def negative_sample(edge_index, edge_meta_type):\n",
        "    \"\"\"\n",
        "    generate negative samples but keep the node type the same\n",
        "    \"\"\"\n",
        "    edge_index_copy = edge_index.clone()\n",
        "\n",
        "    # resample ppi, the meta edge type for ppi is 2\n",
        "    ppi = edge_index_copy[0, torch.squeeze(edge_meta_type == 2)]\n",
        "    new_index = torch.randperm(ppi.shape[0])\n",
        "    new_ppi = ppi[new_index]\n",
        "    edge_index_copy[0, torch.squeeze(edge_meta_type == 2)] = new_ppi\n",
        "\n",
        "    #resample dpi, the meta edge type for ppi is 1\n",
        "    dpi = edge_index_copy[0, torch.squeeze(edge_meta_type == 1)]\n",
        "    new_index = torch.randperm(dpi.shape[0])\n",
        "    new_dpi = dpi[new_index]\n",
        "    edge_index_copy[0, torch.squeeze(edge_meta_type == 1)] = new_dpi\n",
        "\n",
        "    #resample dpi_rev, the meta edge type for ppi is 3\n",
        "    dpi_rev = edge_index_copy[0, torch.squeeze(edge_meta_type == 3)]\n",
        "    new_index = torch.randperm(dpi_rev.shape[0])\n",
        "    new_dpi_rev = dpi_rev[new_index]\n",
        "    edge_index_copy[0, torch.squeeze(edge_meta_type == 3)] = new_dpi_rev\n",
        "\n",
        "    return edge_index_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT5qS5x2v0hX"
      },
      "outputs": [],
      "source": [
        "params = {'in_dim': 128,\n",
        "          'h_dim':64,\n",
        "          'out_dim':64,\n",
        "          'num_rels': num_relations,\n",
        "          'epochs':50}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0opvS9JTv0hX"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = RGCN(params['in_dim'], params['h_dim'], params['out_dim'], params['num_rels']).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzUmQJH0v0hY"
      },
      "source": [
        "Here we construct a 2-layer RGCN with hidden dimension of 64 for both node and edge embeddings. We model it as a binary classification task that tries to minimize the loss between real edge labels and fake edge labels geneated from negative sampling. We use RGCN as the encoder for node embeddings and DistMult as the decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcQa7BbJv0hY"
      },
      "source": [
        "### Model Training\n",
        "Note: the data for training is sampled from GraphSaint, whereas the data for validation is the whole graph. Parameters initialization may affect model convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9cZz0h0v0hY"
      },
      "outputs": [],
      "source": [
        "def train(data, embed):\n",
        "    data = data.to(device)\n",
        "    x = data.x\n",
        "\n",
        "    edge_index_train_pos = data.edge_index[:, data.train_mask]\n",
        "    edge_type_train = torch.squeeze(data.edge_type[data.train_mask])\n",
        "\n",
        "    edge_meta_type = data.edge_meta_type[data.train_mask]\n",
        "    edge_index_train_neg = negative_sample(edge_index_train_pos, edge_meta_type)\n",
        "\n",
        "    edge_index_train_total = torch.cat([edge_index_train_pos, edge_index_train_neg], dim=-1)\n",
        "    edge_type_train_total = torch.cat([edge_type_train, edge_type_train[:edge_index_train_neg.size(1)]], dim=-1)\n",
        "\n",
        "\n",
        "    link_labels = get_link_labels(edge_index_train_pos.size(1), edge_index_train_neg.size(1))\n",
        "    loss, probs, labels = get_metrics(model, embed, edge_index_train_total, edge_type_train_total,\n",
        "                                            link_labels)\n",
        "\n",
        "    auroc = roc_auc_score(labels, probs)\n",
        "    auprc = average_precision_score(labels, probs)\n",
        "\n",
        "    loss_epoch_train.append(loss.item())\n",
        "    auroc_epoch_train.append(auroc)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "@torch.no_grad()\n",
        "def validation(data, embed, evaluate_rel=False):\n",
        "    data = data.to(device)\n",
        "    x = data.x\n",
        "\n",
        "    edge_index_val_pos = data.edge_index[:, data.val_mask]\n",
        "    edge_type_val = torch.squeeze(data.edge_type[data.val_mask])\n",
        "\n",
        "    edge_meta_type = data.edge_meta_type[data.val_mask]\n",
        "    edge_index_val_neg = negative_sample(edge_index_val_pos, edge_meta_type)\n",
        "\n",
        "    edge_index_val_total = torch.cat([edge_index_val_pos, edge_index_val_neg], dim=-1)\n",
        "    edge_type_val_total = torch.cat([edge_type_val, edge_type_val[:edge_index_val_neg.size(1)]], dim=-1)\n",
        "\n",
        "    link_labels = get_link_labels(edge_index_val_pos.size(1), edge_index_val_neg.size(1))\n",
        "    loss, probs, labels = get_metrics(model, embed, edge_index_val_total, edge_type_val_total,\n",
        "                                                                link_labels)\n",
        "    auroc = roc_auc_score(labels, probs)\n",
        "    auprc = average_precision_score(labels, probs)\n",
        "\n",
        "    edge_type_val_total = edge_type_val_total.detach().cpu()\n",
        "\n",
        "    loss_epoch_val.append(loss.item())\n",
        "    auroc_epoch_val.append(auroc)\n",
        "\n",
        "    if not evaluate_rel:\n",
        "        return\n",
        "\n",
        "    for i in range(num_relations):\n",
        "        mask = (edge_type_val_total == i)\n",
        "        if mask.sum() == 0:\n",
        "            continue\n",
        "        probs_per_rel = probs[mask]\n",
        "        labels_per_rel = labels[mask]\n",
        "        auroc_per_rel = roc_auc_score(labels_per_rel, probs_per_rel)\n",
        "        auroc_edge_type[i].append(auroc_per_rel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ibvxCUVSv0hY"
      },
      "outputs": [],
      "source": [
        "loss_train_total, loss_val_total = [], []\n",
        "auroc_train_total, auroc_val_total = [], []\n",
        "\n",
        "for epoch in range(0, params['epochs']):\n",
        "    loss_epoch_train, loss_epoch_val = [], []\n",
        "    auroc_epoch_train, auroc_epoch_val = [], []\n",
        "\n",
        "    for batch in data_loader:\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        model.train()\n",
        "        embed = get_embeddings(batch)\n",
        "        train(batch, embed)\n",
        "        model.eval()\n",
        "        validation(batch, embed)\n",
        "\n",
        "    loss_train_total.append(np.mean(loss_epoch_train))\n",
        "    auroc_train_total.append(np.mean(auroc_epoch_train))\n",
        "    loss_val_total.append(np.mean(loss_epoch_val))\n",
        "    auroc_val_total.append(np.mean(auroc_epoch_val))\n",
        "\n",
        "    print('Epoch: {} | train loss: {} | train auroc: {} |'.format(epoch + 1,\n",
        "                                                                  \"%.2f\" % np.mean(loss_epoch_train),\n",
        "                                                                  \"%.2f\" % np.mean(auroc_epoch_train)))\n",
        "    print('Epoch: {} | val loss: {} | val auroc: {} |'.format(epoch + 1,\n",
        "                                                              \"%.2f\" % np.mean(loss_epoch_val),\n",
        "                                                              \"%.2f\" % np.mean(auroc_epoch_val)))\n",
        "\n",
        "    print('----------------------------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTVMhdO5v0hZ"
      },
      "outputs": [],
      "source": [
        "auroc_edge_type = {rel:[] for rel in range(num_relations)}\n",
        "\n",
        "for batch in data_loader:\n",
        "    embed = get_embeddings(batch)\n",
        "    validation(batch, embed, evaluate_rel=True)\n",
        "\n",
        "for rel, values in auroc_edge_type.items():\n",
        "     print('auroc for relation type {}: {}'.format(edge_type_mapping[rel], \"%.3f\" % np.mean(values)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimentation\n",
        "\n",
        "In this section, we will make modifications to the above model to experiment with multi-edge detection and building your own RGCN. For this exercie we will introduce attention to allow the model to learn how important each relation type is for the task.\n",
        "\n",
        "Useful Link: https://pytorch-geometric.readthedocs.io/en/2.5.3/generated/torch_geometric.nn.conv.RGATConv.html\n",
        "\n",
        "**IMPORTANT: If you lack GPU access then instead of implemnting the expensive attention layers, experiment with modifying the model parameters instead to improve accuracy**"
      ],
      "metadata": {
        "id": "GJCt7nSB_yCo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRiDizNAv0hZ"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import RGATConv\n",
        "#TODO: Create Relational Graph Convolutional Network for multinode prediction including attention.\n",
        "# Conisder modifying and reusing the above code, but feel free to reinvent the wheel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Model with the same model paramters as above and output the loss similar to above.\n",
        "# You might consider reducing batch sizes to reduce time"
      ],
      "metadata": {
        "id": "t1moNRLxByDR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}