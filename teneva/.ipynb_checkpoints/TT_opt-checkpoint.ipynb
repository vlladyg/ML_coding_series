{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb87338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import teneva\n",
    "from time import perf_counter as tpc\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c357184",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "# Target function:\n",
    "f = lambda x: 10. - np.sum(x**2)\n",
    "f_batch = lambda X: np.array([f(x) for x in X])\n",
    "\n",
    "d = 5                              # Dimension\n",
    "a = [-2.]*d                        # Grid lower bounds\n",
    "b = [+2.]*d                        # Grid upper bounds\n",
    "n = [201]*d                        # Grid size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973023ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f9504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build very accurate approximation of the function:\n",
    "Y0 = teneva.rand(n, r=2, seed=42)  # Initial approximation for TT-cross\n",
    "Y = teneva.cross(lambda I: f_batch(teneva.ind_to_poi(I, a, b, n, 'cheb')),\n",
    "    Y0, m=5.E+5, e=None, log=True)\n",
    "Y = teneva.truncate(Y, 1.E-16)\n",
    "\n",
    "# We compute the TT-tensor for Chebyshev interpolation coefficients:\n",
    "A = teneva.func_int(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b35ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We find the maximum modulo point:\n",
    "x_opt = teneva.optima_func_tt_beam(A, k=10)     \n",
    "y_opt = teneva.func_get(x_opt, A, a, b)\n",
    "\n",
    "print(f'x opt appr :', x_opt)\n",
    "print(f'y opt appr :', y_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75adb2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "?teneva.optima_func_tt_beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "?teneva.ind_to_poi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddbce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The demo of using ttopt for maximization. Example with QTT.\n",
    "\n",
    "We'll find the maximum for the 10-dimensional Alpine function with vectorized\n",
    "input. The target function for maximization has the form f(X), where input X is\n",
    "the [samples, dimension] numpy array.\n",
    "\n",
    "Run it from the root of the project as \"python demo/qtt_max.py\".\n",
    "\n",
    "As a result of the script work we expect the output in console like this:\n",
    "\"\n",
    "...\n",
    "Alpine-10d | evals=1.00e+05 | t_cur=1.65e-01 | y= 8.715206e+01\n",
    "----------------------------------------------------------------------\n",
    "Alpine-10d | evals=1.00e+05 | t_all=2.22e+00 | y= 8.715206e+01 \n",
    "\"\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from ttopt import TTOpt\n",
    "from ttopt import ttopt_init\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "d = 10                     # Number of function dimensions:\n",
    "rank = 4                    # Maximum TT-rank while cross-like iterations\n",
    "#def f(X):                   # Target function\n",
    "#    return np.sum(np.abs(X * np.sin(X) + 0.1 * X), axis=1)\n",
    "\n",
    "f = lambda X: (10. - np.sum(X**2, axis = 1))\n",
    "\n",
    "# We initialize the TTOpt class instance with the correct parameters:\n",
    "tto = TTOpt(\n",
    "    f=f,                    # Function for maximization. X is [samples, dim]\n",
    "    d=d,                    # Number of function dimensions\n",
    "    a=-10.,                 # Grid lower bound (number or list of len d)\n",
    "    b=+10.,                 # Grid upper bound (number or list of len d)\n",
    "    p=2,                    # The grid size factor (there will n=p^q points)\n",
    "    q=20,                   # The grid size factor (there will n=p^q points)\n",
    "    evals=5.E+6,            # Number of function evaluations\n",
    "    name='Alpine',          # Function name for log (this is optional)\n",
    "    with_log=True)\n",
    "\n",
    "\n",
    "# And now we launching the maximizer:\n",
    "tto.optimize(rank, is_max=True)\n",
    "\n",
    "\n",
    "# We can extract the results of the computation:\n",
    "x = tto.x_opt          # The found value of the maximum of the function (x)\n",
    "y = tto.y_opt          # The found value of the maximum of the function (y=f(x))\n",
    "k_c = tto.k_cache      # Total number of cache usage (should be 0 in this demo)\n",
    "k_e = tto.k_evals      # Total number of requests to func (is always = evals)\n",
    "k_t = tto.k_total      # Total number of requests (k_cache + k_evals)\n",
    "t_f = tto.t_evals_mean # Average time spent to real function call for 1 point\n",
    "                       # ... (see \"ttopt.py\" and docs for more details)\n",
    "\n",
    "\n",
    "# We log the final state:\n",
    "print('-' * 70 + '\\n' + tto.info() +'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847542ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tto.x_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fb4eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "?TTOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e62b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from gmpy2 import mpfr, get_context\n",
    "get_context().precision=400\n",
    "\n",
    "\n",
    "# Import of functions for creation of GP from the data, \n",
    "# separate training\n",
    "#thhermodynamic consts\n",
    "from GPPhad import create_from_scratch, retrain, consts\n",
    "\n",
    "# Import of covariance functions, \n",
    "#class for GP, \n",
    "#printing function\n",
    "# function for trained GP loading\n",
    "from GPPhad import cov_real, GP_full, print_point, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48048b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "th_full = [mpfr('-6.247035679999999757683326606638729572296142578125',400),\n",
    " mpfr('0.292807738541853712632700990070588886737823486328125',400),\n",
    " mpfr('0.456572889549176519352613468072377145290374755859375',400),\n",
    " mpfr('-0.04191095261276779615489118668847368098795413970947265625',400),\n",
    " mpfr('0.05475806710280016476044551154700457118451595306396484375',400),\n",
    " mpfr('0.10669002480846336011754971195841790176928043365478515625',400),\n",
    " mpfr('-5.05760411554052335958431285689584910869598388671875',400),\n",
    " mpfr('-0.88172228690680132245915956445969641208648681640625',400),\n",
    " mpfr('0.0298605271304240964258536195075066643767058849334716796875',400),\n",
    " mpfr('-0.007369150920216244009253170332840454648248851299285888671875',400),\n",
    " mpfr('11.3855892321983223069992163800634443759918212890625',400),\n",
    " mpfr('-3.817151634735199028369834195473231375217437744140625',400),\n",
    " mpfr('0.27150991454327677576685573512804694473743438720703125',400),\n",
    " mpfr('0.56436708068229746171340366345248185098171234130859375',400),\n",
    " mpfr('-0.06247883283127071696316789939373848028481006622314453125',400),\n",
    " mpfr('-0.035707238514284513064556136896499083377420902252197265625',400),\n",
    " mpfr('-1.2659861741798399403791108852601610124111175537109375',400)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362f9e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = ['liq', 'sol_fcc', 'sol_bcc']\n",
    "x_fixed = [5, 10**30]\n",
    "\n",
    "GP_Li = create_from_scratch(cov_real, th_full, phases, x_fixed = x_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41140f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_Li.marg_like(th_full, recomp = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8506da14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounds for phases volumes\n",
    "GP_Li.bounds = {\"liq\": [10, 24], \"sol_fcc\": [10, 15], 'sol_bcc': [10, 24]}\n",
    "\n",
    "# Melting points with error\n",
    "melt_points = [[['sol_fcc', 'liq'], [12/consts['Pk'], 495.5*consts['k'], 1.6*consts['k']]],\n",
    "                [['sol_bcc', 'liq'], [0/consts['Pk'], 475.7*consts['k'], 1.7*consts['k']]]]\n",
    "\n",
    "# Index of hyperparameters that correspond to phases\n",
    "ind_bounds = {'liq': range(0, 6), 'sol_fcc': range(6, 11), 'sol_bcc': range(11, 17)}\n",
    "\n",
    "# Separate train of phases\n",
    "GP_Li, th_temp = retrain(GP_Li, melt_points, ind_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3914614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_Li.th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832a109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit GP_Li.marg_like(th_temp, recomp = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "fe357edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import gmpy2 as gp\n",
    "gp.get_context().precision=400\n",
    "\n",
    "import mpinv\n",
    "from mpinv import fast_mp_matrix_inverse_symm as f_mp_inv\n",
    "from mpinv import fast_mp_matrix_logdet_symm as f_mp_logdet\n",
    "\n",
    "def marg_like(self, th, phase = None, recomp = False):\n",
    "    \"\"\"Marginal likelihood estimation for the given Gaussian process and list of hyperparameters\n",
    "\n",
    "    INPUT:\n",
    "\n",
    "    set of hyperparameters (th)\n",
    "\n",
    "    phase (phase)\n",
    "\n",
    "    optinal recomputation of inverve covariance matrix of the GP (recomp)\n",
    "\n",
    "    OUTPUT:\n",
    "\n",
    "    -mar - marginal likelihood value * -1\n",
    "\n",
    "    \"\"\"\n",
    "    if not self.melt:\n",
    "        if recomp:\n",
    "            print(phase, th)\n",
    "            self.th = th\n",
    "\n",
    "            # Consruction of new covarience matrix (K) and its inverse (K_inv) for particular phase\n",
    "            self.K[phase] = self.constr_matrix(self.X[phase])\n",
    "            self.err_m[phase] = self.constr_matrix(self.err[phase])\n",
    "            self.K_inv[phase] = f_mp_inv(self.K[phase] + self.err_m[phase])\n",
    "\n",
    "        # [0, 0] element corresponds to the value\n",
    "        val_1 = 1./2.*(self.Y[phase].T@self.K_inv[phase]@self.Y[phase])[0, 0]\n",
    "        val_2 = 1./2.*f_mp_logdet(self.K[phase] + self.err_m[phase])\n",
    "        val_3 = len(self.Y[phase])/2.*gp.log(2.*gp.const_pi(200))\n",
    "\n",
    "        mar = -val_1 - val_2 - val_3\n",
    "        print(-mar)\n",
    "    else:\n",
    "        if recomp:\n",
    "            self.th = th\n",
    "\n",
    "            # Consruction of new covarience matrix (K) and its inverse (K_inv) for particular phase\n",
    "            K = np.array(self.constr_matrix_melt(self.X), dtype = np.float64)\n",
    "            err_m = np.array(self.constr_matrix_melt(self.err), dtype = np.float64)\n",
    "            #K_inv = np.linalg.inv(K + err_m)\n",
    "        \n",
    "            L = np.linalg.cholesky(K + err_m)\n",
    "            #L_inv = np.linalg.inv(L)\n",
    "            \n",
    "            #K_inv =  L_inv.T @ L_inv\n",
    "            \n",
    "            Y = np.array(self.Y, dtype = np.float64)\n",
    "            \n",
    "            alpha = np.linalg.solve(L.T, np.linalg.solve(L, Y))\n",
    "            #alpha = \n",
    "            \n",
    "            #print(alpha.T.shape)\n",
    "            #val_1 = 1./2.*(Y.T@K_inv@Y)[0, 0] # [0, 0] element corresponds to the value\n",
    "            #val_2 = 1./2.*f_mp_logdet(K + err_m)\n",
    "            val_1 = 1/2.* Y[:, 0] @ alpha[:, 0]\n",
    "            #val_1 = 0.\n",
    "            val_2 =  np.sum(np.log(np.diag(L)))\n",
    "            \n",
    "            val_3 = len(self.Y)/2.*np.log(2.*np.pi)\n",
    "            \n",
    "            \n",
    "        mar = -val_1 - val_2 - val_3\n",
    "        #print(-mar)\n",
    "\n",
    "    if 0.0 == np.nan_to_num(np.float64(-mar)):\n",
    "        return 10**10\n",
    "    else:\n",
    "        return -np.float64(-mar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "7cc3ccb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154, 1)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GP_Li.Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "4e5766b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194 ms ± 1.34 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit marg_like(GP_Li, th_temp_arr, recomp = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "75c0b567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291.4834812797729"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marg_like(GP_Li, th_temp_arr, recomp = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "cc10a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "marg_like_fit = lambda th: marg_like(GP_Li, th, recomp = True)\n",
    "\n",
    "th_temp_arr = np.array(th_temp, dtype = np.float64)\n",
    "th_low = (th_temp_arr - np.abs(th_temp_arr) * 0.2).tolist()\n",
    "th_up = (th_temp_arr + np.abs(th_temp_arr) * 0.2).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "752985a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(267.0305543202149, 209.30247004412115)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marg_like(GP_Li, th_up, recomp = True), marg_like(GP_Li, th_low, recomp = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "7a455626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297.4557572251796"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marg_like_fit(th_temp_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "62f15248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17,)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th_temp_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "6855189c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpine-17d | evals=8.00e+01 | t_cur=1.55e+01 | y= 2.914835e+02 \n",
      "Alpine-17d | evals=1.00e+02 | t_cur=1.93e+01 | y= 2.914835e+02 \n",
      "----------------------------------------------------------------------\n",
      "Alpine-17d | evals=1.00e+02 | t_all=1.94e+01 | y= 2.914835e+02 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The demo of using ttopt for maximization. Example with QTT.\n",
    "\n",
    "We'll find the maximum for the 10-dimensional Alpine function with vectorized\n",
    "input. The target function for maximization has the form f(X), where input X is\n",
    "the [samples, dimension] numpy array.\n",
    "\n",
    "Run it from the root of the project as \"python demo/qtt_max.py\".\n",
    "\n",
    "As a result of the script work we expect the output in console like this:\n",
    "\"\n",
    "...\n",
    "Alpine-10d | evals=1.00e+05 | t_cur=1.65e-01 | y= 8.715206e+01\n",
    "----------------------------------------------------------------------\n",
    "Alpine-10d | evals=1.00e+05 | t_all=2.22e+00 | y= 8.715206e+01 \n",
    "\"\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from ttopt import TTOpt\n",
    "from ttopt import ttopt_init\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "d = 17                    # Number of function dimensions:\n",
    "rank = 4                    # Maximum TT-rank while cross-like iterations\n",
    "#def f(X):                   # Target function\n",
    "#    return np.sum(np.abs(X * np.sin(X) + 0.1 * X), axis=1)\n",
    "\n",
    "f = lambda X: (10. - np.sum(X**2, axis = 1))\n",
    "\n",
    "# We initialize the TTOpt class instance with the correct parameters:\n",
    "tto = TTOpt(\n",
    "    f=marg_like_fit,                    # Function for maximization. X is [samples, dim]\n",
    "    d=d,                    # Number of function dimensions\n",
    "    a=th_low,                 # Grid lower bound (number or list of len d)\n",
    "    b=th_up,                 # Grid upper bound (number or list of len d)\n",
    "    p=4,                    # The grid size factor (there will n=p^q points)\n",
    "    q=30,                   # The grid size factor (there will n=p^q points)\n",
    "    evals=1.E+2,            # Number of function evaluations\n",
    "    name='Alpine',          # Function name for log (this is optional)\n",
    "    with_log=True,\n",
    "    is_vect = False)\n",
    "\n",
    "\n",
    "# And now we launching the maximizer:\n",
    "tto.optimize(rank, is_max=True)\n",
    "\n",
    "\n",
    "# We can extract the results of the computation:\n",
    "x = tto.x_opt          # The found value of the maximum of the function (x)\n",
    "y = tto.y_opt          # The found value of the maximum of the function (y=f(x))\n",
    "k_c = tto.k_cache      # Total number of cache usage (should be 0 in this demo)\n",
    "k_e = tto.k_evals      # Total number of requests to func (is always = evals)\n",
    "k_t = tto.k_total      # Total number of requests (k_cache + k_evals)\n",
    "t_f = tto.t_evals_mean # Average time spent to real function call for 1 point\n",
    "                       # ... (see \"ttopt.py\" and docs for more details)\n",
    "\n",
    "\n",
    "# We log the final state:\n",
    "print('-' * 70 + '\\n' + tto.info() +'\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "6a5dc690",
   "metadata": {},
   "outputs": [],
   "source": [
    "tto.x_opt\n",
    "\n",
    "th_temp_arr = np.array(tto.x_opt, dtype = np.float64)\n",
    "th_low = (th_temp_arr - np.abs(th_temp_arr) * 0.4).tolist()\n",
    "th_up = (th_temp_arr + np.abs(th_temp_arr) * 0.4).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "1d0eef1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpine-17d | evals=2.40e+01 | t_cur=4.67e+00 | y= 2.832084e+02 \n",
      "Alpine-17d | evals=5.60e+01 | t_cur=1.07e+01 | y= 2.832084e+02 \n",
      "Alpine-17d | evals=8.80e+01 | t_cur=1.68e+01 | y= 2.832084e+02 \n",
      "Alpine-17d | evals=1.20e+02 | t_cur=2.28e+01 | y= 2.832084e+02 \n",
      "Alpine-17d | evals=1.52e+02 | t_cur=2.89e+01 | y= 2.832084e+02 \n",
      "Alpine-17d | evals=1.84e+02 | t_cur=3.49e+01 | y= 2.832085e+02 \n",
      "Alpine-17d | evals=2.00e+02 | t_cur=3.80e+01 | y= 2.832085e+02 \n",
      "----------------------------------------------------------------------\n",
      "Alpine-17d | evals=2.00e+02 | t_all=3.82e+01 | y= 2.832085e+02 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The demo of using ttopt for maximization. Example with QTT.\n",
    "\n",
    "We'll find the maximum for the 10-dimensional Alpine function with vectorized\n",
    "input. The target function for maximization has the form f(X), where input X is\n",
    "the [samples, dimension] numpy array.\n",
    "\n",
    "Run it from the root of the project as \"python demo/qtt_max.py\".\n",
    "\n",
    "As a result of the script work we expect the output in console like this:\n",
    "\"\n",
    "...\n",
    "Alpine-10d | evals=1.00e+05 | t_cur=1.65e-01 | y= 8.715206e+01\n",
    "----------------------------------------------------------------------\n",
    "Alpine-10d | evals=1.00e+05 | t_all=2.22e+00 | y= 8.715206e+01 \n",
    "\"\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from ttopt import TTOpt\n",
    "from ttopt import ttopt_init\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "d = 17                    # Number of function dimensions:\n",
    "rank = 4                    # Maximum TT-rank while cross-like iterations\n",
    "#def f(X):                   # Target function\n",
    "#    return np.sum(np.abs(X * np.sin(X) + 0.1 * X), axis=1)\n",
    "\n",
    "f = lambda X: (10. - np.sum(X**2, axis = 1))\n",
    "\n",
    "# We initialize the TTOpt class instance with the correct parameters:\n",
    "tto = TTOpt(\n",
    "    f=marg_like_fit,                    # Function for maximization. X is [samples, dim]\n",
    "    d=d,                    # Number of function dimensions\n",
    "    a=th_low,                 # Grid lower bound (number or list of len d)\n",
    "    b=th_up,                 # Grid upper bound (number or list of len d)\n",
    "    p=2,                    # The grid size factor (there will n=p^q points)\n",
    "    q=30,                   # The grid size factor (there will n=p^q points)\n",
    "    evals=2.E+2,            # Number of function evaluations\n",
    "    name='Alpine',          # Function name for log (this is optional)\n",
    "    with_log=True,\n",
    "    is_vect = False)\n",
    "\n",
    "\n",
    "# And now we launching the maximizer:\n",
    "tto.optimize(rank, is_max=True)\n",
    "\n",
    "\n",
    "# We can extract the results of the computation:\n",
    "x = tto.x_opt          # The found value of the maximum of the function (x)\n",
    "y = tto.y_opt          # The found value of the maximum of the function (y=f(x))\n",
    "k_c = tto.k_cache      # Total number of cache usage (should be 0 in this demo)\n",
    "k_e = tto.k_evals      # Total number of requests to func (is always = evals)\n",
    "k_t = tto.k_total      # Total number of requests (k_cache + k_evals)\n",
    "t_f = tto.t_evals_mean # Average time spent to real function call for 1 point\n",
    "                       # ... (see \"ttopt.py\" and docs for more details)\n",
    "\n",
    "\n",
    "# We log the final state:\n",
    "print('-' * 70 + '\\n' + tto.info() +'\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe68477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit GP_Li.constr_matrix_melt(GP_Li.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4857594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit GP_Li.constr_matrix_melt(GP_Li.err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8db8f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPPhad",
   "language": "python",
   "name": "gpphad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
