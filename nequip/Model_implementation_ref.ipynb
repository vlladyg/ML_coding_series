{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04c760c6-3e5f-4686-b42e-d923b72a047c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.4.0+cu121 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# reference example\n",
    "from nequip.data import dataset_from_config\n",
    "from nequip.utils import Config\n",
    "from nequip.utils.misc import get_default_device_name\n",
    "from nequip.utils.config import _GLOBAL_ALL_ASKED_FOR_KEYS\n",
    "\n",
    "from nequip.model import model_from_config\n",
    "\n",
    "\n",
    "default_config = dict(\n",
    "    root=\"./\",\n",
    "    tensorboard=False,\n",
    "    wandb=False,\n",
    "    model_builders=[\n",
    "        \"SimpleIrrepsConfig\",\n",
    "        \"EnergyModel\",\n",
    "        \"PerSpeciesRescale\",\n",
    "        \"StressForceOutput\",\n",
    "        \"RescaleEnergyEtc\",\n",
    "    ],\n",
    "    dataset_statistics_stride=1,\n",
    "    device=get_default_device_name(),\n",
    "    default_dtype=\"float64\",\n",
    "    model_dtype=\"float32\",\n",
    "    allow_tf32=True,\n",
    "    verbose=\"INFO\",\n",
    "    model_debug_mode=False,\n",
    "    equivariance_test=False,\n",
    "    grad_anomaly_mode=False,\n",
    "    gpu_oom_offload=False,\n",
    "    append=False,\n",
    "    warn_unused=False,\n",
    "    _jit_bailout_depth=2,  # avoid 20 iters of pain, see https://github.com/pytorch/pytorch/issues/52286\n",
    "    # Quote from eelison in PyTorch slack:\n",
    "    # https://pytorch.slack.com/archives/CDZD1FANA/p1644259272007529?thread_ts=1644064449.039479&cid=CDZD1FANA\n",
    "    # > Right now the default behavior is to specialize twice on static shapes and then on dynamic shapes.\n",
    "    # > To reduce warmup time you can do something like setFusionStrartegy({{FusionBehavior::DYNAMIC, 3}})\n",
    "    # > ... Although we would wouldn't really expect to recompile a dynamic shape fusion in a model,\n",
    "    # > provided broadcasting patterns remain fixed\n",
    "    # We default to DYNAMIC alone because the number of edges is always dynamic,\n",
    "    # even if the number of atoms is fixed:\n",
    "    _jit_fusion_strategy=[(\"DYNAMIC\", 3)],\n",
    "    # Due to what appear to be ongoing bugs with nvFuser, we default to NNC (fuser1) for now:\n",
    "    # TODO: still default to NNC on CPU regardless even if change this for GPU\n",
    "    # TODO: default for ROCm?\n",
    "    _jit_fuser=\"fuser1\",\n",
    ")\n",
    "\n",
    "# All default_config keys are valid / requested\n",
    "_GLOBAL_ALL_ASKED_FOR_KEYS.update(default_config.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3a95cb6-65c4-46f2-974e-434c8a75fa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/nequip/utils/torch_geometric/dataset.py:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/nequip/utils/torch_geometric/dataset.py:162: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/nequip/data/_dataset/_base_datasets.py:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, include_frames = torch.load(self.processed_paths[0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AtomicData(atom_types=[15, 1], cell=[3, 3], edge_cell_shift=[152, 3], edge_index=[2, 152], forces=[15, 3], pbc=[3], pos=[15, 3], total_energy=[1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config.from_file('./configs/example.yaml', defaults=default_config)\n",
    "    \n",
    "\n",
    "dataset = dataset_from_config(config, prefix=\"dataset\")\n",
    "\n",
    "validation_dataset = None\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c83cac4-15db-4ebb-9540-919c2062749e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Torch device: cuda\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "Replace string dataset_per_atom_total_energy_mean to -11319.556640625\n",
      "Atomic outputs are scaled by: [H, C: None], shifted by [H, C: -11319.556641].\n",
      "Replace string dataset_forces_rms to 30.621034622192383\n",
      "Initially outputs are globally scaled by: 30.621034622192383, total_energy are globally shifted by None.\n"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "from nequip.train.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(model=None, **Config.as_dict(config))\n",
    "\n",
    "# what is this\n",
    "# to update wandb data?\n",
    "config.update(trainer.params)\n",
    "\n",
    "# = Train/test split =\n",
    "trainer.set_dataset(dataset, validation_dataset)\n",
    "\n",
    "# = Build model =\n",
    "final_model = model_from_config(\n",
    "    config=config, initialize=True, dataset=trainer.dataset_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9bad231-aafd-4963-a29d-32b5d9daf7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5120.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(96 + 64)*64/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced3e0ae-66f3-407d-91d9-0b71fe1c866c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphModel(\n",
       "  (model): RescaleOutput(\n",
       "    (model): StressOutput(\n",
       "      (func): SequentialGraphNetwork(\n",
       "        (one_hot): OneHotAtomEncoding()\n",
       "        (spharm_edges): SphericalHarmonicEdgeAttrs(\n",
       "          (sh): SphericalHarmonics()\n",
       "        )\n",
       "        (radial_basis): RadialBasisEdgeEncoding(\n",
       "          (basis): BesselBasis()\n",
       "          (cutoff): PolynomialCutoff()\n",
       "        )\n",
       "        (chemical_embedding): AtomwiseLinear(\n",
       "          (linear): Linear(2x0e -> 32x0e | 64 weights)\n",
       "        )\n",
       "        (layer0_convnet): ConvNetLayer(\n",
       "          (equivariant_nonlin): Gate (96x0e+32x1o+32x2e -> 32x0e+32x2e+32x1o)\n",
       "          (conv): InteractionBlock(\n",
       "            (linear_1): Linear(32x0e -> 32x0e | 1024 weights)\n",
       "            (fc): FullyConnectedNet[8, 64, 64, 96]\n",
       "            (tp): TensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 96 paths | 96 weights)\n",
       "            (linear_2): Linear(32x0e+32x1o+32x2e -> 96x0e+32x1o+32x2e | 5120 weights)\n",
       "            (sc): FullyConnectedTensorProduct(32x0e x 2x0e -> 96x0e+32x1o+32x2e | 6144 paths | 6144 weights)\n",
       "          )\n",
       "        )\n",
       "        (layer1_convnet): ConvNetLayer(\n",
       "          (equivariant_nonlin): Gate (160x0e+32x1o+32x1e+32x2o+32x2e -> 32x0e+32x1e+32x2e+32x1o+32x2o)\n",
       "          (conv): InteractionBlock(\n",
       "            (linear_1): Linear(32x0e+32x2e+32x1o -> 32x0e+32x2e+32x1o | 3072 weights)\n",
       "            (fc): FullyConnectedNet[8, 64, 64, 480]\n",
       "            (tp): TensorProduct(32x0e+32x2e+32x1o x 1x0e+1x1o+1x2e -> 96x0e+128x1o+64x1e+64x2o+128x2e | 480 paths | 480 weights)\n",
       "            (linear_2): Linear(96x0e+128x1o+64x1e+64x2o+128x2e -> 160x0e+32x1o+32x1e+32x2o+32x2e | 27648 weights)\n",
       "            (sc): FullyConnectedTensorProduct(32x0e+32x2e+32x1o x 2x0e -> 160x0e+32x1o+32x1e+32x2o+32x2e | 14336 paths | 14336 weights)\n",
       "          )\n",
       "        )\n",
       "        (layer2_convnet): ConvNetLayer(\n",
       "          (equivariant_nonlin): Gate (32x0o+160x0e+32x1o+32x1e+32x2o+32x2e -> 32x0e+32x0o+32x1e+32x2e+32x1o+32x2o)\n",
       "          (conv): InteractionBlock(\n",
       "            (linear_1): Linear(32x0e+32x1e+32x2e+32x1o+32x2o -> 32x0e+32x1e+32x2e+32x1o+32x2o | 5120 weights)\n",
       "            (fc): FullyConnectedNet[8, 64, 64, 864]\n",
       "            (tp): TensorProduct(32x0e+32x1e+32x2e+32x1o+32x2o x 1x0e+1x1o+1x2e -> 64x0o+96x0e+192x1o+160x1e+160x2o+192x2e | 864 paths | 864 weights)\n",
       "            (linear_2): Linear(64x0o+96x0e+192x1o+160x1e+160x2o+192x2e -> 32x0o+160x0e+32x1o+32x1e+32x2o+32x2e | 39936 weights)\n",
       "            (sc): FullyConnectedTensorProduct(32x0e+32x1e+32x2e+32x1o+32x2o x 2x0e -> 32x0o+160x0e+32x1o+32x1e+32x2o+32x2e | 18432 paths | 18432 weights)\n",
       "          )\n",
       "        )\n",
       "        (layer3_convnet): ConvNetLayer(\n",
       "          (equivariant_nonlin): Gate (32x0o+160x0e+32x1o+32x1e+32x2o+32x2e -> 32x0e+32x0o+32x1e+32x2e+32x1o+32x2o)\n",
       "          (conv): InteractionBlock(\n",
       "            (linear_1): Linear(32x0e+32x0o+32x1e+32x2e+32x1o+32x2o -> 32x0e+32x0o+32x1e+32x2e+32x1o+32x2o | 6144 weights)\n",
       "            (fc): FullyConnectedNet[8, 64, 64, 960]\n",
       "            (tp): TensorProduct(32x0e+32x0o+32x1e+32x2e+32x1o+32x2o x 1x0e+1x1o+1x2e -> 96x0o+96x0e+192x1o+192x1e+192x2o+192x2e | 960 paths | 960 weights)\n",
       "            (linear_2): Linear(96x0o+96x0e+192x1o+192x1e+192x2o+192x2e -> 32x0o+160x0e+32x1o+32x1e+32x2o+32x2e | 43008 weights)\n",
       "            (sc): FullyConnectedTensorProduct(32x0e+32x0o+32x1e+32x2e+32x1o+32x2o x 2x0e -> 32x0o+160x0e+32x1o+32x1e+32x2o+32x2e | 20480 paths | 20480 weights)\n",
       "          )\n",
       "        )\n",
       "        (conv_to_output_hidden): AtomwiseLinear(\n",
       "          (linear): Linear(32x0e+32x0o+32x1e+32x2e+32x1o+32x2o -> 16x0e | 512 weights)\n",
       "        )\n",
       "        (output_hidden_to_scalar): AtomwiseLinear(\n",
       "          (linear): Linear(16x0e -> 1x0e | 16 weights)\n",
       "        )\n",
       "        (per_species_rescale): PerSpeciesScaleShift()\n",
       "        (total_energy_sum): AtomwiseReduce()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bebd23d-298e-4af7-b1b6-111413ec050d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nequip.data import AtomicData\n",
    "\n",
    "out = final_model(AtomicData.to_AtomicDataDict(dataset[0]))\n",
    "\n",
    "# Dets AtomicDataDict as input and returns AtomicDataDict as output\n",
    "type(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0827540c-adfa-4594-b9cd-f52ee00d8282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'edge_index': tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "           1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,\n",
       "           4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,\n",
       "           6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,\n",
       "           7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10,\n",
       "          10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 12,\n",
       "          12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14,\n",
       "          14, 14, 14, 14, 14, 14, 14, 14],\n",
       "         [14,  9,  8, 10,  6,  5,  3,  2,  1,  7,  8, 14, 13, 12, 11,  9,  7, 10,\n",
       "           5,  4,  3,  2,  0,  6, 14, 13, 12, 11, 10,  9,  8,  7,  5,  4,  3,  1,\n",
       "           0,  6, 14, 13, 12, 11, 10,  5,  0,  1,  2,  6,  4, 14, 13, 12, 11, 10,\n",
       "           6,  5,  3,  2,  1, 13, 12, 11, 10, 14,  4,  3,  2,  1,  0,  6, 14, 13,\n",
       "          12, 10,  9,  8,  7,  5,  4,  3,  2,  1,  0,  9, 14, 10,  8,  6,  2,  1,\n",
       "           0, 14, 10,  9,  7,  6,  2,  1,  0, 14, 10,  8,  7,  6,  2,  1,  0, 11,\n",
       "           9,  8,  7,  6,  5,  4,  3,  2,  1,  0, 12, 10,  3,  5,  4,  2,  1, 13,\n",
       "          11,  6,  5,  4,  3,  2,  1, 12, 14,  3,  6,  5,  4,  2,  1,  9,  8,  7,\n",
       "           3,  5,  4,  2,  1,  0, 13,  6]]),\n",
       " 'pos': tensor([[ 2.4820, -0.2356,  0.2074],\n",
       "         [ 0.9971, -0.0607, -0.0288],\n",
       "         [ 0.3445,  0.9913,  0.6555],\n",
       "         [-1.1056,  1.1824,  0.5429],\n",
       "         [-1.8633,  0.2427, -0.1147],\n",
       "         [-1.2421, -0.8962, -0.6924],\n",
       "         [ 0.1072, -1.1862, -0.6613],\n",
       "         [ 2.7046, -0.0950,  1.2725],\n",
       "         [ 2.9099, -1.2337, -0.0882],\n",
       "         [ 3.0360,  0.5970, -0.2657],\n",
       "         [ 0.7717,  1.7472,  1.3145],\n",
       "         [-1.5988,  2.0514,  1.0065],\n",
       "         [-2.9699,  0.2421, -0.0349],\n",
       "         [-2.0275, -1.6781, -1.0431],\n",
       "         [ 0.5120, -2.0800, -1.0749]], grad_fn=<AddmmBackward0>),\n",
       " 'cell': tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " 'edge_cell_shift': tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " 'atom_types': tensor([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]),\n",
       " 'node_attrs': tensor([[0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.]]),\n",
       " 'node_features': tensor([[-0.0952,  0.0788, -0.0030, -0.2546,  0.1410,  0.1115, -0.0438, -0.1413,\n",
       "           0.0589,  0.0765,  0.1298, -0.0028, -0.0410, -0.0104,  0.0709,  0.0283],\n",
       "         [-0.1065,  0.1026,  0.0291, -0.2558,  0.0970,  0.1016, -0.0757, -0.1275,\n",
       "           0.0625,  0.0416,  0.1061, -0.0559, -0.0216, -0.0595,  0.0602,  0.0620],\n",
       "         [-0.0974,  0.1059,  0.0221, -0.2324,  0.1232,  0.1007, -0.0612, -0.1367,\n",
       "           0.0644,  0.0697,  0.1159, -0.0271, -0.0298, -0.0180,  0.0573,  0.0393],\n",
       "         [-0.0991,  0.1088,  0.0240, -0.2505,  0.1153,  0.1043, -0.0690, -0.1429,\n",
       "           0.0656,  0.0640,  0.1153, -0.0373, -0.0236, -0.0360,  0.0609,  0.0477],\n",
       "         [-0.0975,  0.1133,  0.0251, -0.2483,  0.1156,  0.1035, -0.0704, -0.1461,\n",
       "           0.0660,  0.0660,  0.1162, -0.0369, -0.0224, -0.0345,  0.0585,  0.0471],\n",
       "         [-0.0952,  0.1139,  0.0246, -0.2477,  0.1128,  0.1022, -0.0697, -0.1433,\n",
       "           0.0659,  0.0643,  0.1151, -0.0377, -0.0236, -0.0360,  0.0565,  0.0485],\n",
       "         [-0.0958,  0.0995,  0.0181, -0.2335,  0.1148,  0.0995, -0.0622, -0.1319,\n",
       "           0.0606,  0.0637,  0.1139, -0.0301, -0.0306, -0.0272,  0.0599,  0.0415],\n",
       "         [-0.1086, -0.0323,  0.1122, -0.0949,  0.1738,  0.0457,  0.0076,  0.0523,\n",
       "           0.0011,  0.0257, -0.1338,  0.0316, -0.0975,  0.0521,  0.0502,  0.0849],\n",
       "         [-0.1063, -0.0306,  0.1048, -0.0842,  0.1720,  0.0448,  0.0141,  0.0517,\n",
       "          -0.0054,  0.0272, -0.1281,  0.0327, -0.0948,  0.0547,  0.0472,  0.0814],\n",
       "         [-0.1081, -0.0293,  0.1060, -0.0877,  0.1703,  0.0474,  0.0137,  0.0524,\n",
       "          -0.0022,  0.0272, -0.1278,  0.0326, -0.0949,  0.0527,  0.0507,  0.0843],\n",
       "         [-0.0994, -0.0294,  0.0942, -0.0954,  0.1623,  0.0539,  0.0049,  0.0395,\n",
       "          -0.0073,  0.0241, -0.1198,  0.0407, -0.0974,  0.0656,  0.0485,  0.0719],\n",
       "         [-0.0969, -0.0286,  0.0921, -0.0966,  0.1603,  0.0515,  0.0016,  0.0353,\n",
       "          -0.0113,  0.0206, -0.1155,  0.0389, -0.0951,  0.0632,  0.0470,  0.0711],\n",
       "         [-0.0961, -0.0272,  0.0916, -0.0965,  0.1596,  0.0514,  0.0005,  0.0341,\n",
       "          -0.0128,  0.0183, -0.1142,  0.0387, -0.0936,  0.0620,  0.0471,  0.0708],\n",
       "         [-0.0956, -0.0255,  0.0945, -0.0945,  0.1605,  0.0501,  0.0029,  0.0330,\n",
       "          -0.0138,  0.0142, -0.1175,  0.0415, -0.0929,  0.0634,  0.0508,  0.0702],\n",
       "         [-0.1020, -0.0300,  0.0947, -0.0963,  0.1653,  0.0532,  0.0042,  0.0407,\n",
       "          -0.0046,  0.0263, -0.1216,  0.0395, -0.0980,  0.0660,  0.0480,  0.0711]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " 'edge_vectors': tensor([[-1.9700e+00, -1.8444e+00, -1.2823e+00],\n",
       "         [ 5.5407e-01,  8.3257e-01, -4.7319e-01],\n",
       "         [ 4.2792e-01, -9.9815e-01, -2.9565e-01],\n",
       "         [-1.7103e+00,  1.9828e+00,  1.1071e+00],\n",
       "         [-2.3748e+00, -9.5063e-01, -8.6873e-01],\n",
       "         [-3.7241e+00, -6.6063e-01, -8.9983e-01],\n",
       "         [-3.5876e+00,  1.4180e+00,  3.3546e-01],\n",
       "         [-2.1375e+00,  1.2269e+00,  4.4806e-01],\n",
       "         [-1.4849e+00,  1.7489e-01, -2.3623e-01],\n",
       "         [ 2.2265e-01,  1.4062e-01,  1.0650e+00],\n",
       "         [ 1.9128e+00, -1.1730e+00, -5.9420e-02],\n",
       "         [-4.8511e-01, -2.0193e+00, -1.0461e+00],\n",
       "         [-3.0245e+00, -1.6174e+00, -1.0143e+00],\n",
       "         [-3.9669e+00,  3.0279e-01, -6.1320e-03],\n",
       "         [-2.5959e+00,  2.1121e+00,  1.0353e+00],\n",
       "         [ 2.0390e+00,  6.5768e-01, -2.3696e-01],\n",
       "         [ 1.7076e+00, -3.4267e-02,  1.3013e+00],\n",
       "         [-2.2540e-01,  1.8079e+00,  1.3433e+00],\n",
       "         [-2.2392e+00, -8.3551e-01, -6.6360e-01],\n",
       "         [-2.8604e+00,  3.0343e-01, -8.5908e-02],\n",
       "         [-2.1027e+00,  1.2431e+00,  5.7169e-01],\n",
       "         [-6.5259e-01,  1.0520e+00,  6.8429e-01],\n",
       "         [ 1.4849e+00, -1.7489e-01,  2.3623e-01],\n",
       "         [-8.8988e-01, -1.1255e+00, -6.3250e-01],\n",
       "         [ 1.6748e-01, -3.0713e+00, -1.7304e+00],\n",
       "         [-2.3719e+00, -2.6694e+00, -1.6986e+00],\n",
       "         [-3.3143e+00, -7.4925e-01, -6.9042e-01],\n",
       "         [-1.9433e+00,  1.0601e+00,  3.5102e-01],\n",
       "         [ 4.2719e-01,  7.5590e-01,  6.5902e-01],\n",
       "         [ 2.6916e+00, -3.9436e-01, -9.2125e-01],\n",
       "         [ 2.5654e+00, -2.2251e+00, -7.4371e-01],\n",
       "         [ 2.3602e+00, -1.0863e+00,  6.1697e-01],\n",
       "         [-1.5866e+00, -1.8876e+00, -1.3479e+00],\n",
       "         [-2.2078e+00, -7.4862e-01, -7.7020e-01],\n",
       "         [-1.4501e+00,  1.9106e-01, -1.1260e-01],\n",
       "         [ 6.5259e-01, -1.0520e+00, -6.8429e-01],\n",
       "         [ 2.1375e+00, -1.2269e+00, -4.4806e-01],\n",
       "         [-2.3729e-01, -2.1776e+00, -1.3168e+00],\n",
       "         [ 1.6176e+00, -3.2624e+00, -1.6178e+00],\n",
       "         [-9.2184e-01, -2.8605e+00, -1.5860e+00],\n",
       "         [-1.8643e+00, -9.4030e-01, -5.7782e-01],\n",
       "         [-4.9322e-01,  8.6904e-01,  4.6362e-01],\n",
       "         [ 1.8773e+00,  5.6484e-01,  7.7162e-01],\n",
       "         [-1.3650e-01, -2.0786e+00, -1.2353e+00],\n",
       "         [ 3.5876e+00, -1.4180e+00, -3.3546e-01],\n",
       "         [ 2.1027e+00, -1.2431e+00, -5.7169e-01],\n",
       "         [ 1.4501e+00, -1.9106e-01,  1.1260e-01],\n",
       "         [ 1.2128e+00, -2.3686e+00, -1.2042e+00],\n",
       "         [-7.5768e-01, -9.3967e-01, -6.5760e-01],\n",
       "         [ 2.3752e+00, -2.3227e+00, -9.6017e-01],\n",
       "         [-1.6416e-01, -1.9208e+00, -9.2844e-01],\n",
       "         [-1.1066e+00, -6.3300e-04,  7.9776e-02],\n",
       "         [ 2.6446e-01,  1.8087e+00,  1.1212e+00],\n",
       "         [ 2.6350e+00,  1.5045e+00,  1.4292e+00],\n",
       "         [ 1.9705e+00, -1.4289e+00, -5.4659e-01],\n",
       "         [ 6.2118e-01, -1.1389e+00, -5.7769e-01],\n",
       "         [ 7.5768e-01,  9.3967e-01,  6.5760e-01],\n",
       "         [ 2.2078e+00,  7.4862e-01,  7.7020e-01],\n",
       "         [ 2.8604e+00, -3.0343e-01,  8.5908e-02],\n",
       "         [-7.8534e-01, -7.8188e-01, -3.5075e-01],\n",
       "         [-1.7278e+00,  1.1383e+00,  6.5747e-01],\n",
       "         [-3.5672e-01,  2.9476e+00,  1.6989e+00],\n",
       "         [ 2.0138e+00,  2.6435e+00,  2.0069e+00],\n",
       "         [ 1.7541e+00, -1.1838e+00, -3.8247e-01],\n",
       "         [-6.2118e-01,  1.1389e+00,  5.7769e-01],\n",
       "         [ 1.3650e-01,  2.0786e+00,  1.2353e+00],\n",
       "         [ 1.5866e+00,  1.8876e+00,  1.3479e+00],\n",
       "         [ 2.2392e+00,  8.3551e-01,  6.6360e-01],\n",
       "         [ 3.7241e+00,  6.6063e-01,  8.9983e-01],\n",
       "         [ 1.3493e+00, -2.9000e-01,  3.1102e-02],\n",
       "         [ 4.0478e-01, -8.9377e-01, -4.1358e-01],\n",
       "         [-2.1346e+00, -4.9188e-01, -3.8185e-01],\n",
       "         [-3.0770e+00,  1.4283e+00,  6.2637e-01],\n",
       "         [ 6.6448e-01,  2.9335e+00,  1.9758e+00],\n",
       "         [ 2.9289e+00,  1.7832e+00,  3.9554e-01],\n",
       "         [ 2.8027e+00, -4.7523e-02,  5.7308e-01],\n",
       "         [ 2.5974e+00,  1.0913e+00,  1.9338e+00],\n",
       "         [-1.3493e+00,  2.9000e-01, -3.1102e-02],\n",
       "         [-1.9705e+00,  1.4289e+00,  5.4659e-01],\n",
       "         [-1.2128e+00,  2.3686e+00,  1.2042e+00],\n",
       "         [ 2.3729e-01,  2.1776e+00,  1.3168e+00],\n",
       "         [ 8.8988e-01,  1.1255e+00,  6.3250e-01],\n",
       "         [ 2.3748e+00,  9.5063e-01,  8.6873e-01],\n",
       "         [ 3.3142e-01,  6.9195e-01, -1.5382e+00],\n",
       "         [-2.1927e+00, -1.9850e+00, -2.3473e+00],\n",
       "         [-1.9330e+00,  1.8422e+00,  4.2048e-02],\n",
       "         [ 2.0527e-01, -1.1388e+00, -1.3607e+00],\n",
       "         [-2.5974e+00, -1.0913e+00, -1.9338e+00],\n",
       "         [-2.3602e+00,  1.0863e+00, -6.1697e-01],\n",
       "         [-1.7076e+00,  3.4267e-02, -1.3013e+00],\n",
       "         [-2.2265e-01, -1.4062e-01, -1.0650e+00],\n",
       "         [-2.3979e+00, -8.4625e-01, -9.8665e-01],\n",
       "         [-2.1382e+00,  2.9810e+00,  1.4027e+00],\n",
       "         [ 1.2615e-01,  1.8307e+00, -1.7754e-01],\n",
       "         [-2.0527e-01,  1.1388e+00,  1.3607e+00],\n",
       "         [-2.8027e+00,  4.7523e-02, -5.7308e-01],\n",
       "         [-2.5654e+00,  2.2251e+00,  7.4371e-01],\n",
       "         [-1.9128e+00,  1.1730e+00,  5.9420e-02],\n",
       "         [-4.2792e-01,  9.9815e-01,  2.9565e-01],\n",
       "         [-2.5241e+00, -2.6770e+00, -8.0911e-01],\n",
       "         [-2.2644e+00,  1.1503e+00,  1.5803e+00],\n",
       "         [-1.2615e-01, -1.8307e+00,  1.7754e-01],\n",
       "         [-3.3142e-01, -6.9195e-01,  1.5382e+00],\n",
       "         [-2.9289e+00, -1.7832e+00, -3.9554e-01],\n",
       "         [-2.6916e+00,  3.9436e-01,  9.2125e-01],\n",
       "         [-2.0390e+00, -6.5768e-01,  2.3696e-01],\n",
       "         [-5.5407e-01, -8.3257e-01,  4.7319e-01],\n",
       "         [-2.3705e+00,  3.0419e-01, -3.0800e-01],\n",
       "         [ 2.2644e+00, -1.1503e+00, -1.5803e+00],\n",
       "         [ 2.1382e+00, -2.9810e+00, -1.4027e+00],\n",
       "         [ 1.9330e+00, -1.8422e+00, -4.2048e-02],\n",
       "         [-6.6448e-01, -2.9335e+00, -1.9758e+00],\n",
       "         [-2.0138e+00, -2.6435e+00, -2.0069e+00],\n",
       "         [-2.6350e+00, -1.5045e+00, -1.4292e+00],\n",
       "         [-1.8773e+00, -5.6484e-01, -7.7162e-01],\n",
       "         [-4.2719e-01, -7.5590e-01, -6.5902e-01],\n",
       "         [ 2.2540e-01, -1.8079e+00, -1.3433e+00],\n",
       "         [ 1.7103e+00, -1.9828e+00, -1.1071e+00],\n",
       "         [-1.3710e+00, -1.8093e+00, -1.0414e+00],\n",
       "         [ 2.3705e+00, -3.0419e-01,  3.0800e-01],\n",
       "         [ 4.9322e-01, -8.6904e-01, -4.6362e-01],\n",
       "         [ 3.5672e-01, -2.9476e+00, -1.6989e+00],\n",
       "         [-2.6446e-01, -1.8087e+00, -1.1212e+00],\n",
       "         [ 1.9433e+00, -1.0601e+00, -3.5102e-01],\n",
       "         [ 2.5959e+00, -2.1121e+00, -1.0353e+00],\n",
       "         [ 9.4241e-01, -1.9202e+00, -1.0082e+00],\n",
       "         [ 1.3710e+00,  1.8093e+00,  1.0414e+00],\n",
       "         [ 3.0770e+00, -1.4283e+00, -6.2637e-01],\n",
       "         [ 1.7278e+00, -1.1383e+00, -6.5747e-01],\n",
       "         [ 1.1066e+00,  6.3300e-04, -7.9776e-02],\n",
       "         [ 1.8643e+00,  9.4030e-01,  5.7782e-01],\n",
       "         [ 3.3143e+00,  7.4925e-01,  6.9042e-01],\n",
       "         [ 3.9669e+00, -3.0279e-01,  6.1320e-03],\n",
       "         [-9.4241e-01,  1.9202e+00,  1.0082e+00],\n",
       "         [ 2.5394e+00, -4.0189e-01, -3.1724e-02],\n",
       "         [ 9.2184e-01,  2.8605e+00,  1.5860e+00],\n",
       "         [ 2.1346e+00,  4.9188e-01,  3.8185e-01],\n",
       "         [ 7.8534e-01,  7.8188e-01,  3.5075e-01],\n",
       "         [ 1.6416e-01,  1.9208e+00,  9.2844e-01],\n",
       "         [ 2.3719e+00,  2.6694e+00,  1.6986e+00],\n",
       "         [ 3.0245e+00,  1.6174e+00,  1.0143e+00],\n",
       "         [ 2.5241e+00,  2.6770e+00,  8.0911e-01],\n",
       "         [ 2.3979e+00,  8.4625e-01,  9.8665e-01],\n",
       "         [ 2.1927e+00,  1.9850e+00,  2.3473e+00],\n",
       "         [-1.6176e+00,  3.2624e+00,  1.6178e+00],\n",
       "         [-1.7541e+00,  1.1838e+00,  3.8247e-01],\n",
       "         [-2.3752e+00,  2.3227e+00,  9.6017e-01],\n",
       "         [-1.6748e-01,  3.0713e+00,  1.7304e+00],\n",
       "         [ 4.8511e-01,  2.0193e+00,  1.0461e+00],\n",
       "         [ 1.9700e+00,  1.8444e+00,  1.2823e+00],\n",
       "         [-2.5394e+00,  4.0189e-01,  3.1724e-02],\n",
       "         [-4.0478e-01,  8.9377e-01,  4.1358e-01]], grad_fn=<AddBackward0>),\n",
       " 'edge_attrs': tensor([[ 1.0000, -1.1420, -1.0692,  ...,  0.1601,  1.0261, -0.4852],\n",
       "         [ 1.0000,  0.8674,  1.3034,  ...,  0.7813, -1.2465, -0.1314],\n",
       "         [ 1.0000,  0.6585, -1.5360,  ...,  1.5198,  0.9022, -0.1463],\n",
       "         ...,\n",
       "         [ 1.0000,  1.1420,  1.0692,  ...,  0.1601,  1.0261, -0.4852],\n",
       "         [ 1.0000, -1.7106,  0.2707,  ..., -1.0361,  0.0075, -1.8886],\n",
       "         [ 1.0000, -0.6585,  1.4539,  ...,  1.2453,  1.2628,  0.0123]],\n",
       "        grad_fn=<StackBackward0>),\n",
       " 'edge_lengths': tensor([2.9878, 1.1064, 1.1255, 2.8430, 2.7015, 3.8878, 3.8722, 2.5050, 1.5137,\n",
       "         1.0971, 2.2447, 2.3253, 3.5767, 3.9785, 3.5031, 2.1555, 2.1471, 2.2636,\n",
       "         2.4804, 2.8777, 2.5087, 1.4145, 1.5137, 1.5680, 3.5292, 3.9544, 3.4674,\n",
       "         2.2413, 1.0900, 2.8721, 3.4764, 2.6704, 2.8101, 2.4552, 1.4669, 1.4145,\n",
       "         2.5050, 2.5558, 3.9846, 3.3982, 2.1664, 1.1016, 2.1068, 2.4218, 3.8722,\n",
       "         2.5087, 1.4669, 2.9208, 1.3746, 3.4581, 2.1397, 1.1094, 2.1444, 3.3540,\n",
       "         2.4947, 1.4201, 1.3746, 2.4552, 2.8777, 1.1624, 2.1710, 3.4208, 3.8821,\n",
       "         2.1504, 1.4201, 2.4218, 2.8101, 2.4804, 3.8878, 1.3805, 1.0648, 2.2236,\n",
       "         3.4497, 3.5987, 3.4517, 2.8611, 3.4172, 1.3805, 2.4947, 2.9208, 2.5558,\n",
       "         1.5680, 2.7015, 1.7189, 3.7760, 2.6706, 1.7862, 3.4172, 2.6704, 2.1471,\n",
       "         1.0971, 2.7276, 3.9276, 1.8436, 1.7862, 2.8611, 3.4764, 2.2447, 1.1255,\n",
       "         3.7672, 2.9913, 1.8436, 1.7189, 3.4517, 2.8721, 2.1555, 1.1064, 2.4097,\n",
       "         2.9913, 3.9276, 2.6706, 3.5987, 3.8821, 3.3540, 2.1068, 1.0900, 2.2636,\n",
       "         2.8430, 2.4976, 2.4097, 1.1016, 3.4208, 2.1444, 2.2413, 3.5031, 2.3647,\n",
       "         2.4976, 3.4497, 2.1710, 1.1094, 2.1664, 3.4674, 3.9785, 2.3647, 2.5712,\n",
       "         3.3982, 2.2236, 1.1624, 2.1397, 3.9544, 3.5767, 3.7672, 2.7276, 3.7760,\n",
       "         3.9846, 2.1504, 3.4581, 3.5292, 2.3253, 2.9878, 2.5712, 1.0648],\n",
       "        grad_fn=<LinalgVectorNormBackward0>),\n",
       " 'edge_embedding': tensor([[ 0.0393, -0.0551,  0.0378,  ...,  0.0550, -0.0362, -0.0042],\n",
       "         [ 0.3426,  0.4424,  0.2286,  ..., -0.3934, -0.0894,  0.2780],\n",
       "         [ 0.3408,  0.4321,  0.2072,  ..., -0.3658, -0.0419,  0.3127],\n",
       "         ...,\n",
       "         [ 0.0393, -0.0551,  0.0378,  ...,  0.0550, -0.0362, -0.0042],\n",
       "         [ 0.1032, -0.0896, -0.0256,  ..., -0.0498,  0.1146, -0.0496],\n",
       "         [ 0.3464,  0.4644,  0.2761,  ..., -0.4452, -0.1943,  0.1847]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " 'edge_cutoff': tensor([[3.2906e-01],\n",
       "         [9.9269e-01],\n",
       "         [9.9198e-01],\n",
       "         [4.2088e-01],\n",
       "         [5.1025e-01],\n",
       "         [1.1101e-03],\n",
       "         [1.6155e-03],\n",
       "         [6.2744e-01],\n",
       "         [9.6228e-01],\n",
       "         [9.9301e-01],\n",
       "         [7.6026e-01],\n",
       "         [7.2238e-01],\n",
       "         [4.4122e-02],\n",
       "         [7.6294e-06],\n",
       "         [6.6312e-02],\n",
       "         [7.9843e-01],\n",
       "         [8.0180e-01],\n",
       "         [7.5164e-01],\n",
       "         [6.4124e-01],\n",
       "         [3.9875e-01],\n",
       "         [6.2538e-01],\n",
       "         [9.7330e-01],\n",
       "         [9.6228e-01],\n",
       "         [9.5496e-01],\n",
       "         [5.7890e-02],\n",
       "         [7.6294e-05],\n",
       "         [7.8774e-02],\n",
       "         [7.6177e-01],\n",
       "         [9.9325e-01],\n",
       "         [4.0232e-01],\n",
       "         [7.5523e-02],\n",
       "         [5.2948e-01],\n",
       "         [4.4178e-01],\n",
       "         [6.5513e-01],\n",
       "         [9.6783e-01],\n",
       "         [9.7330e-01],\n",
       "         [6.2744e-01],\n",
       "         [5.9828e-01],\n",
       "         [3.8147e-06],\n",
       "         [1.0597e-01],\n",
       "         [7.9395e-01],\n",
       "         [9.9286e-01],\n",
       "         [8.1759e-01],\n",
       "         [6.7310e-01],\n",
       "         [1.6155e-03],\n",
       "         [6.2538e-01],\n",
       "         [9.6783e-01],\n",
       "         [3.7130e-01],\n",
       "         [9.7697e-01],\n",
       "         [8.2188e-02],\n",
       "         [8.0476e-01],\n",
       "         [9.9258e-01],\n",
       "         [8.0289e-01],\n",
       "         [1.2532e-01],\n",
       "         [6.3326e-01],\n",
       "         [9.7275e-01],\n",
       "         [9.7697e-01],\n",
       "         [6.5513e-01],\n",
       "         [3.9875e-01],\n",
       "         [9.9047e-01],\n",
       "         [7.9208e-01],\n",
       "         [9.6646e-02],\n",
       "         [1.2817e-03],\n",
       "         [8.0047e-01],\n",
       "         [9.7275e-01],\n",
       "         [6.7310e-01],\n",
       "         [4.4178e-01],\n",
       "         [6.4124e-01],\n",
       "         [1.1101e-03],\n",
       "         [9.7645e-01],\n",
       "         [9.9406e-01],\n",
       "         [7.6963e-01],\n",
       "         [8.5349e-02],\n",
       "         [3.8419e-02],\n",
       "         [8.4585e-02],\n",
       "         [4.0932e-01],\n",
       "         [9.8135e-02],\n",
       "         [9.7645e-01],\n",
       "         [6.3326e-01],\n",
       "         [3.7130e-01],\n",
       "         [5.9828e-01],\n",
       "         [9.5496e-01],\n",
       "         [5.1025e-01],\n",
       "         [9.2914e-01],\n",
       "         [7.9489e-03],\n",
       "         [5.2939e-01],\n",
       "         [9.1475e-01],\n",
       "         [9.8135e-02],\n",
       "         [5.2948e-01],\n",
       "         [8.0180e-01],\n",
       "         [9.9301e-01],\n",
       "         [4.9395e-01],\n",
       "         [3.0708e-04],\n",
       "         [9.0089e-01],\n",
       "         [9.1475e-01],\n",
       "         [4.0932e-01],\n",
       "         [7.5523e-02],\n",
       "         [7.6026e-01],\n",
       "         [9.9198e-01],\n",
       "         [8.8425e-03],\n",
       "         [3.2689e-01],\n",
       "         [9.0089e-01],\n",
       "         [9.2914e-01],\n",
       "         [8.4585e-02],\n",
       "         [4.0232e-01],\n",
       "         [7.9843e-01],\n",
       "         [9.9269e-01],\n",
       "         [6.7952e-01],\n",
       "         [3.2689e-01],\n",
       "         [3.0708e-04],\n",
       "         [5.2939e-01],\n",
       "         [3.8419e-02],\n",
       "         [1.2817e-03],\n",
       "         [1.2532e-01],\n",
       "         [8.1759e-01],\n",
       "         [9.9325e-01],\n",
       "         [7.5164e-01],\n",
       "         [4.2088e-01],\n",
       "         [6.3161e-01],\n",
       "         [6.7952e-01],\n",
       "         [9.9286e-01],\n",
       "         [9.6646e-02],\n",
       "         [8.0289e-01],\n",
       "         [7.6177e-01],\n",
       "         [6.6312e-02],\n",
       "         [7.0278e-01],\n",
       "         [6.3161e-01],\n",
       "         [8.5349e-02],\n",
       "         [7.9208e-01],\n",
       "         [9.9258e-01],\n",
       "         [7.9395e-01],\n",
       "         [7.8774e-02],\n",
       "         [7.6294e-06],\n",
       "         [7.0278e-01],\n",
       "         [5.8925e-01],\n",
       "         [1.0597e-01],\n",
       "         [7.6963e-01],\n",
       "         [9.9047e-01],\n",
       "         [8.0476e-01],\n",
       "         [7.6294e-05],\n",
       "         [4.4122e-02],\n",
       "         [8.8425e-03],\n",
       "         [4.9395e-01],\n",
       "         [7.9489e-03],\n",
       "         [3.8147e-06],\n",
       "         [8.0047e-01],\n",
       "         [8.2188e-02],\n",
       "         [5.7890e-02],\n",
       "         [7.2238e-01],\n",
       "         [3.2906e-01],\n",
       "         [5.8925e-01],\n",
       "         [9.9406e-01]], grad_fn=<UnsqueezeBackward0>),\n",
       " 'atomic_energy': tensor([[-369.7886],\n",
       "         [-369.7247],\n",
       "         [-369.7451],\n",
       "         [-369.7399],\n",
       "         [-369.7365],\n",
       "         [-369.7349],\n",
       "         [-369.7425],\n",
       "         [-369.8236],\n",
       "         [-369.8173],\n",
       "         [-369.8175],\n",
       "         [-369.8268],\n",
       "         [-369.8220],\n",
       "         [-369.8195],\n",
       "         [-369.8194],\n",
       "         [-369.8294]], dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " 'total_energy': tensor([[-5546.7877]], dtype=torch.float64, grad_fn=<SumBackward1>),\n",
       " 'forces': tensor([[ 0.0956,  0.0177,  0.0425],\n",
       "         [-0.0075,  0.0003, -0.0305],\n",
       "         [ 0.1460,  0.1228,  0.1171],\n",
       "         [-0.0749,  0.1973,  0.1061],\n",
       "         [-0.2427,  0.0029, -0.0019],\n",
       "         [-0.1177, -0.1139, -0.0666],\n",
       "         [ 0.1469, -0.1949, -0.0778],\n",
       "         [ 0.0034, -0.0282, -0.1561],\n",
       "         [-0.0304,  0.1401,  0.0485],\n",
       "         [-0.0730, -0.1219,  0.0917],\n",
       "         [ 0.0005, -0.1025, -0.0844],\n",
       "         [ 0.0293, -0.0741, -0.0467],\n",
       "         [ 0.0976,  0.0090, -0.0074],\n",
       "         [ 0.0411,  0.0344,  0.0101],\n",
       "         [-0.0140,  0.1110,  0.0551]], dtype=torch.float64,\n",
       "        grad_fn=<ToCopyBackward0>),\n",
       " 'stress': tensor([[[-inf, inf, -inf],\n",
       "          [inf, inf, -inf],\n",
       "          [-inf, -inf, inf]]], dtype=torch.float64, grad_fn=<ToCopyBackward0>),\n",
       " 'virial': tensor([[[ 0.2496, -0.0553,  0.1126],\n",
       "          [-0.0553, -0.1758,  0.0179],\n",
       "          [ 0.1126,  0.0179, -0.2129]]], dtype=torch.float64,\n",
       "        grad_fn=<ToCopyBackward0>)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1024646-ad08-4d60-ab43-c590b71e7084",
   "metadata": {},
   "source": [
    "## GraphModuleMixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3defaad2-f327-4be7-b137-2d70591d5357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Dict, Tuple, Callable, Any, Sequence, Union, Mapping, Optional\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "\n",
    "from e3nn import o3\n",
    "\n",
    "from nequip.data import AtomicDataDict\n",
    "from nequip.utils import instantiate\n",
    "\n",
    "\n",
    "class GraphModuleMixin:\n",
    "    r\"\"\"Mixin parent class for ``torch.nn.Module``s that act on and return ``AtomicDataDict.Type`` graph data.\n",
    "\n",
    "    All such classes should call ``_init_irreps`` in their ``__init__`` functions with information on the data fields they expect,\n",
    "    require, and produce, as well as their corresponding irreps.\n",
    "    \"\"\"\n",
    "\n",
    "    def _init_irreps(\n",
    "        self,\n",
    "        irreps_in: Dict[str, Any] = {},\n",
    "        my_irreps_in: Dict[str, Any] = {},\n",
    "        required_irreps_in: Sequence[str] = [],\n",
    "        irreps_out: Dict[str, Any] = {},\n",
    "    ):\n",
    "        \"\"\"Setup the expected data fields and their irreps for this graph module.\n",
    "\n",
    "        ``None`` is a valid irreps in the context for anything that is invariant but not well described by an ``e3nn.o3.Irreps``.\n",
    "        An example are edge indexes in a graph, which are invariant but are integers, not ``0e`` scalars.\n",
    "\n",
    "        Args:\n",
    "            irreps_in (dict): maps names of all input fields from previous modules or\n",
    "                data to their corresponding irreps\n",
    "            my_irreps_in (dict): maps names of fields to the irreps they must have for\n",
    "                this graph module. Will be checked for consistancy with ``irreps_in``\n",
    "            required_irreps_in: sequence of names of fields that must be present in\n",
    "                ``irreps_in``, but that can have any irreps.\n",
    "            irreps_out (dict): mapping names of fields that are modified/output by\n",
    "                this graph module to their irreps.\n",
    "        \"\"\"\n",
    "        # Coerce\n",
    "        irreps_in = {} if irreps_in is None else irreps_in\n",
    "        irreps_in = AtomicDataDict._fix_irreps_dict(irreps_in)\n",
    "        # positions are *always* 1o, and always present\n",
    "        if AtomicDataDict.POSITIONS_KEY in irreps_in:\n",
    "            if irreps_in[AtomicDataDict.POSITIONS_KEY] != o3.Irreps(\"1x1o\"):\n",
    "                raise ValueError(\n",
    "                    f\"Positions must have irreps 1o, got instead `{irreps_in[AtomicDataDict.POSITIONS_KEY]}`\"\n",
    "                )\n",
    "        irreps_in[AtomicDataDict.POSITIONS_KEY] = o3.Irreps(\"1o\")\n",
    "        # edges are also always present\n",
    "        if AtomicDataDict.EDGE_INDEX_KEY in irreps_in:\n",
    "            if irreps_in[AtomicDataDict.EDGE_INDEX_KEY] is not None:\n",
    "                raise ValueError(\n",
    "                    f\"Edge indexes must have irreps None, got instead `{irreps_in[AtomicDataDict.EDGE_INDEX_KEY]}`\"\n",
    "                )\n",
    "        irreps_in[AtomicDataDict.EDGE_INDEX_KEY] = None\n",
    "\n",
    "        my_irreps_in = AtomicDataDict._fix_irreps_dict(my_irreps_in)\n",
    "\n",
    "        irreps_out = AtomicDataDict._fix_irreps_dict(irreps_out)\n",
    "        # Confirm compatibility:\n",
    "        # with my_irreps_in\n",
    "        for k in my_irreps_in:\n",
    "            if k in irreps_in and irreps_in[k] != my_irreps_in[k]:\n",
    "                raise ValueError(\n",
    "                    f\"The given input irreps {irreps_in[k]} for field '{k}' is incompatible with this configuration {type(self)}; should have been {my_irreps_in[k]}\"\n",
    "                )\n",
    "        # with required_irreps_in\n",
    "        for k in required_irreps_in:\n",
    "            if k not in irreps_in:\n",
    "                raise ValueError(\n",
    "                    f\"This {type(self)} requires field '{k}' to be in irreps_in\"\n",
    "                )\n",
    "        # Save stuff\n",
    "        self.irreps_in = irreps_in\n",
    "        # The output irreps of any graph module are whatever inputs it has, overwritten with whatever outputs it has.\n",
    "        new_out = irreps_in.copy()\n",
    "        new_out.update(irreps_out)\n",
    "        self.irreps_out = new_out\n",
    "\n",
    "    def _add_independent_irreps(self, irreps: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Insert some independent irreps that need to be exposed to the self.irreps_in and self.irreps_out.\n",
    "        The terms that have already appeared in the irreps_in will be removed.\n",
    "\n",
    "        Args:\n",
    "            irreps (dict): maps names of all new fields\n",
    "        \"\"\"\n",
    "\n",
    "        irreps = {\n",
    "            key: irrep for key, irrep in irreps.items() if key not in self.irreps_in\n",
    "        }\n",
    "        irreps_in = AtomicDataDict._fix_irreps_dict(irreps)\n",
    "        irreps_out = AtomicDataDict._fix_irreps_dict(\n",
    "            {key: irrep for key, irrep in irreps.items() if key not in self.irreps_out}\n",
    "        )\n",
    "        self.irreps_in.update(irreps_in)\n",
    "        self.irreps_out.update(irreps_out)\n",
    "\n",
    "    def _make_tracing_inputs(self, n):\n",
    "        # We impliment this to be able to trace graph modules\n",
    "        out = []\n",
    "        for _ in range(n):\n",
    "            batch = random.randint(1, 4)\n",
    "            # TODO: handle None case\n",
    "            # TODO: do only required inputs\n",
    "            # TODO: dummy input if empty?\n",
    "            out.append(\n",
    "                {\n",
    "                    \"forward\": (\n",
    "                        {\n",
    "                            k: i.randn(batch, -1)\n",
    "                            for k, i in self.irreps_in.items()\n",
    "                            if i is not None\n",
    "                        },\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "        return out\n",
    "\n",
    "\n",
    "class SequentialGraphNetwork(GraphModuleMixin, torch.nn.Sequential):\n",
    "    r\"\"\"A ``torch.nn.Sequential`` of ``GraphModuleMixin``s.\n",
    "\n",
    "    Args:\n",
    "        modules (list or dict of ``GraphModuleMixin``s): the sequence of graph modules. If a list, the modules will be named ``\"module0\", \"module1\", ...``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        modules: Union[Sequence[GraphModuleMixin], Dict[str, GraphModuleMixin]],\n",
    "    ):\n",
    "        if isinstance(modules, dict):\n",
    "            module_list = list(modules.values())\n",
    "        else:\n",
    "            module_list = list(modules)\n",
    "        # check in/out irreps compatible\n",
    "        for m1, m2 in zip(module_list, module_list[1:]):\n",
    "            assert AtomicDataDict._irreps_compatible(\n",
    "                m1.irreps_out, m2.irreps_in\n",
    "            ), f\"Incompatible irreps_out from {type(m1).__name__} for input to {type(m2).__name__}: {m1.irreps_out} -> {m2.irreps_in}\"\n",
    "        self._init_irreps(\n",
    "            irreps_in=module_list[0].irreps_in,\n",
    "            my_irreps_in=module_list[0].irreps_in,\n",
    "            irreps_out=module_list[-1].irreps_out,\n",
    "        )\n",
    "        # torch.nn.Sequential will name children correctly if passed an OrderedDict\n",
    "        if isinstance(modules, dict):\n",
    "            modules = OrderedDict(modules)\n",
    "        else:\n",
    "            modules = OrderedDict((f\"module{i}\", m) for i, m in enumerate(module_list))\n",
    "        super().__init__(modules)\n",
    "\n",
    "    @classmethod\n",
    "    def from_parameters(\n",
    "        cls,\n",
    "        shared_params: Mapping,\n",
    "        layers: Dict[str, Union[Callable, Tuple[Callable, Dict[str, Any]]]],\n",
    "        irreps_in: Optional[dict] = None,\n",
    "    ):\n",
    "        r\"\"\"Construct a ``SequentialGraphModule`` of modules built from a shared set of parameters.\n",
    "\n",
    "        For some layer, a parameter with name ``param`` will be taken, in order of priority, from:\n",
    "          1. The specific value in the parameter dictionary for that layer, if provided\n",
    "          2. ``name_param`` in ``shared_params`` where ``name`` is the name of the layer\n",
    "          3. ``param`` in ``shared_params``\n",
    "\n",
    "        Args:\n",
    "            shared_params (dict-like): shared parameters from which to pull when instantiating the module\n",
    "            layers (dict): dictionary mapping unique names of layers to either:\n",
    "                  1. A callable (such as a class or function) that can be used to ``instantiate`` a module for that layer\n",
    "                  2. A tuple of such a callable and a dictionary mapping parameter names to values. The given dictionary of parameters will override for this layer values found in ``shared_params``.\n",
    "                Options 1. and 2. can be mixed.\n",
    "            irreps_in (optional dict): ``irreps_in`` for the first module in the sequence.\n",
    "\n",
    "        Returns:\n",
    "            The constructed SequentialGraphNetwork.\n",
    "        \"\"\"\n",
    "        # note that dictionary ordered gueranteed in >=3.7, so its fine to do an ordered sequential as a dict.\n",
    "        built_modules = []\n",
    "        for name, builder in layers.items():\n",
    "            if not isinstance(name, str):\n",
    "                raise ValueError(f\"`'name'` must be a str; got `{name}`\")\n",
    "            if isinstance(builder, tuple):\n",
    "                builder, params = builder\n",
    "            else:\n",
    "                params = {}\n",
    "            if not callable(builder):\n",
    "                raise TypeError(\n",
    "                    f\"The builder has to be a class or a function. got {type(builder)}\"\n",
    "                )\n",
    "\n",
    "            instance, _ = instantiate(\n",
    "                builder=builder,\n",
    "                prefix=name,\n",
    "                positional_args=(\n",
    "                    dict(\n",
    "                        irreps_in=(\n",
    "                            built_modules[-1].irreps_out\n",
    "                            if len(built_modules) > 0\n",
    "                            else irreps_in\n",
    "                        )\n",
    "                    )\n",
    "                ),\n",
    "                optional_args=params,\n",
    "                all_args=shared_params,\n",
    "            )\n",
    "\n",
    "            if not isinstance(instance, GraphModuleMixin):\n",
    "                raise TypeError(\n",
    "                    f\"Builder `{builder}` for layer with name `{name}` did not return a GraphModuleMixin, instead got a {type(instance).__name__}\"\n",
    "                )\n",
    "\n",
    "            built_modules.append(instance)\n",
    "\n",
    "        return cls(\n",
    "            OrderedDict(zip(layers.keys(), built_modules)),\n",
    "        )\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def append(self, name: str, module: GraphModuleMixin) -> None:\n",
    "        r\"\"\"Append a module to the SequentialGraphNetwork.\n",
    "\n",
    "        Args:\n",
    "            name (str): the name for the module\n",
    "            module (GraphModuleMixin): the module to append\n",
    "        \"\"\"\n",
    "        assert AtomicDataDict._irreps_compatible(self.irreps_out, module.irreps_in)\n",
    "        self.add_module(name, module)\n",
    "        self.irreps_out = dict(module.irreps_out)\n",
    "        return\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def append_from_parameters(\n",
    "        self,\n",
    "        shared_params: Mapping,\n",
    "        name: str,\n",
    "        builder: Callable,\n",
    "        params: Dict[str, Any] = {},\n",
    "    ) -> GraphModuleMixin:\n",
    "        r\"\"\"Build a module from parameters and append it.\n",
    "\n",
    "        Args:\n",
    "            shared_params (dict-like): shared parameters from which to pull when instantiating the module\n",
    "            name (str): the name for the module\n",
    "            builder (callable): a class or function to build a module\n",
    "            params (dict, optional): extra specific parameters for this module that take priority over those in ``shared_params``\n",
    "\n",
    "        Returns:\n",
    "            the build module\n",
    "        \"\"\"\n",
    "        instance, _ = instantiate(\n",
    "            builder=builder,\n",
    "            prefix=name,\n",
    "            positional_args=(dict(irreps_in=self[-1].irreps_out)),\n",
    "            optional_args=params,\n",
    "            all_args=shared_params,\n",
    "        )\n",
    "        self.append(name, instance)\n",
    "        return instance\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def insert(\n",
    "        self,\n",
    "        name: str,\n",
    "        module: GraphModuleMixin,\n",
    "        after: Optional[str] = None,\n",
    "        before: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Insert a module after the module with name ``after``.\n",
    "\n",
    "        Args:\n",
    "            name: the name of the module to insert\n",
    "            module: the moldule to insert\n",
    "            after: the module to insert after\n",
    "            before: the module to insert before\n",
    "        \"\"\"\n",
    "\n",
    "        if (before is None) is (after is None):\n",
    "            raise ValueError(\"Only one of before or after argument needs to be defined\")\n",
    "        elif before is None:\n",
    "            insert_location = after\n",
    "        else:\n",
    "            insert_location = before\n",
    "\n",
    "        # This checks names, etc.\n",
    "        self.add_module(name, module)\n",
    "        # Now insert in the right place by overwriting\n",
    "        names = list(self._modules.keys())\n",
    "        modules = list(self._modules.values())\n",
    "        idx = names.index(insert_location)\n",
    "        if before is None:\n",
    "            idx += 1\n",
    "        names.insert(idx, name)\n",
    "        modules.insert(idx, module)\n",
    "\n",
    "        self._modules = OrderedDict(zip(names, modules))\n",
    "\n",
    "        module_list = list(self._modules.values())\n",
    "\n",
    "        # sanity check the compatibility\n",
    "        if idx > 0:\n",
    "            assert AtomicDataDict._irreps_compatible(\n",
    "                module_list[idx - 1].irreps_out, module.irreps_in\n",
    "            )\n",
    "        if len(module_list) > idx:\n",
    "            assert AtomicDataDict._irreps_compatible(\n",
    "                module_list[idx + 1].irreps_in, module.irreps_out\n",
    "            )\n",
    "\n",
    "        # insert the new irreps_out to the later modules\n",
    "        for module_id, next_module in enumerate(module_list[idx + 1 :]):\n",
    "            next_module._add_independent_irreps(module.irreps_out)\n",
    "\n",
    "        # update the final wrapper irreps_out\n",
    "        self.irreps_out = dict(module_list[-1].irreps_out)\n",
    "\n",
    "        return\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def insert_from_parameters(\n",
    "        self,\n",
    "        shared_params: Mapping,\n",
    "        name: str,\n",
    "        builder: Callable,\n",
    "        params: Dict[str, Any] = {},\n",
    "        after: Optional[str] = None,\n",
    "        before: Optional[str] = None,\n",
    "    ) -> GraphModuleMixin:\n",
    "        r\"\"\"Build a module from parameters and insert it after ``after``.\n",
    "\n",
    "        Args:\n",
    "            shared_params (dict-like): shared parameters from which to pull when instantiating the module\n",
    "            name (str): the name for the module\n",
    "            builder (callable): a class or function to build a module\n",
    "            params (dict, optional): extra specific parameters for this module that take priority over those in ``shared_params``\n",
    "            after: the name of the module to insert after\n",
    "            before: the name of the module to insert before\n",
    "\n",
    "        Returns:\n",
    "            the inserted module\n",
    "        \"\"\"\n",
    "        if (before is None) is (after is None):\n",
    "            raise ValueError(\"Only one of before or after argument needs to be defined\")\n",
    "        elif before is None:\n",
    "            insert_location = after\n",
    "        else:\n",
    "            insert_location = before\n",
    "        idx = list(self._modules.keys()).index(insert_location) - 1\n",
    "        if before is None:\n",
    "            idx += 1\n",
    "        instance, _ = instantiate(\n",
    "            builder=builder,\n",
    "            prefix=name,\n",
    "            positional_args=(dict(irreps_in=self[idx].irreps_out)),\n",
    "            optional_args=params,\n",
    "            all_args=shared_params,\n",
    "        )\n",
    "        self.insert(after=after, before=before, name=name, module=instance)\n",
    "        return instance\n",
    "\n",
    "    # Copied from https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#Sequential\n",
    "    # with type annotations added\n",
    "    def forward(self, input: AtomicDataDict.Type) -> AtomicDataDict.Type:\n",
    "        for module in self:\n",
    "            input = module(input)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4244979e-aa79-4e3d-811c-72931efd3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional\n",
    "\n",
    "from e3nn.o3 import Irreps\n",
    "from e3nn.util.jit import compile_mode\n",
    "\n",
    "from nequip.data import AtomicDataDict\n",
    "#from .._graph_mixin import GraphModuleMixin\n",
    "\n",
    "\n",
    "@compile_mode(\"script\")\n",
    "class OneHotAtomEncoding(GraphModuleMixin, torch.nn.Module):\n",
    "    \"\"\"Copmute a one-hot floating point encoding of atoms' discrete atom types.\n",
    "\n",
    "    Args:\n",
    "        set_features: If ``True`` (default), ``node_features`` will be set in addition to ``node_attrs``.\n",
    "    \"\"\"\n",
    "\n",
    "    num_types: int\n",
    "    set_features: bool\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_types: int,\n",
    "        set_features: bool = True,\n",
    "        irreps_in=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_types = num_types\n",
    "        self.set_features = set_features\n",
    "        # Output irreps are num_types even (invariant) scalars\n",
    "        irreps_out = {AtomicDataDict.NODE_ATTRS_KEY: Irreps([(self.num_types, (0, 1))])}\n",
    "        if self.set_features:\n",
    "            irreps_out[AtomicDataDict.NODE_FEATURES_KEY] = irreps_out[\n",
    "                AtomicDataDict.NODE_ATTRS_KEY\n",
    "            ]\n",
    "        self._init_irreps(irreps_in=irreps_in, irreps_out=irreps_out)\n",
    "\n",
    "    def forward(self, data: AtomicDataDict.Type) -> AtomicDataDict.Type:\n",
    "        type_numbers = data[AtomicDataDict.ATOM_TYPE_KEY].squeeze(-1)\n",
    "        one_hot = torch.nn.functional.one_hot(\n",
    "            type_numbers, num_classes=self.num_types\n",
    "        ).to(device=type_numbers.device, dtype=data[AtomicDataDict.POSITIONS_KEY].dtype)\n",
    "        data[AtomicDataDict.NODE_ATTRS_KEY] = one_hot\n",
    "        if self.set_features:\n",
    "            data[AtomicDataDict.NODE_FEATURES_KEY] = one_hot\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f70c482-b589-42ea-bc34-1272750c4a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['edge_index', 'pos', 'cell', 'forces', 'pbc', 'edge_cell_shift', 'total_energy', 'atom_types', 'node_attrs', 'node_features'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot = OneHotAtomEncoding(2)\n",
    "\n",
    "out1 = one_hot(AtomicData.to_AtomicDataDict(dataset[0]))\n",
    "\n",
    "out1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7656b18-8618-4c74-a6bd-c2c94bbb6331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'pos': 1x1o, 'edge_index': None},\n",
       " {'pos': 1x1o, 'edge_index': None, 'node_attrs': 2x0e, 'node_features': 2x0e})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.irreps_in, one_hot.irreps_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6da3e9f8-31d6-4652-9668-44f455cbbaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from e3nn.math import soft_one_hot_linspace\n",
    "from e3nn.util.jit import compile_mode\n",
    "\n",
    "class BesselBasis(nn.Module):\n",
    "    r_max: float\n",
    "    prefactor: float\n",
    "\n",
    "    def __init__(self, r_max, num_basis=8, trainable=True):\n",
    "        r\"\"\"Radial Bessel Basis, as proposed in DimeNet: https://arxiv.org/abs/2003.03123\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        r_max : float\n",
    "            Cutoff radius\n",
    "\n",
    "        num_basis : int\n",
    "            Number of Bessel Basis functions\n",
    "\n",
    "        trainable : bool\n",
    "            Train the :math:`n \\pi` part or not.\n",
    "        \"\"\"\n",
    "        super(BesselBasis, self).__init__()\n",
    "\n",
    "        self.trainable = trainable\n",
    "        self.num_basis = num_basis\n",
    "\n",
    "        self.r_max = float(r_max)\n",
    "        self.prefactor = 2.0 / self.r_max\n",
    "\n",
    "        bessel_weights = (\n",
    "            torch.linspace(start=1.0, end=num_basis, steps=num_basis) * math.pi\n",
    "        )\n",
    "        if self.trainable:\n",
    "            self.bessel_weights = nn.Parameter(bessel_weights)\n",
    "        else:\n",
    "            self.register_buffer(\"bessel_weights\", bessel_weights)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Evaluate Bessel Basis for input x.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input\n",
    "        \"\"\"\n",
    "        numerator = torch.sin(self.bessel_weights * x.unsqueeze(-1) / self.r_max)\n",
    "\n",
    "        return self.prefactor * (numerator / x.unsqueeze(-1))\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def _poly_cutoff(x: torch.Tensor, factor: float, p: float = 6.0) -> torch.Tensor:\n",
    "    x = x * factor\n",
    "\n",
    "    out = 1.0\n",
    "    out = out - (((p + 1.0) * (p + 2.0) / 2.0) * torch.pow(x, p))\n",
    "    out = out + (p * (p + 2.0) * torch.pow(x, p + 1.0))\n",
    "    out = out - ((p * (p + 1.0) / 2) * torch.pow(x, p + 2.0))\n",
    "\n",
    "    return out * (x < 1.0)\n",
    "\n",
    "\n",
    "class PolynomialCutoff(torch.nn.Module):\n",
    "    _factor: float\n",
    "    p: float\n",
    "\n",
    "    def __init__(self, r_max: float, p: float = 6):\n",
    "        r\"\"\"Polynomial cutoff, as proposed in DimeNet: https://arxiv.org/abs/2003.03123\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        r_max : float\n",
    "            Cutoff radius\n",
    "\n",
    "        p : int\n",
    "            Power used in envelope function\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert p >= 2.0\n",
    "        self.p = float(p)\n",
    "        self._factor = 1.0 / float(r_max)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Evaluate cutoff function.\n",
    "\n",
    "        x: torch.Tensor, input distance\n",
    "        \"\"\"\n",
    "        return _poly_cutoff(x, self._factor, p=self.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d536485-1520-4076-ba34-966ceabc4500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "\n",
    "from e3nn import o3\n",
    "from e3nn.util.jit import compile_mode\n",
    "\n",
    "from nequip.data import AtomicDataDict\n",
    "#from .._graph_mixin import GraphModuleMixin\n",
    "#from ..radial_basis import BesselBasis\n",
    "#from ..cutoffs import PolynomialCutoff\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@compile_mode(\"script\")\n",
    "class SphericalHarmonicEdgeAttrs(GraphModuleMixin, torch.nn.Module):\n",
    "    \"\"\"Construct edge attrs as spherical harmonic projections of edge vectors.\n",
    "\n",
    "    Parameters follow ``e3nn.o3.spherical_harmonics``.\n",
    "\n",
    "    Args:\n",
    "        irreps_edge_sh (int, str, or o3.Irreps): if int, will be treated as lmax for o3.Irreps.spherical_harmonics(lmax)\n",
    "        edge_sh_normalization (str): the normalization scheme to use\n",
    "        edge_sh_normalize (bool, default: True): whether to normalize the spherical harmonics\n",
    "        out_field (str, default: AtomicDataDict.EDGE_ATTRS_KEY: data/irreps field\n",
    "    \"\"\"\n",
    "\n",
    "    out_field: str\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        irreps_edge_sh: Union[int, str, o3.Irreps],\n",
    "        edge_sh_normalization: str = \"component\",\n",
    "        edge_sh_normalize: bool = True,\n",
    "        irreps_in=None,\n",
    "        out_field: str = AtomicDataDict.EDGE_ATTRS_KEY,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.out_field = out_field\n",
    "\n",
    "        if isinstance(irreps_edge_sh, int):\n",
    "            self.irreps_edge_sh = o3.Irreps.spherical_harmonics(irreps_edge_sh)\n",
    "        else:\n",
    "            self.irreps_edge_sh = o3.Irreps(irreps_edge_sh)\n",
    "        self._init_irreps(\n",
    "            irreps_in=irreps_in,\n",
    "            irreps_out={out_field: self.irreps_edge_sh},\n",
    "        )\n",
    "        self.sh = o3.SphericalHarmonics(\n",
    "            self.irreps_edge_sh, edge_sh_normalize, edge_sh_normalization\n",
    "        )\n",
    "\n",
    "    def forward(self, data: AtomicDataDict.Type) -> AtomicDataDict.Type:\n",
    "        data = AtomicDataDict.with_edge_vectors(data, with_lengths=False)\n",
    "        edge_vec = data[AtomicDataDict.EDGE_VECTORS_KEY]\n",
    "        edge_sh = self.sh(edge_vec)\n",
    "        data[self.out_field] = edge_sh\n",
    "        return data\n",
    "\n",
    "\n",
    "@compile_mode(\"script\")\n",
    "class RadialBasisEdgeEncoding(GraphModuleMixin, torch.nn.Module):\n",
    "    out_field: str\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        basis=BesselBasis,\n",
    "        cutoff=PolynomialCutoff,\n",
    "        basis_kwargs={},\n",
    "        cutoff_kwargs={},\n",
    "        out_field: str = AtomicDataDict.EDGE_EMBEDDING_KEY,\n",
    "        irreps_in=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.basis = basis(**basis_kwargs)\n",
    "        self.cutoff = cutoff(**cutoff_kwargs)\n",
    "        self.out_field = out_field\n",
    "        self._init_irreps(\n",
    "            irreps_in=irreps_in,\n",
    "            irreps_out={\n",
    "                self.out_field: o3.Irreps([(self.basis.num_basis, (0, 1))]),\n",
    "                AtomicDataDict.EDGE_CUTOFF_KEY: \"0e\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def forward(self, data: AtomicDataDict.Type) -> AtomicDataDict.Type:\n",
    "        data = AtomicDataDict.with_edge_vectors(data, with_lengths=True)\n",
    "        edge_length = data[AtomicDataDict.EDGE_LENGTH_KEY]\n",
    "        cutoff = self.cutoff(edge_length).unsqueeze(-1)\n",
    "        edge_length_embedded = self.basis(edge_length) * cutoff\n",
    "        data[self.out_field] = edge_length_embedded\n",
    "        data[AtomicDataDict.EDGE_CUTOFF_KEY] = cutoff\n",
    "        return data\n",
    "\n",
    "\n",
    "@compile_mode(\"script\")\n",
    "class AddRadialCutoffToData(GraphModuleMixin, torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cutoff=PolynomialCutoff,\n",
    "        cutoff_kwargs={},\n",
    "        irreps_in=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cutoff = cutoff(**cutoff_kwargs)\n",
    "        self._init_irreps(\n",
    "            irreps_in=irreps_in, irreps_out={AtomicDataDict.EDGE_CUTOFF_KEY: \"0e\"}\n",
    "        )\n",
    "\n",
    "    def forward(self, data: AtomicDataDict.Type) -> AtomicDataDict.Type:\n",
    "        data = AtomicDataDict.with_edge_vectors(data, with_lengths=True)\n",
    "        edge_length = data[AtomicDataDict.EDGE_LENGTH_KEY]\n",
    "        cutoff = self.cutoff(edge_length).unsqueeze(-1)\n",
    "        data[AtomicDataDict.EDGE_CUTOFF_KEY] = cutoff\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82c9edfd-3f04-44f1-86a0-dbe2a70d4aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['edge_index', 'pos', 'cell', 'forces', 'pbc', 'edge_cell_shift', 'total_energy', 'atom_types', 'node_attrs', 'node_features', 'edge_vectors', 'edge_attrs'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pos': 1x1o, 'edge_index': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sh = SphericalHarmonicEdgeAttrs(2)\n",
    "\n",
    "out2 = sh(out1); print(out2.keys())\n",
    "sh.irreps_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b41fec7f-13d6-48a6-b54a-bd316d0a0c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([152, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([152, 9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(out2['edge_vectors'].shape)\n",
    "out2['edge_attrs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2dfde7b-5a4d-4662-93c0-ccfd632ff06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000, -1.1420, -1.0692,  ...,  0.1601,  1.0261, -0.4852],\n",
       "        [ 1.0000,  0.8674,  1.3034,  ...,  0.7813, -1.2465, -0.1314],\n",
       "        [ 1.0000,  0.6585, -1.5360,  ...,  1.5198,  0.9022, -0.1463],\n",
       "        ...,\n",
       "        [ 1.0000,  1.1420,  1.0692,  ...,  0.1601,  1.0261, -0.4852],\n",
       "        [ 1.0000, -1.7106,  0.2707,  ..., -1.0361,  0.0075, -1.8886],\n",
       "        [ 1.0000, -0.6585,  1.4539,  ...,  1.2453,  1.2628,  0.0123]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2['edge_attrs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb32cb9c-4be4-4598-80a4-b88d4094e590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pos': 1x1o, 'edge_index': None, 'edge_embedding': 6x0e, 'edge_cutoff': 1x0e}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rad_basis = RadialBasisEdgeEncoding(basis_kwargs={'num_basis': 6, 'r_max': 5}, cutoff_kwargs={'r_max': 5})\n",
    "\n",
    "out3 = rad_basis(out2)\n",
    "\n",
    "out3.keys()\n",
    "\n",
    "rad_basis.irreps_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79c2013c-aed4-4ecb-b7c5-8a2814d8d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e3nn.o3 import Linear\n",
    "\n",
    "class AtomwiseLinear(GraphModuleMixin, torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        field: str = AtomicDataDict.NODE_FEATURES_KEY,\n",
    "        out_field: Optional[str] = None,\n",
    "        irreps_in=None,\n",
    "        irreps_out=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.field = field\n",
    "        out_field = out_field if out_field is not None else field\n",
    "        self.out_field = out_field\n",
    "        if irreps_out is None:\n",
    "            irreps_out = irreps_in[field]\n",
    "\n",
    "        self._init_irreps(\n",
    "            irreps_in=irreps_in,\n",
    "            required_irreps_in=[field],\n",
    "            irreps_out={out_field: irreps_out},\n",
    "        )\n",
    "        self.linear = Linear(\n",
    "            irreps_in=self.irreps_in[field], irreps_out=self.irreps_out[out_field]\n",
    "        )\n",
    "\n",
    "    def forward(self, data: AtomicDataDict.Type) -> AtomicDataDict.Type:\n",
    "        data[self.out_field] = self.linear(data[self.field])\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "616c4a52-bf20-4e27-b12c-2d8f4f00fb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['edge_index', 'pos', 'cell', 'forces', 'pbc', 'edge_cell_shift', 'total_energy', 'atom_types', 'node_attrs', 'node_features', 'edge_vectors', 'edge_attrs', 'edge_lengths', 'edge_embedding', 'edge_cutoff'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pos': 1x1o, 'edge_index': None, 'node_attrs': 2x0e, 'node_features': 32x0e}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "al = AtomwiseLinear(irreps_in=one_hot.irreps_out, irreps_out=o3.Irreps([(32, (0, 1))]))\n",
    "\n",
    "out4 = rad_basis(out3)\n",
    "\n",
    "print(out4.keys())\n",
    "\n",
    "al.irreps_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8d9934-1aa8-412d-b8aa-ff2dcc16fd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Callable\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "from e3nn import o3\n",
    "from e3nn.nn import Gate, NormActivation\n",
    "\n",
    "from nequip.data import AtomicDataDict\n",
    "from nequip.nn import (\n",
    "    GraphModuleMixin,\n",
    "    InteractionBlock,\n",
    ")\n",
    "from nequip.nn.nonlinearities import ShiftedSoftPlus\n",
    "from nequip.utils.tp_utils import tp_path_exists\n",
    "\n",
    "\n",
    "acts = {\n",
    "    \"abs\": torch.abs,\n",
    "    \"tanh\": torch.tanh,\n",
    "    \"ssp\": ShiftedSoftPlus,\n",
    "    \"silu\": torch.nn.functional.silu,\n",
    "}\n",
    "\n",
    "\n",
    "class ConvNetLayer(GraphModuleMixin, torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    resnet: bool\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        irreps_in,\n",
    "        feature_irreps_hidden,\n",
    "        convolution=InteractionBlock,\n",
    "        convolution_kwargs: dict = {},\n",
    "        num_layers: int = 3,\n",
    "        resnet: bool = False,\n",
    "        nonlinearity_type: str = \"gate\",\n",
    "        nonlinearity_scalars: Dict[int, Callable] = {\"e\": \"silu\", \"o\": \"tanh\"},\n",
    "        nonlinearity_gates: Dict[int, Callable] = {\"e\": \"silu\", \"o\": \"tanh\"},\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # initialization\n",
    "        assert nonlinearity_type in (\"gate\", \"norm\")\n",
    "        # make the nonlin dicts from parity ints instead of convinience strs\n",
    "        nonlinearity_scalars = {\n",
    "            1: nonlinearity_scalars[\"e\"],\n",
    "            -1: nonlinearity_scalars[\"o\"],\n",
    "        }\n",
    "        nonlinearity_gates = {\n",
    "            1: nonlinearity_gates[\"e\"],\n",
    "            -1: nonlinearity_gates[\"o\"],\n",
    "        }\n",
    "\n",
    "        self.feature_irreps_hidden = o3.Irreps(feature_irreps_hidden)\n",
    "        self.resnet = resnet\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # We'll set irreps_out later when we know them\n",
    "        self._init_irreps(\n",
    "            irreps_in=irreps_in,\n",
    "            required_irreps_in=[AtomicDataDict.NODE_FEATURES_KEY],\n",
    "        )\n",
    "\n",
    "        edge_attr_irreps = self.irreps_in[AtomicDataDict.EDGE_ATTRS_KEY]\n",
    "        irreps_layer_out_prev = self.irreps_in[AtomicDataDict.NODE_FEATURES_KEY]\n",
    "\n",
    "        irreps_scalars = o3.Irreps(\n",
    "            [\n",
    "                (mul, ir)\n",
    "                for mul, ir in self.feature_irreps_hidden\n",
    "                if ir.l == 0\n",
    "                and tp_path_exists(irreps_layer_out_prev, edge_attr_irreps, ir)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        irreps_gated = o3.Irreps(\n",
    "            [\n",
    "                (mul, ir)\n",
    "                for mul, ir in self.feature_irreps_hidden\n",
    "                if ir.l > 0\n",
    "                and tp_path_exists(irreps_layer_out_prev, edge_attr_irreps, ir)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        irreps_layer_out = (irreps_scalars + irreps_gated).simplify()\n",
    "\n",
    "        if nonlinearity_type == \"gate\":\n",
    "            ir = (\n",
    "                \"0e\"\n",
    "                if tp_path_exists(irreps_layer_out_prev, edge_attr_irreps, \"0e\")\n",
    "                else \"0o\"\n",
    "            )\n",
    "            irreps_gates = o3.Irreps([(mul, ir) for mul, _ in irreps_gated])\n",
    "\n",
    "            # TO DO, it's not that safe to directly use the\n",
    "            # dictionary\n",
    "            equivariant_nonlin = Gate(\n",
    "                irreps_scalars=irreps_scalars,\n",
    "                act_scalars=[\n",
    "                    acts[nonlinearity_scalars[ir.p]] for _, ir in irreps_scalars\n",
    "                ],\n",
    "                irreps_gates=irreps_gates,\n",
    "                act_gates=[acts[nonlinearity_gates[ir.p]] for _, ir in irreps_gates],\n",
    "                irreps_gated=irreps_gated,\n",
    "            )\n",
    "\n",
    "            conv_irreps_out = equivariant_nonlin.irreps_in.simplify()\n",
    "\n",
    "        else:\n",
    "            conv_irreps_out = irreps_layer_out.simplify()\n",
    "\n",
    "            equivariant_nonlin = NormActivation(\n",
    "                irreps_in=conv_irreps_out,\n",
    "                # norm is an even scalar, so use nonlinearity_scalars[1]\n",
    "                scalar_nonlinearity=acts[nonlinearity_scalars[1]],\n",
    "                normalize=True,\n",
    "                epsilon=1e-8,\n",
    "                bias=False,\n",
    "            )\n",
    "\n",
    "        self.equivariant_nonlin = equivariant_nonlin\n",
    "\n",
    "        # TODO: partial resnet?\n",
    "        if irreps_layer_out == irreps_layer_out_prev and resnet:\n",
    "            # We are doing resnet updates and can for this layer\n",
    "            self.resnet = True\n",
    "        else:\n",
    "            self.resnet = False\n",
    "\n",
    "        # TODO: last convolution should go to explicit irreps out\n",
    "        logging.debug(\n",
    "            f\" parameters used to initialize {convolution.__name__}={convolution_kwargs}\"\n",
    "        )\n",
    "\n",
    "        # override defaults for irreps:\n",
    "        convolution_kwargs.pop(\"irreps_in\", None)\n",
    "        convolution_kwargs.pop(\"irreps_out\", None)\n",
    "        self.conv = convolution(\n",
    "            irreps_in=self.irreps_in,\n",
    "            irreps_out=conv_irreps_out,\n",
    "            **convolution_kwargs,\n",
    "        )\n",
    "\n",
    "        # The output features are whatever we got in\n",
    "        # updated with whatever the convolution outputs (which is a full graph module)\n",
    "        self.irreps_out.update(self.conv.irreps_out)\n",
    "        # but with the features updated by the nonlinearity\n",
    "        self.irreps_out[AtomicDataDict.NODE_FEATURES_KEY] = (\n",
    "            self.equivariant_nonlin.irreps_out\n",
    "        )\n",
    "\n",
    "    # All before complex stuff is preparation, the actual is this part and it is pretty simple\n",
    "    # 1. conv\n",
    "    # 2. equiv nonlinear\n",
    "    # 3. add features from prev layer\n",
    "    def forward(self, data: AtomicDataDict.Type) -> AtomicDataDict.Type:\n",
    "        # save old features for resnet\n",
    "        old_x = data[AtomicDataDict.NODE_FEATURES_KEY]\n",
    "        # run convolution\n",
    "        data = self.conv(data)\n",
    "        # do nonlinearity\n",
    "        data[AtomicDataDict.NODE_FEATURES_KEY] = self.equivariant_nonlin(\n",
    "            data[AtomicDataDict.NODE_FEATURES_KEY]\n",
    "        )\n",
    "        # do resnet\n",
    "        if self.resnet:\n",
    "            data[AtomicDataDict.NODE_FEATURES_KEY] = (\n",
    "                old_x + data[AtomicDataDict.NODE_FEATURES_KEY]\n",
    "            )\n",
    "        return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gcnn",
   "language": "python",
   "name": "torch_gcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
