{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "014d6ed1-aff3-44dd-8438-954502cad188",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e721c7c8-a5b3-46d9-a8ba-a98126eba75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Train a network.\"\"\"\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "import warnings\n",
    "\n",
    "# This is a weird hack to avoid Intel MKL issues on the cluster when this is called as a subprocess of a process \n",
    "# that has itself initialized PyTorch.\n",
    "# Since numpy gets imported later anyway for dataset stuff, this shouldn't affect performance.\n",
    "import numpy as np  # noqa: F401\n",
    "\n",
    "from os.path import exists, isdir\n",
    "from shutil import rmtree\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from nequip.model import model_from_config\n",
    "from nequip.utils import Config\n",
    "from nequip.data import dataset_from_config\n",
    "from nequip.utils import load_file\n",
    "from nequip.utils.config import _GLOBAL_ALL_ASKED_FOR_KEYS\n",
    "from nequip.utils.test import assert_AtomicData_equivariant\n",
    "from nequip.utils.versions import check_code_version\n",
    "from nequip.utils.misc import get_default_device_name\n",
    "from nequip.utils._global_options import _set_global_options\n",
    "from nequip.scripts._logger import set_up_script_logger\n",
    "\n",
    "default_config = dict(\n",
    "    root=\"./\",\n",
    "    tensorboard=False,\n",
    "    wandb=False,\n",
    "    model_builders=[\n",
    "        \"SimpleIrrepsConfig\",\n",
    "        \"EnergyModel\",\n",
    "        \"PerSpeciesRescale\",\n",
    "        \"StressForceOutput\",\n",
    "        \"RescaleEnergyEtc\",\n",
    "    ],\n",
    "    dataset_statistics_stride=1,\n",
    "    device=get_default_device_name(),\n",
    "    default_dtype=\"float64\",\n",
    "    model_dtype=\"float32\",\n",
    "    allow_tf32=True,\n",
    "    verbose=\"INFO\",\n",
    "    model_debug_mode=False,\n",
    "    equivariance_test=False,\n",
    "    grad_anomaly_mode=False,\n",
    "    gpu_oom_offload=False,\n",
    "    append=False,\n",
    "    warn_unused=False,\n",
    "    _jit_bailout_depth=2,  # avoid 20 iters of pain, see https://github.com/pytorch/pytorch/issues/52286\n",
    "    # Quote from eelison in PyTorch slack:\n",
    "    # https://pytorch.slack.com/archives/CDZD1FANA/p1644259272007529?thread_ts=1644064449.039479&cid=CDZD1FANA\n",
    "    # > Right now the default behavior is to specialize twice on static shapes and then on dynamic shapes.\n",
    "    # > To reduce warmup time you can do something like setFusionStrartegy({{FusionBehavior::DYNAMIC, 3}})\n",
    "    # > ... Although we would wouldn't really expect to recompile a dynamic shape fusion in a model,\n",
    "    # > provided broadcasting patterns remain fixed\n",
    "    # We default to DYNAMIC alone because the number of edges is always dynamic,\n",
    "    # even if the number of atoms is fixed:\n",
    "    _jit_fusion_strategy=[(\"DYNAMIC\", 3)],\n",
    "    # Due to what appear to be ongoing bugs with nvFuser, we default to NNC (fuser1) for now:\n",
    "    # TODO: still default to NNC on CPU regardless even if change this for GPU\n",
    "    # TODO: default for ROCm?\n",
    "    _jit_fuser=\"fuser1\",\n",
    ")\n",
    "# All default_config keys are valid / requested\n",
    "_GLOBAL_ALL_ASKED_FOR_KEYS.update(default_config.keys())\n",
    "\n",
    "\n",
    "def main(args=None, running_as_script: bool = True):\n",
    "    config = parse_command_line(args)\n",
    "\n",
    "    if running_as_script:\n",
    "        set_up_script_logger(config.get(\"log\", None), config.verbose)\n",
    "\n",
    "    found_restart_file = exists(f\"{config.root}/{config.run_name}/trainer.pth\")\n",
    "    if found_restart_file and not config.append:\n",
    "        raise RuntimeError(\n",
    "            f\"Training instance exists at {config.root}/{config.run_name}; \"\n",
    "            \"either set append to True or use a different root or runname\"\n",
    "        )\n",
    "    elif not found_restart_file and isdir(f\"{config.root}/{config.run_name}\"):\n",
    "        # output directory exists but no ``trainer.pth`` file, suggesting previous run crash during\n",
    "        # first training epoch (usually due to memory):\n",
    "        warnings.warn(\n",
    "            f\"Previous run folder at {config.root}/{config.run_name} exists, but a saved model \"\n",
    "            f\"(trainer.pth file) was not found. This folder will be cleared and a fresh training run will \"\n",
    "            f\"be started.\"\n",
    "        )\n",
    "        rmtree(f\"{config.root}/{config.run_name}\")\n",
    "\n",
    "    # for fresh new train\n",
    "    if not found_restart_file:\n",
    "        trainer = fresh_start(config)\n",
    "    else:\n",
    "        trainer = restart(config)\n",
    "\n",
    "    # Train\n",
    "    trainer.save()\n",
    "    if config.get(\"gpu_oom_offload\", False):\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\n",
    "                \"CUDA is not available; --gpu-oom-offload doesn't make sense.\"\n",
    "            )\n",
    "        warnings.warn(\n",
    "            \"! GPU OOM Offloading is ON:\\n\"\n",
    "            \"This is meant for training models that would be impossible otherwise due to OOM.\\n\"\n",
    "            \"Note that this comes at a speed cost and SHOULD NOT be used if your training fits in GPU memory without it.\\n\"\n",
    "            \"Please also consider whether a smaller model is a more appropriate solution!\\n\"\n",
    "            \"Also, a warning from PyTorch: 'If you overuse pinned memory, it can cause serious problems when running low on RAM!'\"\n",
    "        )\n",
    "        with torch.autograd.graph.save_on_cpu(pin_memory=True):\n",
    "            trainer.train()\n",
    "    else:\n",
    "        trainer.train()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def parse_command_line(args=None):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Train (or restart training of) a NequIP model.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"config\", help=\"YAML file configuring the model, dataset, and other options\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--equivariance-test\",\n",
    "        help=\"test the model's equivariance before training on n (default 1) random frames from the dataset\",\n",
    "        const=1,\n",
    "        type=int,\n",
    "        nargs=\"?\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model-debug-mode\",\n",
    "        help=\"enable model debug mode, which can sometimes give much more useful error messages at the cost of some speed. Do not use for production training!\",\n",
    "        action=\"store_true\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--grad-anomaly-mode\",\n",
    "        help=\"enable PyTorch autograd anomaly mode to debug NaN gradients. Do not use for production training!\",\n",
    "        action=\"store_true\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gpu-oom-offload\",\n",
    "        help=\"Use `torch.autograd.graph.save_on_cpu` to offload intermediate tensors to CPU (host) memory in order to train models that would be impossible otherwise due to OOM. Note that this comes as at a speed cost and SHOULD NOT be used if your training fits in GPU memory without it. Please also consider whether a smaller model is a more appropriate solution.\",\n",
    "        action=\"store_true\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--log\",\n",
    "        help=\"log file to store all the screen logging\",\n",
    "        type=Path,\n",
    "        default=None,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--warn-unused\",\n",
    "        help=\"Warn instead of error when the config contains unused keys\",\n",
    "        action=\"store_true\",\n",
    "    )\n",
    "    args = parser.parse_args(args=args)\n",
    "\n",
    "    config = Config.from_file(args.config, defaults=default_config)\n",
    "    for flag in (\n",
    "        \"model_debug_mode\",\n",
    "        \"equivariance_test\",\n",
    "        \"grad_anomaly_mode\",\n",
    "        \"warn_unused\",\n",
    "        \"gpu_oom_offload\",\n",
    "    ):\n",
    "        config[flag] = getattr(args, flag) or config[flag]\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def fresh_start(config):\n",
    "    # we use add_to_config cause it's a fresh start and need to record it\n",
    "    check_code_version(config, add_to_config=True)\n",
    "    _set_global_options(config)\n",
    "    if config[\"default_dtype\"] != \"float64\":\n",
    "        warnings.warn(\n",
    "            f\"default_dtype={config['default_dtype']} but we strongly recommend float64\"\n",
    "        )\n",
    "\n",
    "    # = Make the trainer =\n",
    "    if config.wandb:\n",
    "\n",
    "        import wandb  # noqa: F401\n",
    "        from nequip.train.trainer_wandb import TrainerWandB as Trainer\n",
    "\n",
    "        # download parameters from wandb in case of sweeping\n",
    "        from nequip.utils.wandb import init_n_update\n",
    "\n",
    "        config = init_n_update(config)\n",
    "\n",
    "    elif config.tensorboard:\n",
    "        from nequip.train.trainer_tensorboard import TrainerTensorBoard as Trainer\n",
    "    else:\n",
    "        from nequip.train.trainer import Trainer # !!! Important to look next\n",
    "\n",
    "    trainer = Trainer(model=None, **Config.as_dict(config))\n",
    "\n",
    "    # what is this\n",
    "    # to update wandb data?\n",
    "    config.update(trainer.params)\n",
    "\n",
    "    # = Load the dataset =\n",
    "    dataset = dataset_from_config(config, prefix=\"dataset\")\n",
    "    logging.info(f\"Successfully loaded the data set of type {dataset}...\")\n",
    "    try:\n",
    "        validation_dataset = dataset_from_config(config, prefix=\"validation_dataset\")\n",
    "        logging.info(\n",
    "            f\"Successfully loaded the validation data set of type {validation_dataset}...\"\n",
    "        )\n",
    "    except KeyError:\n",
    "        # It couldn't be found\n",
    "        validation_dataset = None\n",
    "\n",
    "    # = Train/test split =\n",
    "    trainer.set_dataset(dataset, validation_dataset)\n",
    "\n",
    "    # = Build model =\n",
    "    final_model = model_from_config(\n",
    "        config=config, initialize=True, dataset=trainer.dataset_train\n",
    "    )\n",
    "    logging.info(\"Successfully built the network...\")\n",
    "\n",
    "    # Equivar test\n",
    "    if config.equivariance_test > 0:\n",
    "        n_train: int = len(trainer.dataset_train)\n",
    "        assert config.equivariance_test <= n_train\n",
    "        final_model.eval()\n",
    "        indexes = torch.randperm(n_train)[: config.equivariance_test]\n",
    "        errstr = assert_AtomicData_equivariant(\n",
    "            final_model, [trainer.dataset_train[i] for i in indexes]\n",
    "        )\n",
    "        final_model.train()\n",
    "        logging.info(\n",
    "            \"Equivariance test passed; equivariance errors:\\n\"\n",
    "            \"   Errors are in real units, where relevant.\\n\"\n",
    "            \"   Please note that the large scale of the typical\\n\"\n",
    "            \"   shifts to the (atomic) energy can cause\\n\"\n",
    "            \"   catastrophic cancellation and give incorrectly\\n\"\n",
    "            \"   the equivariance error as zero for those fields.\\n\"\n",
    "            f\"{errstr}\"\n",
    "        )\n",
    "        del errstr, indexes, n_train\n",
    "\n",
    "    # Set the trainer\n",
    "    trainer.model = final_model\n",
    "\n",
    "    # Store any updated config information in the trainer\n",
    "    trainer.update_kwargs(config)\n",
    "\n",
    "    # Only run the unused check as a callback after the trainer has\n",
    "    # initialized everything (metrics, early stopping, etc.)\n",
    "    def _unused_check():\n",
    "        unused = config._unused_keys()\n",
    "        if len(unused) > 0:\n",
    "            message = f\"The following keys in the config file were not used, did you make a typo?: {', '.join(unused)}. (If this sounds wrong, please file an issue. You can turn this error into a warning with `--warn-unused`, but please make sure that the key really is correctly spelled and used!.)\"\n",
    "            if config.warn_unused:\n",
    "                warnings.warn(message)\n",
    "            else:\n",
    "                raise KeyError(message)\n",
    "\n",
    "    trainer._post_init_callback = _unused_check\n",
    "\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def restart(config):\n",
    "    # load the dictionary\n",
    "    restart_file = f\"{config.root}/{config.run_name}/trainer.pth\"\n",
    "    dictionary = load_file(\n",
    "        supported_formats=dict(torch=[\"pt\", \"pth\"]),\n",
    "        filename=restart_file,\n",
    "        enforced_format=\"torch\",\n",
    "    )\n",
    "\n",
    "    # compare dictionary to config and update stop condition related arguments\n",
    "    for k in config.keys():\n",
    "        if config[k] != dictionary.get(k, \"\"):\n",
    "            if k == \"max_epochs\":\n",
    "                dictionary[k] = config[k]\n",
    "                logging.info(f'Update \"{k}\" to {dictionary[k]}')\n",
    "            elif k.startswith(\"early_stop\"):\n",
    "                dictionary[k] = config[k]\n",
    "                logging.info(f'Update \"{k}\" to {dictionary[k]}')\n",
    "            elif isinstance(config[k], type(dictionary.get(k, \"\"))):\n",
    "                raise ValueError(\n",
    "                    f'Key \"{k}\" is different in config and the result trainer.pth file. Please double check'\n",
    "                )\n",
    "\n",
    "    # note, \"trainer.pth\"/dictionary also store code versions,\n",
    "    # which will not be stored in config and thus not checked here\n",
    "    check_code_version(config)\n",
    "\n",
    "    # recursive loop, if same type but different value\n",
    "    # raise error\n",
    "\n",
    "    config = Config(dictionary, exclude_keys=[\"state_dict\", \"progress\"])\n",
    "\n",
    "    # dtype, etc.\n",
    "    _set_global_options(config)\n",
    "\n",
    "    # note, the from_dict method will check whether the code version\n",
    "    # in trainer.pth is consistent and issue warnings\n",
    "    if config.wandb:\n",
    "        from nequip.train.trainer_wandb import TrainerWandB\n",
    "        from nequip.utils.wandb import resume\n",
    "\n",
    "        resume(config)\n",
    "        trainer = TrainerWandB.from_dict(dictionary)\n",
    "    else:\n",
    "        from nequip.train.trainer import Trainer\n",
    "\n",
    "        trainer = Trainer.from_dict(dictionary)\n",
    "\n",
    "    # = Load the dataset =\n",
    "    dataset = dataset_from_config(config, prefix=\"dataset\")\n",
    "    logging.info(f\"Successfully re-loaded the data set of type {dataset}...\")\n",
    "    try:\n",
    "        validation_dataset = dataset_from_config(config, prefix=\"validation_dataset\")\n",
    "        logging.info(\n",
    "            f\"Successfully re-loaded the validation data set of type {validation_dataset}...\"\n",
    "        )\n",
    "    except KeyError:\n",
    "        # It couldn't be found\n",
    "        validation_dataset = None\n",
    "    trainer.set_dataset(dataset, validation_dataset)\n",
    "\n",
    "    return trainer\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(running_as_script=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff9fad78-2e02-4e57-95ce-2791c6737d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Train a network.\"\"\"\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "import warnings\n",
    "\n",
    "# This is a weird hack to avoid Intel MKL issues on the cluster when this is called as a subprocess of a process \n",
    "# that has itself initialized PyTorch.\n",
    "# Since numpy gets imported later anyway for dataset stuff, this shouldn't affect performance.\n",
    "import numpy as np  # noqa: F401\n",
    "\n",
    "from os.path import exists, isdir\n",
    "from shutil import rmtree\n",
    "from pathlib import Path\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2f82d54-b973-4074-b8be-046e1ad33cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Config, _GLOBAL_ALL_ASKED_FOR_KEYS\n",
    "from utils import get_default_device_name\n",
    "from utils import load_callable\n",
    "\n",
    "default_config = dict(\n",
    "    root=\"./\",\n",
    "    tensorboard=False,\n",
    "    wandb=False,\n",
    "    model_builders=[\n",
    "        \"SimpleIrrepsConfig\",\n",
    "        \"EnergyModel\",\n",
    "        \"PerSpeciesRescale\",\n",
    "        \"StressForceOutput\",\n",
    "        \"RescaleEnergyEtc\",\n",
    "    ],\n",
    "    dataset_statistics_stride=1,\n",
    "    device=get_default_device_name(),\n",
    "    default_dtype=\"float64\",\n",
    "    model_dtype=\"float32\",\n",
    "    allow_tf32=True,\n",
    "    verbose=\"INFO\",\n",
    "    model_debug_mode=False,\n",
    "    equivariance_test=False,\n",
    "    grad_anomaly_mode=False,\n",
    "    gpu_oom_offload=False,\n",
    "    append=False,\n",
    "    warn_unused=False,\n",
    "    _jit_bailout_depth=2,  # avoid 20 iters of pain, see https://github.com/pytorch/pytorch/issues/52286\n",
    "    # Quote from eelison in PyTorch slack:\n",
    "    # https://pytorch.slack.com/archives/CDZD1FANA/p1644259272007529?thread_ts=1644064449.039479&cid=CDZD1FANA\n",
    "    # > Right now the default behavior is to specialize twice on static shapes and then on dynamic shapes.\n",
    "    # > To reduce warmup time you can do something like setFusionStrartegy({{FusionBehavior::DYNAMIC, 3}})\n",
    "    # > ... Although we would wouldn't really expect to recompile a dynamic shape fusion in a model,\n",
    "    # > provided broadcasting patterns remain fixed\n",
    "    # We default to DYNAMIC alone because the number of edges is always dynamic,\n",
    "    # even if the number of atoms is fixed:\n",
    "    _jit_fusion_strategy=[(\"DYNAMIC\", 3)],\n",
    "    # Due to what appear to be ongoing bugs with nvFuser, we default to NNC (fuser1) for now:\n",
    "    # TODO: still default to NNC on CPU regardless even if change this for GPU\n",
    "    # TODO: default for ROCm?\n",
    "    _jit_fuser=\"fuser1\",\n",
    ")\n",
    "# All default_config keys are valid / requested\n",
    "_GLOBAL_ALL_ASKED_FOR_KEYS.update(default_config.keys())\n",
    "\n",
    "config = Config.from_file('./example.yaml', defaults=default_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6f2de8c-ae79-4e15-b216-f428c3c78686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_jit_bailout_depth': 2, '_jit_fusion_strategy': [('DYNAMIC', 3)], '_jit_fuser': 'fuser1', 'root': 'results/toluene', 'tensorboard': False, 'wandb': True, 'model_builders': ['SimpleIrrepsConfig', 'EnergyModel', 'PerSpeciesRescale', 'StressForceOutput', 'RescaleEnergyEtc'], 'dataset_statistics_stride': 1, 'device': 'cuda', 'default_dtype': 'float64', 'model_dtype': 'float32', 'allow_tf32': True, 'verbose': 'info', 'model_debug_mode': False, 'equivariance_test': False, 'grad_anomaly_mode': False, 'gpu_oom_offload': False, 'append': True, 'warn_unused': False, 'run_name': 'example-run-toluene', 'seed': 123, 'dataset_seed': 456, 'r_max': 4.0, 'num_layers': 4, 'l_max': 2, 'parity': True, 'num_features': 32, 'nonlinearity_type': 'gate', 'nonlinearity_scalars': {'e': 'silu', 'o': 'tanh'}, 'nonlinearity_gates': {'e': 'silu', 'o': 'tanh'}, 'num_basis': 8, 'BesselBasis_trainable': True, 'PolynomialCutoff_p': 6, 'invariant_layers': 2, 'invariant_neurons': 64, 'avg_num_neighbors': 'auto', 'use_sc': True, 'dataset': 'npz', 'dataset_url': 'http://quantum-machine.org/gdml/data/npz/toluene_ccsd_t.zip', 'dataset_file_name': './benchmark_data/toluene_ccsd_t-train.npz', 'key_mapping': {'z': 'atomic_numbers', 'E': 'total_energy', 'F': 'forces', 'R': 'pos'}, 'npz_fixed_field_keys': ['atomic_numbers'], 'chemical_symbols': ['H', 'C'], 'wandb_project': 'toluene-example', 'log_batch_freq': 100, 'log_epoch_freq': 1, 'save_checkpoint_freq': -1, 'save_ema_checkpoint_freq': -1, 'n_train': 100, 'n_val': 50, 'learning_rate': 0.005, 'batch_size': 5, 'validation_batch_size': 10, 'max_epochs': 100000, 'train_val_split': 'random', 'shuffle': True, 'metrics_key': 'validation_loss', 'use_ema': True, 'ema_decay': 0.99, 'ema_use_num_updates': True, 'report_init_validation': True, 'early_stopping_patiences': {'validation_loss': 50}, 'early_stopping_lower_bounds': {'LR': 1e-05}, 'early_stopping_upper_bounds': {'validation_loss': 10000.0}, 'loss_coeffs': {'forces': 1, 'total_energy': [1, 'PerAtomMSELoss']}, 'metrics_components': [['forces', 'mae'], ['forces', 'rmse'], ['forces', 'mae', {'PerSpecies': True, 'report_per_component': False}], ['forces', 'rmse', {'PerSpecies': True, 'report_per_component': False}], ['total_energy', 'mae'], ['total_energy', 'mae', {'PerAtom': True}]], 'optimizer_name': 'Adam', 'optimizer_amsgrad': True, 'lr_scheduler_name': 'ReduceLROnPlateau', 'lr_scheduler_patience': 100, 'lr_scheduler_factor': 0.5, 'per_species_rescale_shifts': 'dataset_per_atom_total_energy_mean', 'per_species_rescale_scales': None}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2a76f67-e1fd-40a7-b620-121bef5bb0de",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Optional' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_from_config\u001b[39m(\n\u001b[1;32m      2\u001b[0m     config: Config,\n\u001b[1;32m      3\u001b[0m     initialize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m----> 4\u001b[0m     dataset: \u001b[43mOptional\u001b[49m[AtomicDataset] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m     deploy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GraphModuleMixin:\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a model based on `config`.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    Model builders (`model_builders`) can have arguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m        The build model.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Optional' is not defined"
     ]
    }
   ],
   "source": [
    "def model_from_config(\n",
    "    config: Config,\n",
    "    initialize: bool = False,\n",
    "    dataset: Optional[AtomicDataset] = None,\n",
    "    deploy: bool = False,\n",
    ") -> GraphModuleMixin:\n",
    "    \"\"\"Build a model based on `config`.\n",
    "\n",
    "    Model builders (`model_builders`) can have arguments:\n",
    "     - ``config``: the config. Always present.\n",
    "     - ``model``: the model produced by the previous builder. Cannot be requested by the first builder, must be requested by subsequent ones.\n",
    "     - ``initialize``: whether to initialize the model\n",
    "     - ``dataset``: if ``initialize`` is True, the dataset\n",
    "     - ``deploy``: whether the model object is for deployment / inference\n",
    "\n",
    "    Note that this function temporarily sets ``torch.set_default_dtype()`` and as such is not thread safe.\n",
    "\n",
    "    Args:\n",
    "        config\n",
    "        initialize (bool): whether ``model_builders`` should be instructed to initialize the model\n",
    "        dataset: dataset for initializers if ``initialize`` is True.\n",
    "        deploy (bool): whether ``model_builders`` should be told the model is for deployment / inference\n",
    "\n",
    "    Returns:\n",
    "        The build model.\n",
    "    \"\"\"\n",
    "    if isinstance(config, dict):\n",
    "        config = Config.from_dict(config)\n",
    "    # Pre-process config\n",
    "    type_mapper = None\n",
    "    if dataset is not None:\n",
    "        type_mapper = dataset.type_mapper\n",
    "    else:\n",
    "        try:\n",
    "            type_mapper, _ = instantiate(TypeMapper, all_args=config)\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "\n",
    "    if type_mapper is not None:\n",
    "        if \"num_types\" in config:\n",
    "            assert (\n",
    "                config[\"num_types\"] == type_mapper.num_types\n",
    "            ), \"inconsistant config & dataset\"\n",
    "        if \"type_names\" in config:\n",
    "            assert (\n",
    "                config[\"type_names\"] == type_mapper.type_names\n",
    "            ), \"inconsistant config & dataset\"\n",
    "        config[\"num_types\"] = type_mapper.num_types\n",
    "        config[\"type_names\"] = type_mapper.type_names\n",
    "        config[\"type_to_chemical_symbol\"] = type_mapper.type_to_chemical_symbol\n",
    "        # We added them, so they are by definition valid:\n",
    "        _GLOBAL_ALL_ASKED_FOR_KEYS.update(\n",
    "            {\"num_types\", \"type_names\", \"type_to_chemical_symbol\"}\n",
    "        )\n",
    "\n",
    "    default_dtype = torch.get_default_dtype()\n",
    "    model_dtype: torch.dtype = dtype_from_name(config.get(\"model_dtype\", default_dtype))\n",
    "    config[\"model_dtype\"] = str(model_dtype).lstrip(\"torch.\")\n",
    "    # confirm sanity\n",
    "    assert default_dtype in (torch.float32, torch.float64)\n",
    "    if default_dtype == torch.float32 and model_dtype == torch.float64:\n",
    "        raise ValueError(\n",
    "            \"Overall default_dtype=float32, but model_dtype=float64 is a higher precision- change default_dtype to float64\"\n",
    "        )\n",
    "    # temporarily set the default dtype\n",
    "    start_graph_model_builders = None\n",
    "    with torch_default_dtype(model_dtype):\n",
    "\n",
    "        # Build\n",
    "        builders = [\n",
    "            load_callable(b, prefix=\"nequip.model\")\n",
    "            for b in config.get(\"model_builders\", [])\n",
    "        ]\n",
    "\n",
    "        model = None\n",
    "\n",
    "        for builder_i, builder in enumerate(builders):\n",
    "            pnames = inspect.signature(builder).parameters\n",
    "            params = {}\n",
    "            if \"graph_model\" in pnames:\n",
    "                # start graph_model builders, which happen later\n",
    "                start_graph_model_builders = builder_i\n",
    "                break\n",
    "            if \"initialize\" in pnames:\n",
    "                params[\"initialize\"] = initialize\n",
    "            if \"deploy\" in pnames:\n",
    "                params[\"deploy\"] = deploy\n",
    "            if \"config\" in pnames:\n",
    "                params[\"config\"] = config\n",
    "            if \"dataset\" in pnames:\n",
    "                if \"initialize\" not in pnames:\n",
    "                    raise ValueError(\n",
    "                        \"Cannot request dataset without requesting initialize\"\n",
    "                    )\n",
    "                if (\n",
    "                    initialize\n",
    "                    and pnames[\"dataset\"].default == inspect.Parameter.empty\n",
    "                    and dataset is None\n",
    "                ):\n",
    "                    raise RuntimeError(\n",
    "                        f\"Builder {builder.__name__} requires the dataset, initialize is true, but no dataset was provided to `model_from_config`.\"\n",
    "                    )\n",
    "                params[\"dataset\"] = dataset\n",
    "            if \"model\" in pnames:\n",
    "                if model is None:\n",
    "                    raise RuntimeError(\n",
    "                        f\"Builder {builder.__name__} asked for the model as an input, but no previous builder has returned a model\"\n",
    "                    )\n",
    "                params[\"model\"] = model\n",
    "            else:\n",
    "                if model is not None:\n",
    "                    raise RuntimeError(\n",
    "                        f\"All model_builders after the first one that returns a model must take the model as an argument; {builder.__name__} doesn't\"\n",
    "                    )\n",
    "            model = builder(**params)\n",
    "            if model is not None and not isinstance(model, GraphModuleMixin):\n",
    "                raise TypeError(\n",
    "                    f\"Builder {builder.__name__} didn't return a GraphModuleMixin, got {type(model)} instead\"\n",
    "                )\n",
    "    # reset to default dtype by context manager\n",
    "\n",
    "    # Wrap the model up\n",
    "    model = GraphModel(\n",
    "        model,\n",
    "        model_dtype=model_dtype,\n",
    "        model_input_fields=config.get(\"model_input_fields\", {}),\n",
    "    )\n",
    "\n",
    "    # Run GraphModel builders\n",
    "    if start_graph_model_builders is not None:\n",
    "        for builder in builders[start_graph_model_builders:]:\n",
    "            pnames = inspect.signature(builder).parameters\n",
    "            params = {}\n",
    "            assert \"graph_model\" in pnames\n",
    "            params[\"graph_model\"] = model\n",
    "            if \"model\" in pnames:\n",
    "                raise ValueError(\n",
    "                    f\"Once any builder requests `graph_model` (first requested by {builders[start_graph_model_builders].__name__}), no builder can request `model`, but {builder.__name__} did\"\n",
    "                )\n",
    "            if \"initialize\" in pnames:\n",
    "                params[\"initialize\"] = initialize\n",
    "            if \"deploy\" in pnames:\n",
    "                params[\"deploy\"] = deploy\n",
    "            if \"config\" in pnames:\n",
    "                params[\"config\"] = config\n",
    "            if \"dataset\" in pnames:\n",
    "                if \"initialize\" not in pnames:\n",
    "                    raise ValueError(\n",
    "                        \"Cannot request dataset without requesting initialize\"\n",
    "                    )\n",
    "                if (\n",
    "                    initialize\n",
    "                    and pnames[\"dataset\"].default == inspect.Parameter.empty\n",
    "                    and dataset is None\n",
    "                ):\n",
    "                    raise RuntimeError(\n",
    "                        f\"Builder {builder.__name__} requires the dataset, initialize is true, but no dataset was provided to `model_from_config`.\"\n",
    "                    )\n",
    "                params[\"dataset\"] = dataset\n",
    "\n",
    "            model = builder(**params)\n",
    "            if not isinstance(model, GraphModel):\n",
    "                raise TypeError(\n",
    "                    f\"Builder {builder.__name__} didn't return a GraphModel, got {type(model)} instead\"\n",
    "                )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17d2fbe-cbb6-4e2a-a452-0d97af37d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphModuleMixin:\n",
    "    r\"\"\"Mixin parent class for ``torch.nn.Module``s that act on and return ``AtomicDataDict.Type`` graph data.\n",
    "\n",
    "    All such classes should call ``_init_irreps`` in their ``__init__`` functions with information on the data fields they expect,\n",
    "    require, and produce, as well as their corresponding irreps.\n",
    "    \"\"\"\n",
    "\n",
    "    def _init_irreps(\n",
    "        self,\n",
    "        irreps_in: Dict[str, Any] = {},\n",
    "        my_irreps_in: Dict[str, Any] = {},\n",
    "        required_irreps_in: Sequence[str] = [],\n",
    "        irreps_out: Dict[str, Any] = {},\n",
    "    ):\n",
    "        \"\"\"Setup the expected data fields and their irreps for this graph module.\n",
    "\n",
    "        ``None`` is a valid irreps in the context for anything that is invariant but not well described by an ``e3nn.o3.Irreps``.\n",
    "        An example are edge indexes in a graph, which are invariant but are integers, not ``0e`` scalars.\n",
    "\n",
    "        Args:\n",
    "            irreps_in (dict): maps names of all input fields from previous modules or\n",
    "                data to their corresponding irreps\n",
    "            my_irreps_in (dict): maps names of fields to the irreps they must have for\n",
    "                this graph module. Will be checked for consistancy with ``irreps_in``\n",
    "            required_irreps_in: sequence of names of fields that must be present in\n",
    "                ``irreps_in``, but that can have any irreps.\n",
    "            irreps_out (dict): mapping names of fields that are modified/output by\n",
    "                this graph module to their irreps.\n",
    "        \"\"\"\n",
    "        # Coerce\n",
    "        irreps_in = {} if irreps_in is None else irreps_in\n",
    "        irreps_in = AtomicDataDict._fix_irreps_dict(irreps_in)\n",
    "        # positions are *always* 1o, and always present\n",
    "        if AtomicDataDict.POSITIONS_KEY in irreps_in:\n",
    "            if irreps_in[AtomicDataDict.POSITIONS_KEY] != o3.Irreps(\"1x1o\"):\n",
    "                raise ValueError(\n",
    "                    f\"Positions must have irreps 1o, got instead `{irreps_in[AtomicDataDict.POSITIONS_KEY]}`\"\n",
    "                )\n",
    "        irreps_in[AtomicDataDict.POSITIONS_KEY] = o3.Irreps(\"1o\")\n",
    "        # edges are also always present\n",
    "        if AtomicDataDict.EDGE_INDEX_KEY in irreps_in:\n",
    "            if irreps_in[AtomicDataDict.EDGE_INDEX_KEY] is not None:\n",
    "                raise ValueError(\n",
    "                    f\"Edge indexes must have irreps None, got instead `{irreps_in[AtomicDataDict.EDGE_INDEX_KEY]}`\"\n",
    "                )\n",
    "        irreps_in[AtomicDataDict.EDGE_INDEX_KEY] = None\n",
    "\n",
    "        my_irreps_in = AtomicDataDict._fix_irreps_dict(my_irreps_in)\n",
    "\n",
    "        irreps_out = AtomicDataDict._fix_irreps_dict(irreps_out)\n",
    "        # Confirm compatibility:\n",
    "        # with my_irreps_in\n",
    "        for k in my_irreps_in:\n",
    "            if k in irreps_in and irreps_in[k] != my_irreps_in[k]:\n",
    "                raise ValueError(\n",
    "                    f\"The given input irreps {irreps_in[k]} for field '{k}' is incompatible with this configuration {type(self)}; should have been {my_irreps_in[k]}\"\n",
    "                )\n",
    "        # with required_irreps_in\n",
    "        for k in required_irreps_in:\n",
    "            if k not in irreps_in:\n",
    "                raise ValueError(\n",
    "                    f\"This {type(self)} requires field '{k}' to be in irreps_in\"\n",
    "                )\n",
    "        # Save stuff\n",
    "        self.irreps_in = irreps_in\n",
    "        # The output irreps of any graph module are whatever inputs it has, overwritten with whatever outputs it has.\n",
    "        new_out = irreps_in.copy()\n",
    "        new_out.update(irreps_out)\n",
    "        self.irreps_out = new_out\n",
    "\n",
    "    def _add_independent_irreps(self, irreps: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Insert some independent irreps that need to be exposed to the self.irreps_in and self.irreps_out.\n",
    "        The terms that have already appeared in the irreps_in will be removed.\n",
    "\n",
    "        Args:\n",
    "            irreps (dict): maps names of all new fields\n",
    "        \"\"\"\n",
    "\n",
    "        irreps = {\n",
    "            key: irrep for key, irrep in irreps.items() if key not in self.irreps_in\n",
    "        }\n",
    "        irreps_in = AtomicDataDict._fix_irreps_dict(irreps)\n",
    "        irreps_out = AtomicDataDict._fix_irreps_dict(\n",
    "            {key: irrep for key, irrep in irreps.items() if key not in self.irreps_out}\n",
    "        )\n",
    "        self.irreps_in.update(irreps_in)\n",
    "        self.irreps_out.update(irreps_out)\n",
    "\n",
    "    def _make_tracing_inputs(self, n):\n",
    "        # We impliment this to be able to trace graph modules\n",
    "        out = []\n",
    "        for _ in range(n):\n",
    "            batch = random.randint(1, 4)\n",
    "            # TODO: handle None case\n",
    "            # TODO: do only required inputs\n",
    "            # TODO: dummy input if empty?\n",
    "            out.append(\n",
    "                {\n",
    "                    \"forward\": (\n",
    "                        {\n",
    "                            k: i.randn(batch, -1)\n",
    "                            for k, i in self.irreps_in.items()\n",
    "                            if i is not None\n",
    "                        },\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "        return out\n",
    "\n",
    "\n",
    "class SequentialGraphNetwork(GraphModuleMixin, torch.nn.Sequential):\n",
    "    r\"\"\"A ``torch.nn.Sequential`` of ``GraphModuleMixin``s.\n",
    "\n",
    "    Args:\n",
    "        modules (list or dict of ``GraphModuleMixin``s): the sequence of graph modules. \n",
    "        If a list, the modules will be named ``\"module0\", \"module1\", ...``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        modules: Union[Sequence[GraphModuleMixin], Dict[str, GraphModuleMixin]],\n",
    "    ):\n",
    "        if isinstance(modules, dict):\n",
    "            module_list = list(modules.values())\n",
    "        else:\n",
    "            module_list = list(modules)\n",
    "        # check in/out irreps compatible\n",
    "        for m1, m2 in zip(module_list, module_list[1:]):\n",
    "            assert AtomicDataDict._irreps_compatible(\n",
    "                m1.irreps_out, m2.irreps_in\n",
    "            ), f\"Incompatible irreps_out from {type(m1).__name__} for input to {type(m2).__name__}: {m1.irreps_out} -> {m2.irreps_in}\"\n",
    "        self._init_irreps(\n",
    "            irreps_in=module_list[0].irreps_in,\n",
    "            my_irreps_in=module_list[0].irreps_in,\n",
    "            irreps_out=module_list[-1].irreps_out,\n",
    "        )\n",
    "        # torch.nn.Sequential will name children correctly if passed an OrderedDict\n",
    "        if isinstance(modules, dict):\n",
    "            modules = OrderedDict(modules)\n",
    "        else:\n",
    "            modules = OrderedDict((f\"module{i}\", m) for i, m in enumerate(module_list))\n",
    "        super().__init__(modules)\n",
    "\n",
    "    @classmethod\n",
    "    def from_parameters(\n",
    "        cls,\n",
    "        shared_params: Mapping,\n",
    "        layers: Dict[str, Union[Callable, Tuple[Callable, Dict[str, Any]]]],\n",
    "        irreps_in: Optional[dict] = None,\n",
    "    ):\n",
    "        r\"\"\"Construct a ``SequentialGraphModule`` of modules built from a shared set of parameters.\n",
    "\n",
    "        For some layer, a parameter with name ``param`` will be taken, in order of priority, from:\n",
    "          1. The specific value in the parameter dictionary for that layer, if provided\n",
    "          2. ``name_param`` in ``shared_params`` where ``name`` is the name of the layer\n",
    "          3. ``param`` in ``shared_params``\n",
    "\n",
    "        Args:\n",
    "            shared_params (dict-like): shared parameters from which to pull when instantiating the module\n",
    "            layers (dict): dictionary mapping unique names of layers to either:\n",
    "                  1. A callable (such as a class or function) that can be used to ``instantiate`` a module for that layer\n",
    "                  2. A tuple of such a callable and a dictionary mapping parameter names to values. The given dictionary of parameters will override for this layer values found in ``shared_params``.\n",
    "                Options 1. and 2. can be mixed.\n",
    "            irreps_in (optional dict): ``irreps_in`` for the first module in the sequence.\n",
    "\n",
    "        Returns:\n",
    "            The constructed SequentialGraphNetwork.\n",
    "        \"\"\"\n",
    "        # note that dictionary ordered gueranteed in >=3.7, so its fine to do an ordered sequential as a dict.\n",
    "        built_modules = []\n",
    "        for name, builder in layers.items():\n",
    "            if not isinstance(name, str):\n",
    "                raise ValueError(f\"`'name'` must be a str; got `{name}`\")\n",
    "            if isinstance(builder, tuple):\n",
    "                builder, params = builder\n",
    "            else:\n",
    "                params = {}\n",
    "            if not callable(builder):\n",
    "                raise TypeError(\n",
    "                    f\"The builder has to be a class or a function. got {type(builder)}\"\n",
    "                )\n",
    "\n",
    "            instance, _ = instantiate(\n",
    "                builder=builder,\n",
    "                prefix=name,\n",
    "                positional_args=(\n",
    "                    dict(\n",
    "                        irreps_in=(\n",
    "                            built_modules[-1].irreps_out\n",
    "                            if len(built_modules) > 0\n",
    "                            else irreps_in\n",
    "                        )\n",
    "                    )\n",
    "                ),\n",
    "                optional_args=params,\n",
    "                all_args=shared_params,\n",
    "            )\n",
    "\n",
    "            if not isinstance(instance, GraphModuleMixin):\n",
    "                raise TypeError(\n",
    "                    f\"Builder `{builder}` for layer with name `{name}` did not return a GraphModuleMixin, instead got a {type(instance).__name__}\"\n",
    "                )\n",
    "\n",
    "            built_modules.append(instance)\n",
    "\n",
    "        return cls(\n",
    "            OrderedDict(zip(layers.keys(), built_modules)),\n",
    "        )\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def append(self, name: str, module: GraphModuleMixin) -> None:\n",
    "        r\"\"\"Append a module to the SequentialGraphNetwork.\n",
    "\n",
    "        Args:\n",
    "            name (str): the name for the module\n",
    "            module (GraphModuleMixin): the module to append\n",
    "        \"\"\"\n",
    "        assert AtomicDataDict._irreps_compatible(self.irreps_out, module.irreps_in)\n",
    "        self.add_module(name, module)\n",
    "        self.irreps_out = dict(module.irreps_out)\n",
    "        return\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def append_from_parameters(\n",
    "        self,\n",
    "        shared_params: Mapping,\n",
    "        name: str,\n",
    "        builder: Callable,\n",
    "        params: Dict[str, Any] = {},\n",
    "    ) -> GraphModuleMixin:\n",
    "        r\"\"\"Build a module from parameters and append it.\n",
    "\n",
    "        Args:\n",
    "            shared_params (dict-like): shared parameters from which to pull when instantiating the module\n",
    "            name (str): the name for the module\n",
    "            builder (callable): a class or function to build a module\n",
    "            params (dict, optional): extra specific parameters for this module that take priority over those in ``shared_params``\n",
    "\n",
    "        Returns:\n",
    "            the build module\n",
    "        \"\"\"\n",
    "        instance, _ = instantiate(\n",
    "            builder=builder,\n",
    "            prefix=name,\n",
    "            positional_args=(dict(irreps_in=self[-1].irreps_out)),\n",
    "            optional_args=params,\n",
    "            all_args=shared_params,\n",
    "        )\n",
    "        self.append(name, instance)\n",
    "        return instance\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def insert(\n",
    "        self,\n",
    "        name: str,\n",
    "        module: GraphModuleMixin,\n",
    "        after: Optional[str] = None,\n",
    "        before: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Insert a module after the module with name ``after``.\n",
    "\n",
    "        Args:\n",
    "            name: the name of the module to insert\n",
    "            module: the moldule to insert\n",
    "            after: the module to insert after\n",
    "            before: the module to insert before\n",
    "        \"\"\"\n",
    "\n",
    "        if (before is None) is (after is None):\n",
    "            raise ValueError(\"Only one of before or after argument needs to be defined\")\n",
    "        elif before is None:\n",
    "            insert_location = after\n",
    "        else:\n",
    "            insert_location = before\n",
    "\n",
    "        # This checks names, etc.\n",
    "        self.add_module(name, module)\n",
    "        # Now insert in the right place by overwriting\n",
    "        names = list(self._modules.keys())\n",
    "        modules = list(self._modules.values())\n",
    "        idx = names.index(insert_location)\n",
    "        if before is None:\n",
    "            idx += 1\n",
    "        names.insert(idx, name)\n",
    "        modules.insert(idx, module)\n",
    "\n",
    "        self._modules = OrderedDict(zip(names, modules))\n",
    "\n",
    "        module_list = list(self._modules.values())\n",
    "\n",
    "        # sanity check the compatibility\n",
    "        if idx > 0:\n",
    "            assert AtomicDataDict._irreps_compatible(\n",
    "                module_list[idx - 1].irreps_out, module.irreps_in\n",
    "            )\n",
    "        if len(module_list) > idx:\n",
    "            assert AtomicDataDict._irreps_compatible(\n",
    "                module_list[idx + 1].irreps_in, module.irreps_out\n",
    "            )\n",
    "\n",
    "        # insert the new irreps_out to the later modules\n",
    "        for module_id, next_module in enumerate(module_list[idx + 1 :]):\n",
    "            next_module._add_independent_irreps(module.irreps_out)\n",
    "\n",
    "        # update the final wrapper irreps_out\n",
    "        self.irreps_out = dict(module_list[-1].irreps_out)\n",
    "\n",
    "        return\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def insert_from_parameters(\n",
    "        self,\n",
    "        shared_params: Mapping,\n",
    "        name: str,\n",
    "        builder: Callable,\n",
    "        params: Dict[str, Any] = {},\n",
    "        after: Optional[str] = None,\n",
    "        before: Optional[str] = None,\n",
    "    ) -> GraphModuleMixin:\n",
    "        r\"\"\"Build a module from parameters and insert it after ``after``.\n",
    "\n",
    "        Args:\n",
    "            shared_params (dict-like): shared parameters from which to pull when instantiating the module\n",
    "            name (str): the name for the module\n",
    "            builder (callable): a class or function to build a module\n",
    "            params (dict, optional): extra specific parameters for this module that take priority over those in ``shared_params``\n",
    "            after: the name of the module to insert after\n",
    "            before: the name of the module to insert before\n",
    "\n",
    "        Returns:\n",
    "            the inserted module\n",
    "        \"\"\"\n",
    "        if (before is None) is (after is None):\n",
    "            raise ValueError(\"Only one of before or after argument needs to be defined\")\n",
    "        elif before is None:\n",
    "            insert_location = after\n",
    "        else:\n",
    "            insert_location = before\n",
    "        idx = list(self._modules.keys()).index(insert_location) - 1\n",
    "        if before is None:\n",
    "            idx += 1\n",
    "        instance, _ = instantiate(\n",
    "            builder=builder,\n",
    "            prefix=name,\n",
    "            positional_args=(dict(irreps_in=self[idx].irreps_out)),\n",
    "            optional_args=params,\n",
    "            all_args=shared_params,\n",
    "        )\n",
    "        self.insert(after=after, before=before, name=name, module=instance)\n",
    "        return instance\n",
    "\n",
    "    # Copied from https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#Sequential\n",
    "    # with type annotations added\n",
    "    def forward(self, input: AtomicDataDict.Type) -> AtomicDataDict.Type:\n",
    "        for module in self:\n",
    "            input = module(input)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfff259-12ea-429f-a24d-c3dbcc52f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "from e3nn.util._argtools import _get_device\n",
    "\n",
    "from nequip.data import AtomicDataDict\n",
    "\n",
    "\n",
    "class GraphModel(GraphModuleMixin, torch.nn.Module):\n",
    "    \"\"\"Top-level module for any complete `nequip` model.\n",
    "\n",
    "    Manages top-level rescaling, dtypes, and more.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model_dtype: torch.dtype\n",
    "    model_input_fields: List[str]\n",
    "\n",
    "    _num_rescale_layers: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: GraphModuleMixin,\n",
    "        model_dtype: Optional[torch.dtype] = None,\n",
    "        model_input_fields: Dict[str, Any] = {},\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        irreps_in = {\n",
    "            # Things that always make sense as inputs:\n",
    "            AtomicDataDict.POSITIONS_KEY: \"1o\",\n",
    "            AtomicDataDict.EDGE_INDEX_KEY: None,\n",
    "            AtomicDataDict.EDGE_CELL_SHIFT_KEY: None,\n",
    "            AtomicDataDict.CELL_KEY: \"1o\",  # 3 of them, but still\n",
    "            AtomicDataDict.BATCH_KEY: None,\n",
    "            AtomicDataDict.BATCH_PTR_KEY: None,\n",
    "            AtomicDataDict.ATOM_TYPE_KEY: None,\n",
    "        }\n",
    "        model_input_fields = AtomicDataDict._fix_irreps_dict(model_input_fields)\n",
    "        assert len(set(irreps_in.keys()).intersection(model_input_fields.keys())) == 0\n",
    "        irreps_in.update(model_input_fields)\n",
    "        self._init_irreps(irreps_in=irreps_in, irreps_out=model.irreps_out)\n",
    "        for k, irreps in model.irreps_in.items():\n",
    "            if self.irreps_in.get(k, None) != irreps:\n",
    "                raise RuntimeError(\n",
    "                    f\"Model has `{k}` in its irreps_in with irreps `{irreps}`, but `{k}` is missing from/has inconsistent irreps in model_input_fields of `{self.irreps_in.get(k, 'missing')}`\"\n",
    "                )\n",
    "        self.model = model\n",
    "        self.model_dtype = (\n",
    "            model_dtype if model_dtype is not None else torch.get_default_dtype()\n",
    "        )\n",
    "        self.model_input_fields = list(self.irreps_in.keys())\n",
    "\n",
    "        self._num_rescale_layers = 0\n",
    "        outer_layer = self.model\n",
    "        while isinstance(outer_layer, RescaleOutput):\n",
    "            self._num_rescale_layers += 1\n",
    "            outer_layer = outer_layer.model\n",
    "\n",
    "    # == Rescaling ==\n",
    "    @torch.jit.unused\n",
    "    def all_RescaleOutputs(self) -> List[RescaleOutput]:\n",
    "        \"\"\"All ``RescaleOutput``s wrapping the model, in evaluation order.\"\"\"\n",
    "        if self._num_rescale_layers == 0:\n",
    "            return []\n",
    "        # we know there's at least one\n",
    "        out = [self.model]\n",
    "        for _ in range(self._num_rescale_layers - 1):\n",
    "            out.append(out[-1].model)\n",
    "        # we iterated outermost to innermost, which is opposite of evaluation order\n",
    "        assert len(out) == self._num_rescale_layers\n",
    "        return out[::-1]\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def unscale(\n",
    "        self, data: AtomicDataDict.Type, force_process: bool = False\n",
    "    ) -> AtomicDataDict.Type:\n",
    "        data_unscaled = data.copy()\n",
    "        # we need to unscale from the outside-in:\n",
    "        for layer in self.all_RescaleOutputs()[::-1]:\n",
    "            data_unscaled = layer.unscale(data_unscaled, force_process=force_process)\n",
    "        return data_unscaled\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def scale(\n",
    "        self, data: AtomicDataDict.Type, force_process: bool = False\n",
    "    ) -> AtomicDataDict.Type:\n",
    "        data_scaled = data.copy()\n",
    "        # we need to scale from the inside out:\n",
    "        for layer in self.all_RescaleOutputs():\n",
    "            data_scaled = layer.scale(data_scaled, force_process=force_process)\n",
    "        return data_scaled\n",
    "\n",
    "    # == Inference ==\n",
    "\n",
    "    def forward(self, data: AtomicDataDict.Type) -> AtomicDataDict.Type:\n",
    "        # restrict the input data to allowed keys, and cast to model_dtype\n",
    "        # this also prevents the model from direclty using the dict from the outside,\n",
    "        # preventing weird pass-by-reference bugs\n",
    "        new_data: AtomicDataDict.Type = {}\n",
    "        for k, v in data.items():\n",
    "            if k in self.model_input_fields:\n",
    "                if v.is_floating_point():\n",
    "                    v = v.to(dtype=self.model_dtype)\n",
    "                new_data[k] = v\n",
    "        # run the model\n",
    "        data = self.model(new_data)\n",
    "        return data\n",
    "\n",
    "    # == Helpers ==\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def get_device(self) -> torch.device:\n",
    "        return _get_device(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4128a10-3203-45b6-953a-fd32f27d119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EnergyModel(\n",
    "    config, initialize: bool, dataset: Optional[AtomicDataset] = None\n",
    ") -> SequentialGraphNetwork:\n",
    "    \"\"\"Base default energy model archetecture.\n",
    "\n",
    "    For minimal and full configuration option listings, see ``minimal.yaml`` and ``example.yaml``.\n",
    "    \"\"\"\n",
    "    logging.debug(\"Start building the network model\")\n",
    "\n",
    "    builder_utils.add_avg_num_neighbors(\n",
    "        config=config, initialize=initialize, dataset=dataset\n",
    "    )\n",
    "\n",
    "    num_layers = config.get(\"num_layers\", 3)\n",
    "\n",
    "    layers = {\n",
    "        # -- Encode --\n",
    "        \"one_hot\": OneHotAtomEncoding,\n",
    "        \"spharm_edges\": SphericalHarmonicEdgeAttrs,\n",
    "        \"radial_basis\": RadialBasisEdgeEncoding,\n",
    "        # -- Embed features --\n",
    "        \"chemical_embedding\": AtomwiseLinear,\n",
    "    }\n",
    "\n",
    "    # add convnet layers\n",
    "    # insertion preserves order\n",
    "    for layer_i in range(num_layers):\n",
    "        layers[f\"layer{layer_i}_convnet\"] = ConvNetLayer\n",
    "\n",
    "    # .update also maintains insertion order\n",
    "    layers.update(\n",
    "        {\n",
    "            # TODO: the next linear throws out all L > 0, don't create them in the last layer of convnet\n",
    "            # -- output block --\n",
    "            \"conv_to_output_hidden\": AtomwiseLinear,\n",
    "            \"output_hidden_to_scalar\": (\n",
    "                AtomwiseLinear,\n",
    "                dict(irreps_out=\"1x0e\", out_field=AtomicDataDict.PER_ATOM_ENERGY_KEY),\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    layers[\"total_energy_sum\"] = (\n",
    "        AtomwiseReduce,\n",
    "        dict(\n",
    "            reduce=\"sum\",\n",
    "            field=AtomicDataDict.PER_ATOM_ENERGY_KEY,\n",
    "            out_field=AtomicDataDict.TOTAL_ENERGY_KEY,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return SequentialGraphNetwork.from_parameters(\n",
    "        shared_params=config,\n",
    "        layers=layers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ef3e88c-0b9a-4c19-9864-46757ce76ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SimpleIrrepsConfig',\n",
       " 'EnergyModel',\n",
       " 'PerSpeciesRescale',\n",
       " 'StressForceOutput',\n",
       " 'RescaleEnergyEtc']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build\n",
    "#builders = [\n",
    "#    load_callable(b, prefix=\"model\")\n",
    "#    for b in config.get(\"model_builders\", [])\n",
    "#]\n",
    "\n",
    "config.get(\"model_builders\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fb6bd1d-fa11-4517-9ee4-32d2165531a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one_hot': 1,\n",
       " 'spharm_edges': 1,\n",
       " 'radial_basis': 1,\n",
       " 'chemical_embedding': 1,\n",
       " 'layer0_convnet': 1,\n",
       " 'layer1_convnet': 1,\n",
       " 'layer2_convnet': 1,\n",
       " 'conv_to_output_hidden': 1,\n",
       " 'output_hidden_to_scalar': 1,\n",
       " 'total_energy_sum': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = {\n",
    "    # -- Encode --\n",
    "    \"one_hot\": 1,\n",
    "    \"spharm_edges\": 1,\n",
    "    \"radial_basis\": 1,\n",
    "    # -- Embed features --\n",
    "    \"chemical_embedding\": 1,\n",
    "}\n",
    "\n",
    "# add convnet layers\n",
    "# insertion preserves order\n",
    "for layer_i in range(3):\n",
    "    layers[f\"layer{layer_i}_convnet\"] = 1\n",
    "\n",
    "# .update also maintains insertion order\n",
    "layers.update(\n",
    "    {\n",
    "        # TODO: the next linear throws out all L > 0, don't create them in the last layer of convnet\n",
    "        # -- output block --\n",
    "        \"conv_to_output_hidden\": 1,\n",
    "        \"output_hidden_to_scalar\": 1\n",
    "    }\n",
    ")\n",
    "\n",
    "layers[\"total_energy_sum\"] = 1\n",
    "\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3341fa1e-457b-4754-b0f0-6d04252fd2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=None, **Config.as_dict(config))\n",
    "\n",
    "# what is this\n",
    "# to update wandb data?\n",
    "config.update(trainer.params)\n",
    "\n",
    "# = Load the dataset =\n",
    "dataset = dataset_from_config(config, prefix=\"dataset\")\n",
    "\n",
    "# = Train/test split =\n",
    "trainer.set_dataset(dataset, validation_dataset)\n",
    "\n",
    "# = Build model =\n",
    "final_model = model_from_config(\n",
    "    config=config, initialize=True, dataset=trainer.dataset_train\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_mkl",
   "language": "python",
   "name": "torch_mkl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
