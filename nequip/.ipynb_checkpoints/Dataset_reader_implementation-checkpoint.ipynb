{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "681e2b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nequip.data.AtomicData.AtomicData'>\n",
      "Total energy: tensor([-189.3690])\n",
      "Force on atom 0: tensor([0.0591, 0.8510, 0.0488])\n"
     ]
    }
   ],
   "source": [
    "# Refrence example. Want to know how this one works\n",
    "\n",
    "import torch\n",
    "import ase.io\n",
    "from nequip.data import AtomicData, AtomicDataDict\n",
    "from nequip.scripts.deploy import load_deployed_model, R_MAX_KEY\n",
    "\n",
    "#device = \"cpu\"  # \"cuda\" etc.\n",
    "#model, metadata = load_deployed_model(\n",
    "#    \"models/NaBr/NaBr-deployed.pth\",\n",
    "#    device=device,\n",
    "#)\n",
    "\n",
    "# Load some input structure from an XYZ or other ASE readable file:\n",
    "data = AtomicData.from_ase(ase.io.read(\"./datasets/NaBr-data.xyz\"), r_max=5.)\n",
    "#data = data.to(device)\n",
    "\n",
    "#out = model(AtomicData.to_AtomicDataDict(data))\n",
    "\n",
    "print(type(data))\n",
    "print(f\"Total energy: {data[AtomicDataDict.TOTAL_ENERGY_KEY]}\")\n",
    "print(f\"Force on atom 0: {data[AtomicDataDict.FORCE_KEY][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d55f976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtomicData(atomic_numbers=[64, 1], cell=[3, 3], edge_cell_shift=[1226, 3], edge_index=[2, 1226], forces=[64, 3], free_energy=[1], pbc=[3], pos=[64, 3], stress=[3, 3], total_energy=[1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7d5280",
   "metadata": {},
   "source": [
    "## AtomicDataDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98a55c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Keys for dictionaries/AtomicData objects.\n",
    "\n",
    "This is a seperate module to compensate for a TorchScript bug that can only recognize constants \n",
    "when they are accessed as attributes of an imported module.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "if sys.version_info[1] >= 8:\n",
    "    from typing import Final\n",
    "else:\n",
    "    from typing_extensions import Final\n",
    "\n",
    "# == Define allowed keys as constants ==\n",
    "# The positions of the atoms in the system\n",
    "POSITIONS_KEY: Final[str] = \"pos\"\n",
    "# The [2, n_edge] index tensor giving center -> neighbor relations\n",
    "EDGE_INDEX_KEY: Final[str] = \"edge_index\"\n",
    "# A [n_edge, 3] tensor of how many periodic cells each edge crosses in each cell vector\n",
    "EDGE_CELL_SHIFT_KEY: Final[str] = \"edge_cell_shift\"\n",
    "# [n_batch, 3, 3] or [3, 3] tensor where rows are the cell vectors\n",
    "CELL_KEY: Final[str] = \"cell\"\n",
    "# [n_batch, 3] bool tensor\n",
    "PBC_KEY: Final[str] = \"pbc\"\n",
    "# [n_atom, 1] long tensor\n",
    "ATOMIC_NUMBERS_KEY: Final[str] = \"atomic_numbers\"\n",
    "# [n_atom, 1] long tensor\n",
    "ATOM_TYPE_KEY: Final[str] = \"atom_types\"\n",
    "\n",
    "BASIC_STRUCTURE_KEYS: Final[List[str]] = [\n",
    "    POSITIONS_KEY,\n",
    "    EDGE_INDEX_KEY,\n",
    "    EDGE_CELL_SHIFT_KEY,\n",
    "    CELL_KEY,\n",
    "    PBC_KEY,\n",
    "    ATOM_TYPE_KEY,\n",
    "    ATOMIC_NUMBERS_KEY,\n",
    "]\n",
    "\n",
    "# A [n_edge, 3] tensor of displacement vectors associated to edges\n",
    "EDGE_VECTORS_KEY: Final[str] = \"edge_vectors\"\n",
    "# A [n_edge] tensor of the lengths of EDGE_VECTORS\n",
    "EDGE_LENGTH_KEY: Final[str] = \"edge_lengths\"\n",
    "# [n_edge, dim] (possibly equivariant) attributes of each edge\n",
    "EDGE_ATTRS_KEY: Final[str] = \"edge_attrs\"\n",
    "# [n_edge, dim] invariant embedding of the edges\n",
    "EDGE_EMBEDDING_KEY: Final[str] = \"edge_embedding\"\n",
    "EDGE_FEATURES_KEY: Final[str] = \"edge_features\"\n",
    "# [n_edge, 1] invariant of the radial cutoff envelope for each edge, allows reuse of cutoff envelopes\n",
    "EDGE_CUTOFF_KEY: Final[str] = \"edge_cutoff\"\n",
    "# edge energy as in Allegro\n",
    "EDGE_ENERGY_KEY: Final[str] = \"edge_energy\"\n",
    "\n",
    "NODE_FEATURES_KEY: Final[str] = \"node_features\"\n",
    "NODE_ATTRS_KEY: Final[str] = \"node_attrs\"\n",
    "\n",
    "PER_ATOM_ENERGY_KEY: Final[str] = \"atomic_energy\"\n",
    "TOTAL_ENERGY_KEY: Final[str] = \"total_energy\"\n",
    "FORCE_KEY: Final[str] = \"forces\"\n",
    "PARTIAL_FORCE_KEY: Final[str] = \"partial_forces\"\n",
    "STRESS_KEY: Final[str] = \"stress\"\n",
    "VIRIAL_KEY: Final[str] = \"virial\"\n",
    "\n",
    "ALL_ENERGY_KEYS: Final[List[str]] = [\n",
    "    EDGE_ENERGY_KEY,\n",
    "    PER_ATOM_ENERGY_KEY,\n",
    "    TOTAL_ENERGY_KEY,\n",
    "    FORCE_KEY,\n",
    "    PARTIAL_FORCE_KEY,\n",
    "    STRESS_KEY,\n",
    "    VIRIAL_KEY,\n",
    "]\n",
    "\n",
    "BATCH_KEY: Final[str] = \"batch\"\n",
    "BATCH_PTR_KEY: Final[str] = \"ptr\"\n",
    "\n",
    "# Make a list of allowed keys\n",
    "ALLOWED_KEYS: List[str] = [\n",
    "    getattr(sys.modules[__name__], k)\n",
    "    for k in sys.modules[__name__].__dict__.keys()\n",
    "    if k.endswith(\"_KEY\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00c5c3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"nequip.data.jit: TorchScript functions for dealing with AtomicData.\n",
    "\n",
    "These TorchScript functions operate on ``Dict[str, torch.Tensor]`` representations\n",
    "of the ``AtomicData`` class which are produced by ``AtomicData.to_AtomicDataDict()``.\n",
    "\n",
    "Authors: Albert Musaelian\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.jit\n",
    "\n",
    "from e3nn import o3\n",
    "\n",
    "# Make the keys available in this module\n",
    "#from ._keys import *  # noqa: F403, F401\n",
    "\n",
    "# Also import the module to use in TorchScript, this is a hack to avoid bug:\n",
    "# https://github.com/pytorch/pytorch/issues/52312\n",
    "#from . import _keys\n",
    "\n",
    "# Define a type alias\n",
    "Type = Dict[str, torch.Tensor]\n",
    "\n",
    "\n",
    "def validate_keys(keys, graph_required=True):\n",
    "    # Validate combinations\n",
    "    if graph_required:\n",
    "        if not (POSITIONS_KEY in keys and EDGE_INDEX_KEY in keys):\n",
    "            raise KeyError(\"At least pos and edge_index must be supplied\")\n",
    "    if EDGE_CELL_SHIFT_KEY in keys and \"cell\" not in keys:\n",
    "        raise ValueError(\"If `edge_cell_shift` given, `cell` must be given.\")\n",
    "\n",
    "\n",
    "_SPECIAL_IRREPS = [None]\n",
    "\n",
    "\n",
    "def _fix_irreps_dict(d: Dict[str, Any]):\n",
    "    \"\"\"Just adds a zero irrep to the dict of irreps\"\"\"\n",
    "    return {k: (i if i in _SPECIAL_IRREPS else o3.Irreps(i)) for k, i in d.items()}\n",
    "\n",
    "\n",
    "def _irreps_compatible(ir1: Dict[str, o3.Irreps], ir2: Dict[str, o3.Irreps]):\n",
    "    \"\"\"Checks if two dicts have the same irreps\"\"\"\n",
    "    return all(ir1[k] == ir2[k] for k in ir1 if k in ir2)\n",
    "\n",
    "\n",
    "#@torch.jit.script\n",
    "def with_edge_vectors(data: Type, with_lengths: bool = True) -> Type:\n",
    "    \"\"\"Compute the edge displacement vectors for a graph.\n",
    "\n",
    "    If ``data.pos.requires_grad`` and/or ``data.cell.requires_grad``, this\n",
    "    method will return edge vectors correctly connected in the autograd graph.\n",
    "\n",
    "    Returns:\n",
    "        Tensor [n_edges, 3] edge displacement vectors\n",
    "    \"\"\"\n",
    "    # If already computed\n",
    "    if EDGE_VECTORS_KEY in data:\n",
    "        if with_lengths and EDGE_LENGTH_KEY not in data:\n",
    "            data[EDGE_LENGTH_KEY] = torch.linalg.norm(\n",
    "                data[EDGE_VECTORS_KEY], dim=-1\n",
    "            )\n",
    "        return data\n",
    "    else:\n",
    "        # Build it dynamically\n",
    "        # Note that this is\n",
    "        # (1) backwardable, because everything (pos, cell, shifts)\n",
    "        #     is Tensors.\n",
    "        # (2) works on a Batch constructed from AtomicData\n",
    "        pos = data[POSITIONS_KEY]\n",
    "        edge_index = data[EDGE_INDEX_KEY]\n",
    "        # edge_vec = pos[edge_index[1]] - pos[edge_index[0]]\n",
    "        edge_vec = torch.index_select(pos, 0, edge_index[1]) - torch.index_select(\n",
    "            pos, 0, edge_index[0]\n",
    "        )\n",
    "        if CELL_KEY in data: # just cell is given in data\n",
    "            # ^ note that to save time we don't check that the edge_cell_shifts \n",
    "            # are trivial if no cell is provided; we just assume they are either not present or all zero.\n",
    "            # -1 gives a batch dim no matter what\n",
    "            cell = data[CELL_KEY].view(-1, 3, 3)\n",
    "            edge_cell_shift = data[_keys.EDGE_CELL_SHIFT_KEY]\n",
    "            if cell.shape[0] > 1:\n",
    "                batch = data[BATCH_KEY]\n",
    "                # Cell has a batch dimension\n",
    "                # note the ASE cell vectors as rows convention\n",
    "                edge_vec = edge_vec + torch.einsum(\n",
    "                    \"ni,nij->nj\", edge_cell_shift, cell[batch[edge_index[0]]]\n",
    "                )\n",
    "                # TODO: is there a more efficient way to do the above without\n",
    "                # creating an [n_edge] and [n_edge, 3, 3] tensor?\n",
    "            else:\n",
    "                # Cell has either no batch dimension, or a useless one,\n",
    "                # so we can avoid creating the large intermediate cell tensor.\n",
    "                # Note that we do NOT check that the batch array, if it is present,\n",
    "                # is trivial — but this does need to be consistent.\n",
    "                edge_vec = edge_vec + torch.einsum(\n",
    "                    \"ni,ij->nj\",\n",
    "                    edge_cell_shift,\n",
    "                    cell.squeeze(0),  # remove batch dimension\n",
    "                )\n",
    "        data[EDGE_VECTORS_KEY] = edge_vec\n",
    "        if with_lengths:\n",
    "            data[EDGE_LENGTH_KEY] = torch.linalg.norm(edge_vec, dim=-1)\n",
    "        return data\n",
    "\n",
    "\n",
    "#@torch.jit.script\n",
    "def with_batch(data: Type) -> Type:\n",
    "    \"\"\"Get batch Tensor.\n",
    "\n",
    "    If this AtomicDataPrimitive has no ``batch``, one of all zeros will be\n",
    "    allocated and returned.\n",
    "    \"\"\"\n",
    "    if BATCH_KEY in data:\n",
    "        return data\n",
    "    else:\n",
    "        pos = data[POSITIONS_KEY]\n",
    "        batch = torch.zeros(len(pos), dtype=torch.long, device=pos.device)\n",
    "        data[BATCH_KEY] = batch\n",
    "        # ugly way to make a tensor of [0, len(pos)], but it avoids transfers or casts\n",
    "        data[BATCH_PTR_KEY] = torch.arange(\n",
    "            start=0,\n",
    "            end=len(pos) + 1,\n",
    "            step=len(pos),\n",
    "            dtype=torch.long,\n",
    "            device=pos.device,\n",
    "        )\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8572bfc4",
   "metadata": {},
   "source": [
    "### Some utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d151ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# There is no built-in way to check if a Tensor is of an integer type\n",
    "_TORCH_INTEGER_DTYPES = (torch.int, torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9a116f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "\n",
    "# from ..utils.num_nodes import maybe_num_nodes\n",
    "\n",
    "__num_nodes_warn_msg__ = (\n",
    "    \"The number of nodes in your data object can only be inferred by its {} \"\n",
    "    \"indices, and hence may result in unexpected batch-wise behavior, e.g., \"\n",
    "    \"in case there exists isolated nodes. Please consider explicitly setting \"\n",
    "    \"the number of nodes for this data object by assigning it to \"\n",
    "    \"data.num_nodes.\"\n",
    ")\n",
    "\n",
    "\n",
    "def size_repr(key, item, indent=0):\n",
    "    indent_str = \" \" * indent\n",
    "    if torch.is_tensor(item) and item.dim() == 0:\n",
    "        out = item.item()\n",
    "    elif torch.is_tensor(item):\n",
    "        out = str(list(item.size()))\n",
    "    elif isinstance(item, list) or isinstance(item, tuple):\n",
    "        out = str([len(item)])\n",
    "    elif isinstance(item, dict):\n",
    "        lines = [indent_str + size_repr(k, v, 2) for k, v in item.items()]\n",
    "        out = \"{\\n\" + \",\\n\".join(lines) + \"\\n\" + indent_str + \"}\"\n",
    "    elif isinstance(item, str):\n",
    "        out = f'\"{item}\"'\n",
    "    else:\n",
    "        out = str(item)\n",
    "\n",
    "    return f\"{indent_str}{key}={out}\"\n",
    "\n",
    "\n",
    "class Data(object):\n",
    "    r\"\"\"A plain old python object modeling a single graph with various\n",
    "    (optional) attributes:\n",
    "\n",
    "    Args:\n",
    "        x (Tensor, optional): Node feature matrix with shape :obj:`[num_nodes,\n",
    "            num_node_features]`. (default: :obj:`None`)\n",
    "        edge_index (LongTensor, optional): Graph connectivity in COO format\n",
    "            with shape :obj:`[2, num_edges]`. (default: :obj:`None`)\n",
    "        edge_attr (Tensor, optional): Edge feature matrix with shape\n",
    "            :obj:`[num_edges, num_edge_features]`. (default: :obj:`None`)\n",
    "        y (Tensor, optional): Graph or node targets with arbitrary shape.\n",
    "            (default: :obj:`None`)\n",
    "        pos (Tensor, optional): Node position matrix with shape\n",
    "            :obj:`[num_nodes, num_dimensions]`. (default: :obj:`None`)\n",
    "        normal (Tensor, optional): Normal vector matrix with shape\n",
    "            :obj:`[num_nodes, num_dimensions]`. (default: :obj:`None`)\n",
    "        face (LongTensor, optional): Face adjacency matrix with shape\n",
    "            :obj:`[3, num_faces]`. (default: :obj:`None`)\n",
    "\n",
    "    The data object is not restricted to these attributes and can be extented\n",
    "    by any other additional data.\n",
    "\n",
    "    Example::\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index)\n",
    "        data.train_idx = torch.tensor([...], dtype=torch.long)\n",
    "        data.test_mask = torch.tensor([...], dtype=torch.bool)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        x=None,\n",
    "        edge_index=None,\n",
    "        edge_attr=None,\n",
    "        y=None,\n",
    "        pos=None,\n",
    "        normal=None,\n",
    "        face=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.x = x\n",
    "        self.edge_index = edge_index\n",
    "        self.edge_attr = edge_attr\n",
    "        self.y = y\n",
    "        self.pos = pos\n",
    "        self.normal = normal\n",
    "        self.face = face\n",
    "        for key, item in kwargs.items():\n",
    "            if key == \"num_nodes\":\n",
    "                self.__num_nodes__ = item\n",
    "            else:\n",
    "                self[key] = item\n",
    "\n",
    "        if edge_index is not None and edge_index.dtype != torch.long:\n",
    "            raise ValueError(\n",
    "                (\n",
    "                    f\"Argument `edge_index` needs to be of type `torch.long` but \"\n",
    "                    f\"found type `{edge_index.dtype}`.\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if face is not None and face.dtype != torch.long:\n",
    "            raise ValueError(\n",
    "                (\n",
    "                    f\"Argument `face` needs to be of type `torch.long` but found \"\n",
    "                    f\"type `{face.dtype}`.\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, dictionary):\n",
    "        r\"\"\"Creates a data object from a python dictionary.\"\"\"\n",
    "        data = cls()\n",
    "\n",
    "        for key, item in dictionary.items():\n",
    "            data[key] = item\n",
    "\n",
    "        return data\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {key: item for key, item in self}\n",
    "\n",
    "    def to_namedtuple(self):\n",
    "        keys = self.keys\n",
    "        DataTuple = collections.namedtuple(\"DataTuple\", keys)\n",
    "        return DataTuple(*[self[key] for key in keys])\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        r\"\"\"Gets the data of the attribute :obj:`key`.\"\"\"\n",
    "        return getattr(self, key, None)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        \"\"\"Sets the attribute :obj:`key` to :obj:`value`.\"\"\"\n",
    "        setattr(self, key, value)\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        r\"\"\"Delete the data of the attribute :obj:`key`.\"\"\"\n",
    "        return delattr(self, key)\n",
    "\n",
    "    @property\n",
    "    def keys(self):\n",
    "        r\"\"\"Returns all names of graph attributes.\"\"\"\n",
    "        keys = [key for key in self.__dict__.keys() if self[key] is not None]\n",
    "        keys = [key for key in keys if key[:2] != \"__\" and key[-2:] != \"__\"]\n",
    "        return keys\n",
    "\n",
    "    def __len__(self):\n",
    "        r\"\"\"Returns the number of all present attributes.\"\"\"\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        r\"\"\"Returns :obj:`True`, if the attribute :obj:`key` is present in the\n",
    "        data.\"\"\"\n",
    "        return key in self.keys\n",
    "\n",
    "    def __iter__(self):\n",
    "        r\"\"\"Iterates over all present attributes in the data, yielding their\n",
    "        attribute names and content.\"\"\"\n",
    "        for key in sorted(self.keys):\n",
    "            yield key, self[key]\n",
    "\n",
    "    def __call__(self, *keys):\n",
    "        r\"\"\"Iterates over all attributes :obj:`*keys` in the data, yielding\n",
    "        their attribute names and content.\n",
    "        If :obj:`*keys` is not given this method will iterative over all\n",
    "        present attributes.\"\"\"\n",
    "        for key in sorted(self.keys) if not keys else keys:\n",
    "            if key in self:\n",
    "                yield key, self[key]\n",
    "\n",
    "    def __cat_dim__(self, key, value):\n",
    "        r\"\"\"Returns the dimension for which :obj:`value` of attribute\n",
    "        :obj:`key` will get concatenated when creating batches.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            This method is for internal use only, and should only be overridden\n",
    "            if the batch concatenation process is corrupted for a specific data\n",
    "            attribute.\n",
    "        \"\"\"\n",
    "        if bool(re.search(\"(index|face)\", key)):\n",
    "            return -1\n",
    "        return 0\n",
    "\n",
    "    def __inc__(self, key, value):\n",
    "        r\"\"\"Returns the incremental count to cumulatively increase the value\n",
    "        of the next attribute of :obj:`key` when creating batches.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            This method is for internal use only, and should only be overridden\n",
    "            if the batch concatenation process is corrupted for a specific data\n",
    "            attribute.\n",
    "        \"\"\"\n",
    "        # Only `*index*` and `*face*` attributes should be cumulatively summed\n",
    "        # up when creating batches.\n",
    "        return self.num_nodes if bool(re.search(\"(index|face)\", key)) else 0\n",
    "\n",
    "    @property\n",
    "    def num_nodes(self):\n",
    "        r\"\"\"Returns or sets the number of nodes in the graph.\n",
    "\n",
    "        .. note::\n",
    "            The number of nodes in your data object is typically automatically\n",
    "            inferred, *e.g.*, when node features :obj:`x` are present.\n",
    "            In some cases however, a graph may only be given by its edge\n",
    "            indices :obj:`edge_index`.\n",
    "            PyTorch Geometric then *guesses* the number of nodes\n",
    "            according to :obj:`edge_index.max().item() + 1`, but in case there\n",
    "            exists isolated nodes, this number has not to be correct and can\n",
    "            therefore result in unexpected batch-wise behavior.\n",
    "            Thus, we recommend to set the number of nodes in your data object\n",
    "            explicitly via :obj:`data.num_nodes = ...`.\n",
    "            You will be given a warning that requests you to do so.\n",
    "        \"\"\"\n",
    "        if hasattr(self, \"__num_nodes__\"):\n",
    "            return self.__num_nodes__\n",
    "        for key, item in self(\"x\", \"pos\", \"normal\", \"batch\"):\n",
    "            return item.size(self.__cat_dim__(key, item))\n",
    "        if hasattr(self, \"adj\"):\n",
    "            return self.adj.size(0)\n",
    "        if hasattr(self, \"adj_t\"):\n",
    "            return self.adj_t.size(1)\n",
    "        # if self.face is not None:\n",
    "        #     logging.warning(__num_nodes_warn_msg__.format(\"face\"))\n",
    "        #     return maybe_num_nodes(self.face)\n",
    "        # if self.edge_index is not None:\n",
    "        #     logging.warning(__num_nodes_warn_msg__.format(\"edge\"))\n",
    "        #     return maybe_num_nodes(self.edge_index)\n",
    "        return None\n",
    "\n",
    "    @num_nodes.setter # I guess python have override of functions for classes\n",
    "    def num_nodes(self, num_nodes):\n",
    "        self.__num_nodes__ = num_nodes\n",
    "\n",
    "    @property\n",
    "    def num_edges(self):\n",
    "        \"\"\"\n",
    "        Returns the number of edges in the graph.\n",
    "        For undirected graphs, this will return the number of bi-directional\n",
    "        edges, which is double the amount of unique edges.\n",
    "        \"\"\"\n",
    "        for key, item in self(\"edge_index\", \"edge_attr\"):\n",
    "            return item.size(self.__cat_dim__(key, item))\n",
    "        for key, item in self(\"adj\", \"adj_t\"):\n",
    "            return item.nnz()\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def num_faces(self):\n",
    "        r\"\"\"Returns the number of faces in the mesh.\"\"\"\n",
    "        if self.face is not None:\n",
    "            return self.face.size(self.__cat_dim__(\"face\", self.face))\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def num_node_features(self):\n",
    "        r\"\"\"Returns the number of features per node in the graph.\"\"\"\n",
    "        if self.x is None:\n",
    "            return 0\n",
    "        return 1 if self.x.dim() == 1 else self.x.size(1)\n",
    "\n",
    "    @property\n",
    "    def num_features(self):\n",
    "        r\"\"\"Alias for :py:attr:`~num_node_features`.\"\"\"\n",
    "        return self.num_node_features\n",
    "\n",
    "    @property\n",
    "    def num_edge_features(self):\n",
    "        r\"\"\"Returns the number of features per edge in the graph.\"\"\"\n",
    "        if self.edge_attr is None:\n",
    "            return 0\n",
    "        return 1 if self.edge_attr.dim() == 1 else self.edge_attr.size(1)\n",
    "\n",
    "    def __apply__(self, item, func):\n",
    "        if torch.is_tensor(item):\n",
    "            return func(item)\n",
    "        elif isinstance(item, (tuple, list)):\n",
    "            return [self.__apply__(v, func) for v in item]\n",
    "        elif isinstance(item, dict):\n",
    "            return {k: self.__apply__(v, func) for k, v in item.items()}\n",
    "        else:\n",
    "            return item\n",
    "\n",
    "    def apply(self, func, *keys):\n",
    "        r\"\"\"Applies the function :obj:`func` to all tensor attributes\n",
    "        :obj:`*keys`. If :obj:`*keys` is not given, :obj:`func` is applied to\n",
    "        all present attributes.\n",
    "        \"\"\"\n",
    "        for key, item in self(*keys):\n",
    "            self[key] = self.__apply__(item, func)\n",
    "        return self\n",
    "\n",
    "    def contiguous(self, *keys):\n",
    "        r\"\"\"Ensures a contiguous memory layout for all attributes :obj:`*keys`.\n",
    "        If :obj:`*keys` is not given, all present attributes are ensured to\n",
    "        have a contiguous memory layout.\"\"\"\n",
    "        return self.apply(lambda x: x.contiguous(), *keys)\n",
    "\n",
    "    def to(self, device, *keys, **kwargs):\n",
    "        r\"\"\"Performs tensor dtype and/or device conversion to all attributes\n",
    "        :obj:`*keys`.\n",
    "        If :obj:`*keys` is not given, the conversion is applied to all present\n",
    "        attributes.\"\"\"\n",
    "        return self.apply(lambda x: x.to(device, **kwargs), *keys)\n",
    "\n",
    "    def cpu(self, *keys):\n",
    "        r\"\"\"Copies all attributes :obj:`*keys` to CPU memory.\n",
    "        If :obj:`*keys` is not given, the conversion is applied to all present\n",
    "        attributes.\"\"\"\n",
    "        return self.apply(lambda x: x.cpu(), *keys)\n",
    "\n",
    "    def cuda(self, device=None, non_blocking=False, *keys):\n",
    "        r\"\"\"Copies all attributes :obj:`*keys` to CUDA memory.\n",
    "        If :obj:`*keys` is not given, the conversion is applied to all present\n",
    "        attributes.\"\"\"\n",
    "        return self.apply(\n",
    "            lambda x: x.cuda(device=device, non_blocking=non_blocking), *keys\n",
    "        )\n",
    "\n",
    "    def clone(self):\n",
    "        r\"\"\"Performs a deep-copy of the data object.\"\"\"\n",
    "        return self.__class__.from_dict(\n",
    "            {\n",
    "                k: v.clone() if torch.is_tensor(v) else copy.deepcopy(v)\n",
    "                for k, v in self.__dict__.items()\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def pin_memory(self, *keys):\n",
    "        r\"\"\"Copies all attributes :obj:`*keys` to pinned memory.\n",
    "        If :obj:`*keys` is not given, the conversion is applied to all present\n",
    "        attributes.\"\"\"\n",
    "        return self.apply(lambda x: x.pin_memory(), *keys)\n",
    "\n",
    "    def debug(self):\n",
    "        if self.edge_index is not None:\n",
    "            if self.edge_index.dtype != torch.long:\n",
    "                raise RuntimeError(\n",
    "                    (\n",
    "                        \"Expected edge indices of dtype {}, but found dtype \" \" {}\"\n",
    "                    ).format(torch.long, self.edge_index.dtype)\n",
    "                )\n",
    "\n",
    "        if self.face is not None:\n",
    "            if self.face.dtype != torch.long:\n",
    "                raise RuntimeError(\n",
    "                    (\n",
    "                        \"Expected face indices of dtype {}, but found dtype \" \" {}\"\n",
    "                    ).format(torch.long, self.face.dtype)\n",
    "                )\n",
    "\n",
    "        if self.edge_index is not None:\n",
    "            if self.edge_index.dim() != 2 or self.edge_index.size(0) != 2:\n",
    "                raise RuntimeError(\n",
    "                    (\n",
    "                        \"Edge indices should have shape [2, num_edges] but found\"\n",
    "                        \" shape {}\"\n",
    "                    ).format(self.edge_index.size())\n",
    "                )\n",
    "\n",
    "        if self.edge_index is not None and self.num_nodes is not None:\n",
    "            if self.edge_index.numel() > 0:\n",
    "                min_index = self.edge_index.min()\n",
    "                max_index = self.edge_index.max()\n",
    "            else:\n",
    "                min_index = max_index = 0\n",
    "            if min_index < 0 or max_index > self.num_nodes - 1:\n",
    "                raise RuntimeError(\n",
    "                    (\n",
    "                        \"Edge indices must lay in the interval [0, {}]\"\n",
    "                        \" but found them in the interval [{}, {}]\"\n",
    "                    ).format(self.num_nodes - 1, min_index, max_index)\n",
    "                )\n",
    "\n",
    "        if self.face is not None:\n",
    "            if self.face.dim() != 2 or self.face.size(0) != 3:\n",
    "                raise RuntimeError(\n",
    "                    (\n",
    "                        \"Face indices should have shape [3, num_faces] but found\"\n",
    "                        \" shape {}\"\n",
    "                    ).format(self.face.size())\n",
    "                )\n",
    "\n",
    "        if self.face is not None and self.num_nodes is not None:\n",
    "            if self.face.numel() > 0:\n",
    "                min_index = self.face.min()\n",
    "                max_index = self.face.max()\n",
    "            else:\n",
    "                min_index = max_index = 0\n",
    "            if min_index < 0 or max_index > self.num_nodes - 1:\n",
    "                raise RuntimeError(\n",
    "                    (\n",
    "                        \"Face indices must lay in the interval [0, {}]\"\n",
    "                        \" but found them in the interval [{}, {}]\"\n",
    "                    ).format(self.num_nodes - 1, min_index, max_index)\n",
    "                )\n",
    "\n",
    "        if self.edge_index is not None and self.edge_attr is not None:\n",
    "            if self.edge_index.size(1) != self.edge_attr.size(0):\n",
    "                raise RuntimeError(\n",
    "                    (\n",
    "                        \"Edge indices and edge attributes hold a differing \"\n",
    "                        \"number of edges, found {} and {}\"\n",
    "                    ).format(self.edge_index.size(), self.edge_attr.size())\n",
    "                )\n",
    "\n",
    "        if self.x is not None and self.num_nodes is not None:\n",
    "            if self.x.size(0) != self.num_nodes:\n",
    "                raise RuntimeError(\n",
    "                    (\n",
    "                        \"Node features should hold {} elements in the first \"\n",
    "                        \"dimension but found {}\"\n",
    "                    ).format(self.num_nodes, self.x.size(0))\n",
    "                )\n",
    "\n",
    "        if self.pos is not None and self.num_nodes is not None:\n",
    "            if self.pos.size(0) != self.num_nodes:\n",
    "                raise RuntimeError(\n",
    "                    (\n",
    "                        \"Node positions should hold {} elements in the first \"\n",
    "                        \"dimension but found {}\"\n",
    "                    ).format(self.num_nodes, self.pos.size(0))\n",
    "                )\n",
    "\n",
    "        if self.normal is not None and self.num_nodes is not None:\n",
    "            if self.normal.size(0) != self.num_nodes:\n",
    "                raise RuntimeError(\n",
    "                    (\n",
    "                        \"Node normals should hold {} elements in the first \"\n",
    "                        \"dimension but found {}\"\n",
    "                    ).format(self.num_nodes, self.normal.size(0))\n",
    "                )\n",
    "\n",
    "    def __repr__(self):\n",
    "        cls = str(self.__class__.__name__)\n",
    "        has_dict = any([isinstance(item, dict) for _, item in self])\n",
    "\n",
    "        if not has_dict:\n",
    "            info = [size_repr(key, item) for key, item in self]\n",
    "            return \"{}({})\".format(cls, \", \".join(info))\n",
    "        else:\n",
    "            info = [size_repr(key, item, indent=2) for key, item in self]\n",
    "            return \"{}(\\n{}\\n)\".format(cls, \",\\n\".join(info))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20bfb1",
   "metadata": {},
   "source": [
    "## Atomic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb1ac5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"AtomicData: neighbor graphs in (periodic) real space.\n",
    "\n",
    "Authors: Albert Musaelian\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from typing import Union, Tuple, Dict, Optional, List, Set, Sequence, Final\n",
    "from collections.abc import Mapping\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import ase\n",
    "from ase.calculators.singlepoint import SinglePointCalculator, SinglePointDFTCalculator\n",
    "from ase.calculators.calculator import all_properties as ase_all_properties # maybe problem\n",
    "from ase.stress import voigt_6_to_full_3x3_stress, full_3x3_to_voigt_6_stress\n",
    "\n",
    "import torch\n",
    "import e3nn.o3\n",
    "from e3nn.io import CartesianTensor# may be problem\n",
    "#from . import AtomicDataDict\n",
    "#from nequip.data._util import _TORCH_INTEGER_DTYPES\n",
    "#from nequip.utils.torch_geometric import Data\n",
    "\n",
    "# A type representing ASE-style periodic boundary condtions, which can be partial (the tuple case)\n",
    "PBC = Union[bool, Tuple[bool, bool, bool]]\n",
    "\n",
    "# === Key Registration ===\n",
    "\n",
    "_DEFAULT_LONG_FIELDS: Set[str] = {\n",
    "    EDGE_INDEX_KEY,\n",
    "    ATOMIC_NUMBERS_KEY,\n",
    "    ATOM_TYPE_KEY,\n",
    "    BATCH_KEY,\n",
    "}\n",
    "_DEFAULT_NODE_FIELDS: Set[str] = {\n",
    "    POSITIONS_KEY,\n",
    "    NODE_FEATURES_KEY,\n",
    "    NODE_ATTRS_KEY,\n",
    "    ATOMIC_NUMBERS_KEY,\n",
    "    ATOM_TYPE_KEY,\n",
    "    FORCE_KEY,\n",
    "    PER_ATOM_ENERGY_KEY,\n",
    "    BATCH_KEY,\n",
    "}\n",
    "_DEFAULT_EDGE_FIELDS: Set[str] = {\n",
    "    EDGE_CELL_SHIFT_KEY,\n",
    "    EDGE_VECTORS_KEY,\n",
    "    EDGE_LENGTH_KEY,\n",
    "    EDGE_ATTRS_KEY,\n",
    "    EDGE_EMBEDDING_KEY,\n",
    "    EDGE_FEATURES_KEY,\n",
    "    EDGE_CUTOFF_KEY,\n",
    "    EDGE_ENERGY_KEY,\n",
    "}\n",
    "_DEFAULT_GRAPH_FIELDS: Set[str] = {\n",
    "    TOTAL_ENERGY_KEY,\n",
    "    STRESS_KEY,\n",
    "    VIRIAL_KEY,\n",
    "    PBC_KEY,\n",
    "    CELL_KEY,\n",
    "    BATCH_PTR_KEY,\n",
    "}\n",
    "_DEFAULT_CARTESIAN_TENSOR_FIELDS: Dict[str, str] = {\n",
    "    STRESS_KEY: \"ij=ji\",\n",
    "    VIRIAL_KEY: \"ij=ji\",\n",
    "}\n",
    "_NODE_FIELDS: Set[str] = set(_DEFAULT_NODE_FIELDS)\n",
    "_EDGE_FIELDS: Set[str] = set(_DEFAULT_EDGE_FIELDS)\n",
    "_GRAPH_FIELDS: Set[str] = set(_DEFAULT_GRAPH_FIELDS)\n",
    "_LONG_FIELDS: Set[str] = set(_DEFAULT_LONG_FIELDS)\n",
    "_CARTESIAN_TENSOR_FIELDS: Dict[str, str] = dict(_DEFAULT_CARTESIAN_TENSOR_FIELDS)\n",
    "\n",
    "\n",
    "def register_fields(\n",
    "    node_fields: Sequence[str] = [],\n",
    "    edge_fields: Sequence[str] = [],\n",
    "    graph_fields: Sequence[str] = [],\n",
    "    long_fields: Sequence[str] = [],\n",
    "    cartesian_tensor_fields: Dict[str, str] = {},\n",
    ") -> None:\n",
    "    r\"\"\"Register fields as being per-atom, per-edge, or per-frame.\n",
    "\n",
    "    Args:\n",
    "        node_permute_fields: fields that are equivariant to node permutations.\n",
    "        edge_permute_fields: fields that are equivariant to edge permutations.\n",
    "    \"\"\"\n",
    "    node_fields: set = set(node_fields)\n",
    "    edge_fields: set = set(edge_fields)\n",
    "    graph_fields: set = set(graph_fields)\n",
    "    long_fields: set = set(long_fields)\n",
    "\n",
    "    # error checking: prevents registering fields as contradictory types\n",
    "    # potentially unregistered fields\n",
    "    assert len(node_fields.intersection(edge_fields)) == 0\n",
    "    assert len(node_fields.intersection(graph_fields)) == 0\n",
    "    assert len(edge_fields.intersection(graph_fields)) == 0\n",
    "    # already registered fields\n",
    "    assert len(_NODE_FIELDS.intersection(edge_fields)) == 0\n",
    "    assert len(_NODE_FIELDS.intersection(graph_fields)) == 0\n",
    "    assert len(_EDGE_FIELDS.intersection(node_fields)) == 0\n",
    "    assert len(_EDGE_FIELDS.intersection(graph_fields)) == 0\n",
    "    assert len(_GRAPH_FIELDS.intersection(edge_fields)) == 0\n",
    "    assert len(_GRAPH_FIELDS.intersection(node_fields)) == 0\n",
    "\n",
    "    # check that Cartesian tensor fields to add are rank-2 (higher ranks not supported)\n",
    "    for cart_tensor_key in cartesian_tensor_fields:\n",
    "        cart_tensor_rank = len(\n",
    "            CartesianTensor(cartesian_tensor_fields[cart_tensor_key]).indices\n",
    "        )\n",
    "        if cart_tensor_rank != 2:\n",
    "            raise NotImplementedError(\n",
    "                f\"Only rank-2 tensor data processing supported, but got {cart_tensor_key} is rank {cart_tensor_rank}. Consider raising a GitHub issue if higher-rank tensor data processing is desired.\"\n",
    "            )\n",
    "\n",
    "    # update fields\n",
    "    _NODE_FIELDS.update(node_fields)\n",
    "    _EDGE_FIELDS.update(edge_fields)\n",
    "    _GRAPH_FIELDS.update(graph_fields)\n",
    "    _LONG_FIELDS.update(long_fields)\n",
    "    _CARTESIAN_TENSOR_FIELDS.update(cartesian_tensor_fields)\n",
    "\n",
    "\n",
    "def deregister_fields(*fields: Sequence[str]) -> None:\n",
    "    r\"\"\"Deregister a field registered with ``register_fields``.\n",
    "\n",
    "    Silently ignores fields that were never registered to begin with.\n",
    "\n",
    "    Args:\n",
    "        *fields: fields to deregister.\n",
    "    \"\"\"\n",
    "    for f in fields:\n",
    "        assert f not in _DEFAULT_NODE_FIELDS, \"Cannot deregister built-in field\"\n",
    "        assert f not in _DEFAULT_EDGE_FIELDS, \"Cannot deregister built-in field\"\n",
    "        assert f not in _DEFAULT_GRAPH_FIELDS, \"Cannot deregister built-in field\"\n",
    "        assert f not in _DEFAULT_LONG_FIELDS, \"Cannot deregister built-in field\"\n",
    "        assert (\n",
    "            f not in _DEFAULT_CARTESIAN_TENSOR_FIELDS\n",
    "        ), \"Cannot deregister built-in field\"\n",
    "\n",
    "        _NODE_FIELDS.discard(f)\n",
    "        _EDGE_FIELDS.discard(f)\n",
    "        _GRAPH_FIELDS.discard(f)\n",
    "        _LONG_FIELDS.discard(f)\n",
    "        _CARTESIAN_TENSOR_FIELDS.pop(f, None)\n",
    "\n",
    "\n",
    "def _register_field_prefix(prefix: str) -> None:\n",
    "    \"\"\"Re-register all registered fields as the same type, but with `prefix` added on.\"\"\"\n",
    "    assert prefix.endswith(\"_\")\n",
    "    register_fields(\n",
    "        node_fields=[prefix + e for e in _NODE_FIELDS],\n",
    "        edge_fields=[prefix + e for e in _EDGE_FIELDS],\n",
    "        graph_fields=[prefix + e for e in _GRAPH_FIELDS],\n",
    "        long_fields=[prefix + e for e in _LONG_FIELDS],\n",
    "    )\n",
    "\n",
    "\n",
    "#  === AtomicData ===\n",
    "\n",
    "\n",
    "def _process_dict(kwargs, ignore_fields=[]):\n",
    "    \"\"\"Convert a dict of data into correct dtypes/shapes according to key\"\"\"\n",
    "    # Deal with _some_ dtype issues\n",
    "    for k, v in kwargs.items():\n",
    "        if k in ignore_fields:\n",
    "            continue\n",
    "\n",
    "        if k in _LONG_FIELDS:\n",
    "            # Any property used as an index must be long (or byte or bool, but those are not relevant for atomic scale systems)\n",
    "            # int32 would pass later checks, but is actually disallowed by torch\n",
    "            kwargs[k] = torch.as_tensor(v, dtype=torch.long)\n",
    "        elif isinstance(v, bool):\n",
    "            kwargs[k] = torch.as_tensor(v)\n",
    "        elif isinstance(v, np.ndarray):\n",
    "            if np.issubdtype(v.dtype, np.floating):\n",
    "                kwargs[k] = torch.as_tensor(v, dtype=torch.get_default_dtype())\n",
    "            else:\n",
    "                kwargs[k] = torch.as_tensor(v)\n",
    "        elif isinstance(v, list):\n",
    "            ele_dtype = np.array(v).dtype\n",
    "            if np.issubdtype(ele_dtype, np.floating):\n",
    "                kwargs[k] = torch.as_tensor(v, dtype=torch.get_default_dtype())\n",
    "            else:\n",
    "                kwargs[k] = torch.as_tensor(v)\n",
    "        elif np.issubdtype(type(v), np.floating):\n",
    "            # Force scalars to be tensors with a data dimension\n",
    "            # This makes them play well with irreps\n",
    "            kwargs[k] = torch.as_tensor(v, dtype=torch.get_default_dtype())\n",
    "        elif isinstance(v, torch.Tensor) and len(v.shape) == 0:\n",
    "            # ^ this tensor is a scalar; we need to give it\n",
    "            # a data dimension to play nice with irreps\n",
    "            kwargs[k] = v\n",
    "        elif isinstance(v, torch.Tensor):\n",
    "            # This is a tensor, so we just don't do anything except avoid the warning in the `else`\n",
    "            pass\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"Value for field {k} was of unsupported type {type(v)} (value was {v})\"\n",
    "            )\n",
    "\n",
    "    if BATCH_KEY in kwargs:\n",
    "        num_frames = kwargs[BATCH_KEY].max() + 1\n",
    "    else:\n",
    "        num_frames = 1\n",
    "\n",
    "    for k, v in kwargs.items():\n",
    "        if k in ignore_fields:\n",
    "            continue\n",
    "\n",
    "        if len(v.shape) == 0:\n",
    "            kwargs[k] = v.unsqueeze(-1)\n",
    "            v = kwargs[k]\n",
    "\n",
    "        if k in set.union(_NODE_FIELDS, _EDGE_FIELDS) and len(v.shape) == 1:\n",
    "            kwargs[k] = v.unsqueeze(-1)\n",
    "            v = kwargs[k]\n",
    "\n",
    "        if (\n",
    "            k in _NODE_FIELDS\n",
    "            and POSITIONS_KEY in kwargs\n",
    "            and v.shape[0] != kwargs[POSITIONS_KEY].shape[0]\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"{k} is a node field but has the wrong dimension {v.shape}\"\n",
    "            )\n",
    "        elif (\n",
    "            k in _EDGE_FIELDS\n",
    "            and EDGE_INDEX_KEY in kwargs\n",
    "            and v.shape[0] != kwargs[EDGE_INDEX_KEY].shape[1]\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"{k} is a edge field but has the wrong dimension {v.shape}\"\n",
    "            )\n",
    "        elif k in _GRAPH_FIELDS:\n",
    "            if num_frames > 1 and v.shape[0] != num_frames:\n",
    "                raise ValueError(f\"Wrong shape for graph property {k}\")\n",
    "\n",
    "\n",
    "class AtomicData(Data):\n",
    "    \"\"\"A neighbor graph for points in (periodic triclinic) real space.\n",
    "\n",
    "    For typical cases either ``from_points`` or ``from_ase`` should be used to\n",
    "    construct a AtomicData; they also standardize and check their input much more\n",
    "    thoroughly.\n",
    "\n",
    "    In general, ``node_features`` are features or input information on the nodes \n",
    "    that will be fed through and transformed by the network, while ``node_attrs`` are _encodings_ fixed, \n",
    "    inherant attributes of the atoms themselves that remain constant through the network.\n",
    "    For example, a one-hot _encoding_ of atomic species is a node attribute, \n",
    "    while some observed instantaneous property of that atom (current partial charge, for example), \n",
    "    would be a feature.\n",
    "\n",
    "    In general, ``torch.Tensor`` arguments should be of consistant dtype. \n",
    "    Numpy arrays will be converted to ``torch.Tensor``s; \n",
    "    those of floating point dtype will be converted to ``torch.get_current_dtype()`` \n",
    "    regardless of their original precision. \n",
    "    Scalar values (Python scalars or ``torch.Tensor``s of shape ``()``) a resized to tensors of shape ``[1]``. \n",
    "    Per-atom scalar values should be given with shape ``[N_at, 1]``.\n",
    "\n",
    "    ``AtomicData`` should be used for all data creation and manipulation outside of the model;\n",
    "    inside of the model ``Type`` is used.\n",
    "\n",
    "    Args:\n",
    "        pos (Tensor [n_nodes, 3]): Positions of the nodes.\n",
    "        edge_index (LongTensor [2, n_edges]): ``edge_index[0]`` is the per-edge\n",
    "            index of the source node and ``edge_index[1]`` is the target node.\n",
    "        edge_cell_shift (Tensor [n_edges, 3], optional): which periodic image\n",
    "            of the target point each edge goes to, relative to the source point.\n",
    "        cell (Tensor [1, 3, 3], optional): the periodic cell for\n",
    "            ``edge_cell_shift`` as the three triclinic cell vectors.\n",
    "        node_features (Tensor [n_atom, ...]): the input features of the nodes, optional\n",
    "        node_attrs (Tensor [n_atom, ...]): the attributes of the nodes, for instance the atom type, optional\n",
    "        batch (Tensor [n_atom]): the graph to which the node belongs, optional\n",
    "        atomic_numbers (Tensor [n_atom]): optional.\n",
    "        atom_type (Tensor [n_atom]): optional.\n",
    "        **kwargs: other data, optional.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, irreps: Dict[str, e3nn.o3.Irreps] = {}, _validate: bool = True, **kwargs\n",
    "    ):\n",
    "        # empty init needed by get_example\n",
    "        if len(kwargs) == 0 and len(irreps) == 0:\n",
    "            super().__init__()\n",
    "            return\n",
    "\n",
    "        # Check the keys\n",
    "        if _validate:\n",
    "            validate_keys(kwargs)\n",
    "            _process_dict(kwargs)\n",
    "\n",
    "        super().__init__(num_nodes=len(kwargs[\"pos\"]), **kwargs)\n",
    "\n",
    "        if _validate:\n",
    "            # Validate shapes\n",
    "            assert self.pos.dim() == 2 and self.pos.shape[1] == 3\n",
    "            assert self.edge_index.dim() == 2 and self.edge_index.shape[0] == 2\n",
    "            if \"edge_cell_shift\" in self and self.edge_cell_shift is not None:\n",
    "                assert self.edge_cell_shift.shape == (self.num_edges, 3)\n",
    "                assert self.edge_cell_shift.dtype == self.pos.dtype\n",
    "            if \"cell\" in self and self.cell is not None:\n",
    "                assert (self.cell.shape == (3, 3)) or (\n",
    "                    self.cell.dim() == 3 and self.cell.shape[1:] == (3, 3)\n",
    "                )\n",
    "                assert self.cell.dtype == self.pos.dtype\n",
    "            if \"node_features\" in self and self.node_features is not None:\n",
    "                assert self.node_features.shape[0] == self.num_nodes\n",
    "                assert self.node_features.dtype == self.pos.dtype\n",
    "            if \"node_attrs\" in self and self.node_attrs is not None:\n",
    "                assert self.node_attrs.shape[0] == self.num_nodes\n",
    "                assert self.node_attrs.dtype == self.pos.dtype\n",
    "\n",
    "            if (\n",
    "                ATOMIC_NUMBERS_KEY in self\n",
    "                and self.atomic_numbers is not None\n",
    "            ):\n",
    "                assert self.atomic_numbers.dtype in _TORCH_INTEGER_DTYPES\n",
    "            if \"batch\" in self and self.batch is not None:\n",
    "                assert self.batch.dim() == 2 and self.batch.shape[0] == self.num_nodes\n",
    "                # Check that there are the right number of cells\n",
    "                if \"cell\" in self and self.cell is not None:\n",
    "                    cell = self.cell.view(-1, 3, 3)\n",
    "                    assert cell.shape[0] == self.batch.max() + 1\n",
    "\n",
    "            # Validate irreps\n",
    "            # __*__ is the only way to hide from torch_geometric\n",
    "            self.__irreps__ = _fix_irreps_dict(irreps)\n",
    "            for field, irreps in self.__irreps__:\n",
    "                if irreps is not None:\n",
    "                    assert self[field].shape[-1] == irreps.dim\n",
    "\n",
    "    @classmethod\n",
    "    def from_points(\n",
    "        cls,  # points to the class, so can be used as consturctor as cls(...)\n",
    "        pos=None,\n",
    "        r_max: float = None,\n",
    "        self_interaction: bool = False,\n",
    "        strict_self_interaction: bool = True,\n",
    "        cell=None,\n",
    "        pbc: Optional[PBC] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Build neighbor graph from points, optionally with PBC.\n",
    "\n",
    "        Args:\n",
    "            pos (np.ndarray/torch.Tensor shape [N, 3]): node positions. If Tensor, must be on the CPU.\n",
    "            r_max (float): neighbor cutoff radius.\n",
    "            cell (ase.Cell/ndarray [3,3], optional): periodic cell for the points. Defaults to ``None``.\n",
    "            pbc (bool or 3-tuple of bool, optional): whether to apply periodic boundary conditions to all\n",
    "            or each of the three cell vector directions. Defaults to ``False``.\n",
    "            self_interaction (bool, optional): whether to include self edges for points. Defaults to ``False``. Note\n",
    "            that edges between the same atom in different periodic images are still included. (See\n",
    "            ``strict_self_interaction`` to control this behaviour.)\n",
    "            strict_self_interaction (bool): Whether to include *any* self interaction edges in the graph, even\n",
    "            if the two instances of the atom are in different periodic images. \n",
    "            Defaults to True, should be True for most applications.\n",
    "            **kwargs (optional): other fields to add. Keys listed in ``*_KEY` will be treated specially.\n",
    "        \"\"\"\n",
    "        if pos is None or r_max is None:\n",
    "            raise ValueError(\"pos and r_max must be given.\")\n",
    "\n",
    "        if pbc is None:\n",
    "            if cell is not None:\n",
    "                raise ValueError(\n",
    "                    \"A cell was provided, but pbc weren't. Please explicitly probide PBC.\"\n",
    "                )\n",
    "            # there are no PBC if cell and pbc are not provided\n",
    "            pbc = False\n",
    "\n",
    "        if isinstance(pbc, bool):\n",
    "            pbc = (pbc,) * 3\n",
    "        else:\n",
    "            assert len(pbc) == 3\n",
    "\n",
    "        pos = torch.as_tensor(pos, dtype=torch.get_default_dtype())\n",
    "\n",
    "        edge_index, edge_cell_shift, cell = neighbor_list_and_relative_vec(\n",
    "            pos=pos,\n",
    "            r_max=r_max,\n",
    "            self_interaction=self_interaction,\n",
    "            strict_self_interaction=strict_self_interaction,\n",
    "            cell=cell,\n",
    "            pbc=pbc,\n",
    "        )\n",
    "\n",
    "        # Make torch tensors for data:\n",
    "        if cell is not None:\n",
    "            kwargs[CELL_KEY] = cell.view(3, 3)\n",
    "            kwargs[EDGE_CELL_SHIFT_KEY] = edge_cell_shift\n",
    "        if pbc is not None:\n",
    "            kwargs[PBC_KEY] = torch.as_tensor(\n",
    "                pbc, dtype=torch.bool\n",
    "            ).view(3)\n",
    "\n",
    "        return cls(edge_index=edge_index, pos=torch.as_tensor(pos), **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_ase(\n",
    "        cls,\n",
    "        atoms,\n",
    "        r_max,\n",
    "        key_mapping: Optional[Dict[str, str]] = {},\n",
    "        include_keys: Optional[list] = [],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Build a ``AtomicData`` from an ``ase.Atoms`` object.\n",
    "\n",
    "        Respects ``atoms``'s ``pbc`` and ``cell``.\n",
    "\n",
    "        First tries to extract energies and forces from a single-point calculator associated with the ``Atoms`` \n",
    "        if one is present and has those fields.\n",
    "        If either is not found, the method will look for ``energy``/``energies`` \n",
    "        and ``force``/``forces`` in ``atoms.arrays``.\n",
    "\n",
    "        `get_atomic_numbers()` will be stored as the atomic_numbers attribute.\n",
    "\n",
    "        Args:\n",
    "            atoms (ase.Atoms): the input.\n",
    "            r_max (float): neighbor cutoff radius.\n",
    "            features (torch.Tensor shape [N, M], optional): per-atom M-dimensional feature vectors. If ``None`` (the\n",
    "             default), uses a one-hot encoding of the species present in ``atoms``.\n",
    "            include_keys (list): list of additional keys to include in AtomicData aside from the ones defined in\n",
    "                 ase.calculators.calculator.all_properties. Optional\n",
    "            key_mapping (dict): rename ase property name to a new string name. Optional\n",
    "            **kwargs (optional): other arguments for the ``AtomicData`` constructor.\n",
    "\n",
    "        Returns:\n",
    "            A ``AtomicData``.\n",
    "        \"\"\"\n",
    "        from nequip.ase import NequIPCalculator\n",
    "\n",
    "        assert \"pos\" not in kwargs\n",
    "\n",
    "        default_args = set(\n",
    "            [\n",
    "                \"numbers\",\n",
    "                \"positions\",\n",
    "            ]  # ase internal names for position and atomic_numbers\n",
    "            + [\"pbc\", \"cell\", \"pos\", \"r_max\"]  # arguments for from_points method\n",
    "            + list(kwargs.keys())\n",
    "        )\n",
    "        # the keys that are duplicated in kwargs are removed from the include_keys\n",
    "        include_keys = list(\n",
    "            set(include_keys + ase_all_properties + list(key_mapping.keys()))\n",
    "            - default_args\n",
    "        )\n",
    "\n",
    "        km = {\n",
    "            \"forces\": FORCE_KEY,\n",
    "            \"energy\": TOTAL_ENERGY_KEY,\n",
    "        }\n",
    "        km.update(key_mapping)\n",
    "        key_mapping = km\n",
    "\n",
    "        add_fields = {}\n",
    "\n",
    "        # Get info from atoms.arrays; lowest priority. copy first\n",
    "        add_fields = {\n",
    "            key_mapping.get(k, k): v\n",
    "            for k, v in atoms.arrays.items()\n",
    "            if k in include_keys\n",
    "        }\n",
    "\n",
    "        # Get info from atoms.info; second lowest priority.\n",
    "        add_fields.update(\n",
    "            {\n",
    "                key_mapping.get(k, k): v\n",
    "                for k, v in atoms.info.items()\n",
    "                if k in include_keys\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if atoms.calc is not None:\n",
    "            if isinstance(\n",
    "                atoms.calc, (SinglePointCalculator, SinglePointDFTCalculator)\n",
    "            ):\n",
    "                add_fields.update(\n",
    "                    {\n",
    "                        key_mapping.get(k, k): deepcopy(v)\n",
    "                        for k, v in atoms.calc.results.items()\n",
    "                        if k in include_keys\n",
    "                    }\n",
    "                )\n",
    "            elif isinstance(atoms.calc, NequIPCalculator):\n",
    "                pass  # otherwise the calculator breaks\n",
    "            else:\n",
    "                raise NotImplementedError(\n",
    "                    f\"`from_ase` does not support calculator {atoms.calc}\"\n",
    "                )\n",
    "\n",
    "        add_fields[ATOMIC_NUMBERS_KEY] = atoms.get_atomic_numbers()\n",
    "\n",
    "        # cell and pbc in kwargs can override the ones stored in atoms\n",
    "        cell = kwargs.pop(\"cell\", atoms.get_cell())\n",
    "        pbc = kwargs.pop(\"pbc\", atoms.pbc)\n",
    "\n",
    "        # IMPORTANT: the following reshape logic only applies to rank-2 Cartesian tensor fields\n",
    "        for key in add_fields:\n",
    "            if key in _CARTESIAN_TENSOR_FIELDS:\n",
    "                # enforce (3, 3) shape for graph fields, e.g. stress, virial\n",
    "                if key in _GRAPH_FIELDS:\n",
    "                    # handle ASE-style 6 element Voigt order stress\n",
    "                    if key in (STRESS_KEY, VIRIAL_KEY):\n",
    "                        if add_fields[key].shape == (6,):\n",
    "                            add_fields[key] = voigt_6_to_full_3x3_stress(\n",
    "                                add_fields[key]\n",
    "                            )\n",
    "                    if add_fields[key].shape == (3, 3):\n",
    "                        # it's already 3x3, do nothing else\n",
    "                        pass\n",
    "                    elif add_fields[key].shape == (9,):\n",
    "                        add_fields[key] = add_fields[key].reshape((3, 3))\n",
    "                    else:\n",
    "                        raise RuntimeError(\n",
    "                            f\"bad shape for {key} registered as a Cartesian tensor graph field---please note that only rank-2 Cartesian tensors are currently supported\"\n",
    "                        )\n",
    "                # enforce (N_atom, 3, 3) shape for node fields, e.g. Born effective charges\n",
    "                elif key in _NODE_FIELDS:\n",
    "                    if add_fields[key].shape[1:] == (3, 3):\n",
    "                        pass\n",
    "                    elif add_fields[key].shape[1:] == (9,):\n",
    "                        add_fields[key] = add_fields[key].reshape((-1, 3, 3))\n",
    "                    else:\n",
    "                        raise RuntimeError(\n",
    "                            f\"bad shape for {key} registered as a Cartesian tensor node field---please note that only rank-2 Cartesian tensors are currently supported\"\n",
    "                        )\n",
    "                else:\n",
    "                    raise RuntimeError(\n",
    "                        f\"{key} registered as a Cartesian tensor field was not registered as either a graph or node field\"\n",
    "                    )\n",
    "\n",
    "        return cls.from_points(\n",
    "            pos=atoms.positions,\n",
    "            r_max=r_max,\n",
    "            cell=cell,\n",
    "            pbc=pbc,\n",
    "            **kwargs,\n",
    "            **add_fields,\n",
    "        )\n",
    "\n",
    "    def to_ase(\n",
    "        self,\n",
    "        type_mapper=None,\n",
    "        extra_fields: List[str] = [],\n",
    "    ) -> Union[List[ase.Atoms], ase.Atoms]:\n",
    "        \"\"\"Build a (list of) ``ase.Atoms`` object(s) from an ``AtomicData`` object.\n",
    "\n",
    "        For each unique batch number provided in ``BATCH_KEY``,\n",
    "        an ``ase.Atoms`` object is created. If ``BATCH_KEY`` does not\n",
    "        exist in self, a single ``ase.Atoms`` object is created.\n",
    "\n",
    "        Args:\n",
    "            type_mapper: if provided, will be used to map ``ATOM_TYPES`` back into\n",
    "                elements, if the configuration of the ``type_mapper`` allows.\n",
    "            extra_fields: fields other than those handled explicitly (currently\n",
    "                those defining the structure as well as energy, per-atom energy,\n",
    "                and forces) to include in the output object. Per-atom (per-node)\n",
    "                quantities will be included in ``arrays``; per-graph and per-edge\n",
    "                quantities will be included in ``info``.\n",
    "\n",
    "        Returns:\n",
    "            A list of ``ase.Atoms`` objects if ``BATCH_KEY`` is in self\n",
    "            and is not None. Otherwise, a single ``ase.Atoms`` object is returned.\n",
    "        \"\"\"\n",
    "        positions = self.pos\n",
    "        edge_index = self[EDGE_INDEX_KEY]\n",
    "        if positions.device != torch.device(\"cpu\"):\n",
    "            raise TypeError(\n",
    "                \"Explicitly move this `AtomicData` to CPU using `.to()` before calling `to_ase()`.\"\n",
    "            )\n",
    "        if ATOMIC_NUMBERS_KEY in self:\n",
    "            atomic_nums = self.atomic_numbers\n",
    "        elif type_mapper is not None and type_mapper.has_chemical_symbols:\n",
    "            atomic_nums = type_mapper.untransform(self[ATOM_TYPE_KEY])\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                \"AtomicData.to_ase(): self didn't contain atomic numbers... using atom_type as atomic numbers instead, but this means the chemical symbols in ASE (outputs) will be wrong\"\n",
    "            )\n",
    "            atomic_nums = self[ATOM_TYPE_KEY]\n",
    "        pbc = getattr(self, PBC_KEY, None)\n",
    "        cell = getattr(self, CELL_KEY, None)\n",
    "        batch = getattr(self, BATCH_KEY, None)\n",
    "        energy = getattr(self, TOTAL_ENERGY_KEY, None)\n",
    "        energies = getattr(self, PER_ATOM_ENERGY_KEY, None)\n",
    "        force = getattr(self, FORCE_KEY, None)\n",
    "        do_calc = any(\n",
    "            k in self\n",
    "            for k in [\n",
    "                TOTAL_ENERGY_KEY,\n",
    "                FORCE_KEY,\n",
    "                PER_ATOM_ENERGY_KEY,\n",
    "                STRESS_KEY,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # exclude those that are special for ASE and that we process seperately\n",
    "        special_handling_keys = [\n",
    "            POSITIONS_KEY,\n",
    "            CELL_KEY,\n",
    "            PBC_KEY,\n",
    "            ATOMIC_NUMBERS_KEY,\n",
    "            TOTAL_ENERGY_KEY,\n",
    "            FORCE_KEY,\n",
    "            PER_ATOM_ENERGY_KEY,\n",
    "            STRESS_KEY,\n",
    "        ]\n",
    "        assert (\n",
    "            len(set(extra_fields).intersection(special_handling_keys)) == 0\n",
    "        ), f\"Cannot specify keys handled in special ways ({special_handling_keys}) as `extra_fields` for atoms output--- they are output by default\"\n",
    "\n",
    "        if cell is not None:\n",
    "            cell = cell.view(-1, 3, 3)\n",
    "        if pbc is not None:\n",
    "            pbc = pbc.view(-1, 3)\n",
    "\n",
    "        if batch is not None:\n",
    "            n_batches = batch.max() + 1\n",
    "            cell = cell.expand(n_batches, 3, 3) if cell is not None else None\n",
    "            pbc = pbc.expand(n_batches, 3) if pbc is not None else None\n",
    "        else:\n",
    "            n_batches = 1\n",
    "\n",
    "        batch_atoms = []\n",
    "        for batch_idx in range(n_batches):\n",
    "            if batch is not None:\n",
    "                mask = batch == batch_idx\n",
    "                mask = mask.view(-1)\n",
    "                # if both ends of the edge are in the batch, the edge is in the batch\n",
    "                edge_mask = mask[edge_index[0]] & mask[edge_index[1]]\n",
    "            else:\n",
    "                mask = slice(None)\n",
    "                edge_mask = slice(None)\n",
    "\n",
    "            mol = ase.Atoms(\n",
    "                numbers=atomic_nums[mask].view(-1),  # must be flat for ASE\n",
    "                positions=positions[mask],\n",
    "                cell=cell[batch_idx] if cell is not None else None,\n",
    "                pbc=pbc[batch_idx] if pbc is not None else None,\n",
    "            )\n",
    "\n",
    "            if do_calc:\n",
    "                fields = {}\n",
    "                if energies is not None:\n",
    "                    fields[\"energies\"] = energies[mask].cpu().numpy()\n",
    "                if energy is not None:\n",
    "                    fields[\"energy\"] = energy[batch_idx].cpu().numpy()\n",
    "                if force is not None:\n",
    "                    fields[\"forces\"] = force[mask].cpu().numpy()\n",
    "                if STRESS_KEY in self:\n",
    "                    fields[\"stress\"] = full_3x3_to_voigt_6_stress(\n",
    "                        self[\"stress\"].view(-1, 3, 3)[batch_idx].cpu().numpy()\n",
    "                    )\n",
    "                mol.calc = SinglePointCalculator(mol, **fields)\n",
    "\n",
    "            # add other information\n",
    "            for key in extra_fields:\n",
    "                if key in _NODE_FIELDS:\n",
    "                    # mask it\n",
    "                    mol.arrays[key] = (\n",
    "                        self[key][mask].cpu().numpy().reshape(mask.sum(), -1)\n",
    "                    )\n",
    "                elif key in _EDGE_FIELDS:\n",
    "                    mol.info[key] = (\n",
    "                        self[key][edge_mask].cpu().numpy().reshape(edge_mask.sum(), -1)\n",
    "                    )\n",
    "                elif key == EDGE_INDEX_KEY:\n",
    "                    mol.info[key] = self[key][:, edge_mask].cpu().numpy()\n",
    "                elif key in _GRAPH_FIELDS:\n",
    "                    mol.info[key] = self[key][batch_idx].cpu().numpy().reshape(-1)\n",
    "                else:\n",
    "                    raise RuntimeError(\n",
    "                        f\"Extra field `{key}` isn't registered as node/edge/graph\"\n",
    "                    )\n",
    "\n",
    "            batch_atoms.append(mol)\n",
    "\n",
    "        if batch is not None:\n",
    "            return batch_atoms\n",
    "        else:\n",
    "            assert len(batch_atoms) == 1\n",
    "            return batch_atoms[0]\n",
    "\n",
    "    def get_edge_vectors(data: Data) -> torch.Tensor:\n",
    "        data = with_edge_vectors(AtomicData.to_AtomicDataDict(data))\n",
    "        return data[EDGE_VECTORS_KEY]\n",
    "\n",
    "    @staticmethod\n",
    "    def to_AtomicDataDict(\n",
    "        data: Union[Data, Mapping], exclude_keys=tuple()\n",
    "    ) -> Type:\n",
    "        if isinstance(data, Data):\n",
    "            keys = data.keys\n",
    "        elif isinstance(data, Mapping):\n",
    "            keys = data.keys()\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid data `{repr(data)}`\")\n",
    "\n",
    "        return {\n",
    "            k: data[k]\n",
    "            for k in keys\n",
    "            if (\n",
    "                k not in exclude_keys\n",
    "                and data[k] is not None\n",
    "                and isinstance(data[k], torch.Tensor)\n",
    "            )\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_AtomicDataDict(cls, data: Type):\n",
    "        # it's an AtomicDataDict, so don't validate-- assume valid:\n",
    "        return cls(_validate=False, **data)\n",
    "\n",
    "    @property\n",
    "    def irreps(self):\n",
    "        return self.__irreps__\n",
    "\n",
    "    def __cat_dim__(self, key, value):\n",
    "        if key == EDGE_INDEX_KEY:\n",
    "            return 1  # always cat in the edge dimension\n",
    "        elif key in _GRAPH_FIELDS:\n",
    "            # graph-level properties and so need a new batch dimension\n",
    "            return None\n",
    "        else:\n",
    "            return 0  # cat along node/edge dimension\n",
    "\n",
    "    def without_nodes(self, which_nodes):\n",
    "        \"\"\"Return a copy of ``self`` with ``which_nodes`` removed.\n",
    "        The returned object may share references to some underlying data tensors with ``self``.\n",
    "        Args:\n",
    "            which_nodes (index tensor or boolean mask)\n",
    "        Returns:\n",
    "            A new data object.\n",
    "        \"\"\"\n",
    "        which_nodes = torch.as_tensor(which_nodes)\n",
    "        if which_nodes.dtype == torch.bool:\n",
    "            mask = ~which_nodes\n",
    "        else:\n",
    "            mask = torch.ones(self.num_nodes, dtype=torch.bool)\n",
    "            mask[which_nodes] = False\n",
    "        assert mask.shape == (self.num_nodes,)\n",
    "        n_keeping = mask.sum()\n",
    "\n",
    "        # Only keep edges where both from and to are kept\n",
    "        edge_mask = mask[self.edge_index[0]] & mask[self.edge_index[1]]\n",
    "        # Create an index mapping:\n",
    "        new_index = torch.full((self.num_nodes,), -1, dtype=torch.long)\n",
    "        new_index[mask] = torch.arange(n_keeping, dtype=torch.long)\n",
    "\n",
    "        new_dict = {}\n",
    "        for k in self.keys:\n",
    "            if k == EDGE_INDEX_KEY:\n",
    "                new_dict[EDGE_INDEX_KEY] = new_index[\n",
    "                    self.edge_index[:, edge_mask]\n",
    "                ]\n",
    "            elif k == EDGE_CELL_SHIFT_KEY:\n",
    "                new_dict[EDGE_CELL_SHIFT_KEY] = self.edge_cell_shift[\n",
    "                    edge_mask\n",
    "                ]\n",
    "            elif k == CELL_KEY:\n",
    "                new_dict[k] = self[k]\n",
    "            else:\n",
    "                if isinstance(self[k], torch.Tensor) and len(self[k]) == self.num_nodes:\n",
    "                    new_dict[k] = self[k][mask]\n",
    "                else:\n",
    "                    new_dict[k] = self[k]\n",
    "\n",
    "        new_dict[\"irreps\"] = self.__irreps__\n",
    "\n",
    "        return type(self)(**new_dict)\n",
    "\n",
    "\n",
    "_ERROR_ON_NO_EDGES: bool = os.environ.get(\"NEQUIP_ERROR_ON_NO_EDGES\", \"true\").lower()\n",
    "assert _ERROR_ON_NO_EDGES in (\"true\", \"false\")\n",
    "_ERROR_ON_NO_EDGES = _ERROR_ON_NO_EDGES == \"true\"\n",
    "\n",
    "# use \"ase\" as default\n",
    "# TODO: eventually, choose fastest as default\n",
    "# NOTE:\n",
    "# - vesin and matscipy do not support self-interaction\n",
    "# - vesin does not allow for mixed pbcs\n",
    "_NEQUIP_NL: Final[str] = os.environ.get(\"NEQUIP_NL\", \"ase\").lower()\n",
    "\n",
    "if _NEQUIP_NL == \"vesin\":\n",
    "    from vesin import NeighborList as vesin_nl\n",
    "elif _NEQUIP_NL == \"matscipy\":\n",
    "    import matscipy.neighbours\n",
    "elif _NEQUIP_NL == \"ase\":\n",
    "    import ase.neighborlist\n",
    "else:\n",
    "    raise NotImplementedError(f\"Unknown neighborlist NEQUIP_NL = {_NEQUIP_NL}\")\n",
    "\n",
    "\n",
    "def neighbor_list_and_relative_vec(\n",
    "    pos,\n",
    "    r_max,\n",
    "    self_interaction=False,\n",
    "    strict_self_interaction=True,\n",
    "    cell=None,\n",
    "    pbc=False,\n",
    "):\n",
    "    \"\"\"Create neighbor list and neighbor vectors based on radial cutoff.\n",
    "\n",
    "    Create neighbor list (``edge_index``) and relative vectors\n",
    "    (``edge_attr``) based on radial cutoff.\n",
    "\n",
    "    Edges are given by the following convention:\n",
    "    - ``edge_index[0]`` is the *source* (convolution center).\n",
    "    - ``edge_index[1]`` is the *target* (neighbor).\n",
    "\n",
    "    Thus, ``edge_index`` has the same convention as the relative vectors:\n",
    "    :math:`\\\\vec{r}_{source, target}`\n",
    "\n",
    "    If the input positions are a tensor with ``requires_grad == True``,\n",
    "    the output displacement vectors will be correctly attached to the inputs\n",
    "    for autograd.\n",
    "\n",
    "    All outputs are Tensors on the same device as ``pos``; this allows future\n",
    "    optimization of the neighbor list on the GPU.\n",
    "\n",
    "    Args:\n",
    "        pos (shape [N, 3]): Positional coordinate; Tensor or numpy array. If Tensor, must be on CPU.\n",
    "        r_max (float): Radial cutoff distance for neighbor finding.\n",
    "        cell (numpy shape [3, 3]): Cell for periodic boundary conditions. Ignored if ``pbc == False``.\n",
    "        pbc (bool or 3-tuple of bool): Whether the system is periodic in each of the three cell dimensions.\n",
    "        self_interaction (bool): Whether or not to include same periodic image self-edges in the neighbor list.\n",
    "        strict_self_interaction (bool): Whether to include *any* self interaction edges in the graph, even if the two\n",
    "            instances of the atom are in different periodic images. Defaults to True, should be True for most applications.\n",
    "\n",
    "    Returns:\n",
    "        edge_index (torch.tensor shape [2, num_edges]): List of edges.\n",
    "        edge_cell_shift (torch.tensor shape [num_edges, 3]): Relative cell shift\n",
    "            vectors. Returned only if cell is not None.\n",
    "        cell (torch.Tensor [3, 3]): the cell as a tensor on the correct device.\n",
    "            Returned only if cell is not None.\n",
    "    \"\"\"\n",
    "    if isinstance(pbc, bool):\n",
    "        pbc = (pbc,) * 3\n",
    "\n",
    "    # Either the position or the cell may be on the GPU as tensors\n",
    "    if isinstance(pos, torch.Tensor):\n",
    "        temp_pos = pos.detach().cpu().numpy()\n",
    "        out_device = pos.device\n",
    "        out_dtype = pos.dtype\n",
    "    else:\n",
    "        temp_pos = np.asarray(pos)\n",
    "        out_device = torch.device(\"cpu\")\n",
    "        out_dtype = torch.get_default_dtype()\n",
    "\n",
    "    # Right now, GPU tensors require a round trip\n",
    "    if out_device.type != \"cpu\":\n",
    "        warnings.warn(\n",
    "            \"Currently, neighborlists require a round trip to the CPU. Please pass CPU tensors if possible.\"\n",
    "        )\n",
    "\n",
    "    # Get a cell on the CPU no matter what\n",
    "    if isinstance(cell, torch.Tensor):\n",
    "        temp_cell = cell.detach().cpu().numpy()\n",
    "        cell_tensor = cell.to(device=out_device, dtype=out_dtype)\n",
    "    elif cell is not None:\n",
    "        temp_cell = np.asarray(cell)\n",
    "        cell_tensor = torch.as_tensor(temp_cell, device=out_device, dtype=out_dtype)\n",
    "    else:\n",
    "        # ASE will \"complete\" this correctly.\n",
    "        temp_cell = np.zeros((3, 3), dtype=temp_pos.dtype)\n",
    "        cell_tensor = torch.as_tensor(temp_cell, device=out_device, dtype=out_dtype)\n",
    "\n",
    "    # ASE dependent part\n",
    "    temp_cell = ase.geometry.complete_cell(temp_cell)\n",
    "\n",
    "    if _NEQUIP_NL == \"vesin\":\n",
    "        assert strict_self_interaction and not self_interaction\n",
    "        # use same mixed pbc logic as\n",
    "        # https://github.com/Luthaf/vesin/blob/main/python/vesin/src/vesin/_ase.py\n",
    "        if pbc[0] and pbc[1] and pbc[2]:\n",
    "            periodic = True\n",
    "        elif not pbc[0] and not pbc[1] and not pbc[2]:\n",
    "            periodic = False\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"different periodic boundary conditions on different axes are not supported by vesin neighborlist, use ASE or matscipy\"\n",
    "            )\n",
    "\n",
    "        first_idex, second_idex, shifts = vesin_nl(\n",
    "            cutoff=float(r_max), full_list=True\n",
    "        ).compute(points=temp_pos, box=temp_cell, periodic=periodic, quantities=\"ijS\")\n",
    "\n",
    "    elif _NEQUIP_NL == \"matscipy\":\n",
    "        assert strict_self_interaction and not self_interaction\n",
    "        first_idex, second_idex, shifts = matscipy.neighbours.neighbour_list(\n",
    "            \"ijS\",\n",
    "            pbc=pbc,\n",
    "            cell=temp_cell,\n",
    "            positions=temp_pos,\n",
    "            cutoff=float(r_max),\n",
    "        )\n",
    "    elif _NEQUIP_NL == \"ase\":\n",
    "        first_idex, second_idex, shifts = ase.neighborlist.primitive_neighbor_list(\n",
    "            \"ijS\",\n",
    "            pbc,\n",
    "            temp_cell,\n",
    "            temp_pos,\n",
    "            cutoff=float(r_max),\n",
    "            self_interaction=strict_self_interaction,  # we want edges from atom to itself in different periodic images!\n",
    "            use_scaled_positions=False,\n",
    "        )\n",
    "\n",
    "    # Eliminate true self-edges that don't cross periodic boundaries\n",
    "    if not self_interaction:\n",
    "        bad_edge = first_idex == second_idex\n",
    "        bad_edge &= np.all(shifts == 0, axis=1)\n",
    "        keep_edge = ~bad_edge\n",
    "        if _ERROR_ON_NO_EDGES and (not np.any(keep_edge)):\n",
    "            raise ValueError(\n",
    "                f\"Every single atom has no neighbors within the cutoff r_max={r_max} (after eliminating self edges, no edges remain in this system)\"\n",
    "            )\n",
    "        first_idex = first_idex[keep_edge]\n",
    "        second_idex = second_idex[keep_edge]\n",
    "        shifts = shifts[keep_edge]\n",
    "\n",
    "    # Build output:\n",
    "    edge_index = torch.vstack(\n",
    "        (torch.LongTensor(first_idex), torch.LongTensor(second_idex))\n",
    "    ).to(device=out_device)\n",
    "\n",
    "    shifts = torch.as_tensor(\n",
    "        shifts,\n",
    "        dtype=out_dtype,\n",
    "        device=out_device,\n",
    "    )\n",
    "    return edge_index, shifts, cell_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bc29f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.AtomicData'>\n",
      "Total energy: tensor([-189.3690])\n",
      "Force on atom 0: tensor([0.0591, 0.8510, 0.0488])\n"
     ]
    }
   ],
   "source": [
    "# Load some input structure from an XYZ or other ASE readable file:\n",
    "data = AtomicData.from_ase(ase.io.read(\"./datasets/NaBr-data.xyz\"), r_max=5.)\n",
    "#data = data.to(device)\n",
    "\n",
    "#out = model(AtomicData.to_AtomicDataDict(data))\n",
    "\n",
    "print(type(data))\n",
    "print(f\"Total energy: {data[AtomicDataDict.TOTAL_ENERGY_KEY]}\")\n",
    "print(f\"Force on atom 0: {data[AtomicDataDict.FORCE_KEY][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e0431b",
   "metadata": {},
   "source": [
    "### Going to implement the collections of atomic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c473e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference example\n",
    "from nequip.data import dataset_from_config\n",
    "from nequip.utils import Config\n",
    "from nequip.utils.misc import get_default_device_name\n",
    "from nequip.utils.config import _GLOBAL_ALL_ASKED_FOR_KEYS\n",
    "\n",
    "default_config = dict(\n",
    "    root=\"./\",\n",
    "    tensorboard=False,\n",
    "    wandb=False,\n",
    "    model_builders=[\n",
    "        \"SimpleIrrepsConfig\",\n",
    "        \"EnergyModel\",\n",
    "        \"PerSpeciesRescale\",\n",
    "        \"StressForceOutput\",\n",
    "        \"RescaleEnergyEtc\",\n",
    "    ],\n",
    "    dataset_statistics_stride=1,\n",
    "    device=get_default_device_name(),\n",
    "    default_dtype=\"float64\",\n",
    "    model_dtype=\"float32\",\n",
    "    allow_tf32=True,\n",
    "    verbose=\"INFO\",\n",
    "    model_debug_mode=False,\n",
    "    equivariance_test=False,\n",
    "    grad_anomaly_mode=False,\n",
    "    gpu_oom_offload=False,\n",
    "    append=False,\n",
    "    warn_unused=False,\n",
    "    _jit_bailout_depth=2,  # avoid 20 iters of pain, see https://github.com/pytorch/pytorch/issues/52286\n",
    "    # Quote from eelison in PyTorch slack:\n",
    "    # https://pytorch.slack.com/archives/CDZD1FANA/p1644259272007529?thread_ts=1644064449.039479&cid=CDZD1FANA\n",
    "    # > Right now the default behavior is to specialize twice on static shapes and then on dynamic shapes.\n",
    "    # > To reduce warmup time you can do something like setFusionStrartegy({{FusionBehavior::DYNAMIC, 3}})\n",
    "    # > ... Although we would wouldn't really expect to recompile a dynamic shape fusion in a model,\n",
    "    # > provided broadcasting patterns remain fixed\n",
    "    # We default to DYNAMIC alone because the number of edges is always dynamic,\n",
    "    # even if the number of atoms is fixed:\n",
    "    _jit_fusion_strategy=[(\"DYNAMIC\", 3)],\n",
    "    # Due to what appear to be ongoing bugs with nvFuser, we default to NNC (fuser1) for now:\n",
    "    # TODO: still default to NNC on CPU regardless even if change this for GPU\n",
    "    # TODO: default for ROCm?\n",
    "    _jit_fuser=\"fuser1\",\n",
    ")\n",
    "\n",
    "# All default_config keys are valid / requested\n",
    "_GLOBAL_ALL_ASKED_FOR_KEYS.update(default_config.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ccf7d286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_jit_bailout_depth': 2, '_jit_fusion_strategy': [('DYNAMIC', 3)], '_jit_fuser': 'fuser1', 'root': 'results/NaBr-tutorial', 'tensorboard': False, 'wandb': True, 'model_builders': ['allegro.model.Allegro', 'PerSpeciesRescale', 'ForceOutput', 'RescaleEnergyEtc'], 'dataset_statistics_stride': 1, 'device': 'cpu', 'default_dtype': 'float32', 'model_dtype': 'float32', 'allow_tf32': True, 'verbose': 'info', 'model_debug_mode': False, 'equivariance_test': False, 'grad_anomaly_mode': False, 'gpu_oom_offload': False, 'append': True, 'warn_unused': False, 'run_name': 'NaBr', 'seed': 123456, 'dataset_seed': 123456, 'r_max': 5.0, 'avg_num_neighbors': 'auto', 'BesselBasis_trainable': True, 'PolynomialCutoff_p': 6, 'l_max': 1, 'parity': 'o3_full', 'num_layers': 1, 'env_embed_multiplicity': 8, 'embed_initial_edge': True, 'two_body_latent_mlp_latent_dimensions': [32, 64, 128], 'two_body_latent_mlp_nonlinearity': 'silu', 'two_body_latent_mlp_initialization': 'uniform', 'latent_mlp_latent_dimensions': [128], 'latent_mlp_nonlinearity': 'silu', 'latent_mlp_initialization': 'uniform', 'latent_resnet': True, 'env_embed_mlp_latent_dimensions': [], 'env_embed_mlp_nonlinearity': None, 'env_embed_mlp_initialization': 'uniform', 'edge_eng_mlp_latent_dimensions': [32], 'edge_eng_mlp_nonlinearity': None, 'edge_eng_mlp_initialization': 'uniform', 'dataset': 'ase', 'dataset_file_name': '../datasets/NaBr-data.xyz', 'ase_args': {'format': 'extxyz'}, 'chemical_symbol_to_type': {'Na': 0, 'Br': 1}, 'wandb_project': 'allegro-tutorial', 'log_batch_freq': 10, 'n_train': 150, 'n_val': 30, 'batch_size': 1, 'max_epochs': 100, 'learning_rate': 0.002, 'train_val_split': 'random', 'shuffle': True, 'metrics_key': 'validation_loss', 'use_ema': True, 'ema_decay': 0.99, 'ema_use_num_updates': True, 'loss_coeffs': {'forces': 1.0, 'total_energy': [1.0, 'PerAtomMSELoss']}, 'optimizer_name': 'Adam', 'optimizer_params': {'amsgrad': False, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0}, 'metrics_components': [['forces', 'mae'], ['forces', 'rmse'], ['total_energy', 'mae'], ['total_energy', 'mae', {'PerAtom': True}]], 'lr_scheduler_name': 'ReduceLROnPlateau', 'lr_scheduler_patience': 50, 'lr_scheduler_factor': 0.5, 'early_stopping_lower_bounds': {'LR': 1e-05}, 'early_stopping_patiences': {'validation_loss': 100}, 'dataset_AtomicData_options': {'r_max': 5.0}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce523e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtomicData(atom_types=[64, 1], cell=[3, 3], edge_cell_shift=[1146, 3], edge_index=[2, 1146], forces=[64, 3], free_energy=[1], pbc=[3], pos=[64, 3], stress=[3, 3], total_energy=[1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config.from_file('./configs/NaBr.yaml', defaults=default_config)\n",
    "    \n",
    "\n",
    "dataset = dataset_from_config(config, prefix=\"dataset\")\n",
    "\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce0f4c5",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37c8975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Union, List\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "\n",
    "import ase.data\n",
    "\n",
    "#from nequip.data import AtomicData, AtomicDataDict\n",
    "\n",
    "\n",
    "class TypeMapper:\n",
    "    \"\"\"Based on a configuration, map atomic numbers to types.\"\"\"\n",
    "\n",
    "    num_types: int\n",
    "    chemical_symbol_to_type: Optional[Dict[str, int]]\n",
    "    type_to_chemical_symbol: Optional[Dict[int, str]]\n",
    "    type_names: List[str]\n",
    "    _min_Z: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        type_names: Optional[List[str]] = None,\n",
    "        chemical_symbol_to_type: Optional[Dict[str, int]] = None,\n",
    "        type_to_chemical_symbol: Optional[Dict[int, str]] = None,\n",
    "        chemical_symbols: Optional[List[str]] = None,\n",
    "    ):\n",
    "        if chemical_symbols is not None:\n",
    "            if chemical_symbol_to_type is not None:\n",
    "                raise ValueError(\n",
    "                    \"Cannot provide both `chemical_symbols` and `chemical_symbol_to_type`\"\n",
    "                )\n",
    "            # repro old, sane NequIP behaviour\n",
    "            # checks also for validity of keys\n",
    "            atomic_nums = [ase.data.atomic_numbers[sym] for sym in chemical_symbols]\n",
    "            # https://stackoverflow.com/questions/29876580/how-to-sort-a-list-according-to-another-list-python\n",
    "            chemical_symbols = [\n",
    "                e[1] for e in sorted(zip(atomic_nums, chemical_symbols))\n",
    "            ]\n",
    "            chemical_symbol_to_type = {k: i for i, k in enumerate(chemical_symbols)}\n",
    "            del chemical_symbols\n",
    "\n",
    "        if type_to_chemical_symbol is not None:\n",
    "            type_to_chemical_symbol = {\n",
    "                int(k): v for k, v in type_to_chemical_symbol.items()\n",
    "            }\n",
    "            assert all(\n",
    "                v in ase.data.chemical_symbols for v in type_to_chemical_symbol.values()\n",
    "            )\n",
    "\n",
    "        # Build from chem->type mapping, if provided\n",
    "        self.chemical_symbol_to_type = chemical_symbol_to_type\n",
    "        if self.chemical_symbol_to_type is not None:\n",
    "            # Validate\n",
    "            for sym, type in self.chemical_symbol_to_type.items():\n",
    "                assert sym in ase.data.atomic_numbers, f\"Invalid chemical symbol {sym}\"\n",
    "                assert 0 <= type, f\"Invalid type number {type}\"\n",
    "            assert set(self.chemical_symbol_to_type.values()) == set(\n",
    "                range(len(self.chemical_symbol_to_type))\n",
    "            )\n",
    "            if type_names is None:\n",
    "                # Make type_names\n",
    "                type_names = [None] * len(self.chemical_symbol_to_type)\n",
    "                for sym, type in self.chemical_symbol_to_type.items():\n",
    "                    type_names[type] = sym\n",
    "            else:\n",
    "                # Make sure they agree on types\n",
    "                # We already checked that chem->type is contiguous,\n",
    "                # so enough to check length since type_names is a list\n",
    "                assert len(type_names) == len(self.chemical_symbol_to_type)\n",
    "            # Make mapper array\n",
    "            valid_atomic_numbers = [\n",
    "                ase.data.atomic_numbers[sym] for sym in self.chemical_symbol_to_type\n",
    "            ]\n",
    "            self._min_Z = min(valid_atomic_numbers)\n",
    "            self._max_Z = max(valid_atomic_numbers)\n",
    "            Z_to_index = torch.full(\n",
    "                size=(1 + self._max_Z - self._min_Z,), fill_value=-1, dtype=torch.long\n",
    "            )\n",
    "            for sym, type in self.chemical_symbol_to_type.items():\n",
    "                Z_to_index[ase.data.atomic_numbers[sym] - self._min_Z] = type\n",
    "            self._Z_to_index = Z_to_index\n",
    "            self._index_to_Z = torch.zeros(\n",
    "                size=(len(self.chemical_symbol_to_type),), dtype=torch.long\n",
    "            )\n",
    "            for sym, type_idx in self.chemical_symbol_to_type.items():\n",
    "                self._index_to_Z[type_idx] = ase.data.atomic_numbers[sym]\n",
    "            self._valid_set = set(valid_atomic_numbers)\n",
    "            true_type_to_chemical_symbol = {\n",
    "                type_id: sym for sym, type_id in self.chemical_symbol_to_type.items()\n",
    "            }\n",
    "            if type_to_chemical_symbol is not None:\n",
    "                assert type_to_chemical_symbol == true_type_to_chemical_symbol\n",
    "            else:\n",
    "                type_to_chemical_symbol = true_type_to_chemical_symbol\n",
    "\n",
    "        # check\n",
    "        if type_names is None:\n",
    "            raise ValueError(\n",
    "                \"None of chemical_symbols, chemical_symbol_to_type, nor type_names was provided; exactly one is required\"\n",
    "            )\n",
    "        # validate type names\n",
    "        assert all(\n",
    "            n.isalnum() for n in type_names\n",
    "        ), \"Type names must contain only alphanumeric characters\"\n",
    "        # Set to however many maps specified -- we already checked contiguous\n",
    "        self.num_types = len(type_names)\n",
    "        # Check type_names\n",
    "        self.type_names = type_names\n",
    "        self.type_to_chemical_symbol = type_to_chemical_symbol\n",
    "        if self.type_to_chemical_symbol is not None:\n",
    "            assert set(type_to_chemical_symbol.keys()) == set(range(self.num_types))\n",
    "\n",
    "    def __call__(\n",
    "        self, data: Union[AtomicDataDict.Type, AtomicData], types_required: bool = True\n",
    "    ) -> Union[AtomicDataDict.Type, AtomicData]:\n",
    "        if AtomicDataDict.ATOM_TYPE_KEY in data:\n",
    "            if AtomicDataDict.ATOMIC_NUMBERS_KEY in data:\n",
    "                warnings.warn(\n",
    "                    \"Data contained both ATOM_TYPE_KEY and ATOMIC_NUMBERS_KEY; ignoring ATOMIC_NUMBERS_KEY\"\n",
    "                )\n",
    "        elif AtomicDataDict.ATOMIC_NUMBERS_KEY in data:\n",
    "            assert (\n",
    "                self.chemical_symbol_to_type is not None\n",
    "            ), \"Atomic numbers provided but there is no chemical_symbols/chemical_symbol_to_type mapping!\"\n",
    "            atomic_numbers = data[AtomicDataDict.ATOMIC_NUMBERS_KEY]\n",
    "            del data[AtomicDataDict.ATOMIC_NUMBERS_KEY]\n",
    "\n",
    "            data[AtomicDataDict.ATOM_TYPE_KEY] = self.transform(atomic_numbers)\n",
    "        else:\n",
    "            if types_required:\n",
    "                raise KeyError(\n",
    "                    \"Data doesn't contain any atom type information (ATOM_TYPE_KEY or ATOMIC_NUMBERS_KEY)\"\n",
    "                )\n",
    "        return data\n",
    "\n",
    "    def transform(self, atomic_numbers):\n",
    "        \"\"\"core function to transform an array to specie index list\"\"\"\n",
    "\n",
    "        if atomic_numbers.min() < self._min_Z or atomic_numbers.max() > self._max_Z:\n",
    "            bad_set = set(torch.unique(atomic_numbers).cpu().tolist()) - self._valid_set\n",
    "            raise ValueError(\n",
    "                f\"Data included atomic numbers {bad_set} that are not part of the atomic number -> type mapping!\"\n",
    "            )\n",
    "\n",
    "        return self._Z_to_index.to(device=atomic_numbers.device)[\n",
    "            atomic_numbers - self._min_Z\n",
    "        ]\n",
    "\n",
    "    def untransform(self, atom_types):\n",
    "        \"\"\"Transform atom types back into atomic numbers\"\"\"\n",
    "        return self._index_to_Z[atom_types].to(device=atom_types.device)\n",
    "\n",
    "    @property\n",
    "    def has_chemical_symbols(self) -> bool:\n",
    "        return self.chemical_symbol_to_type is not None\n",
    "\n",
    "    @staticmethod\n",
    "    def format(\n",
    "        data: list, type_names: List[str], element_formatter: str = \".6f\"\n",
    "    ) -> str:\n",
    "        data = torch.as_tensor(data) if data is not None else None\n",
    "        if data is None:\n",
    "            return f\"[{', '.join(type_names)}: None]\"\n",
    "        elif data.ndim == 0:\n",
    "            return (f\"[{', '.join(type_names)}: {{:{element_formatter}}}]\").format(data)\n",
    "        elif data.ndim == 1 and len(data) == len(type_names):\n",
    "            return (\n",
    "                \"[\"\n",
    "                + \", \".join(\n",
    "                    f\"{{{i}[0]}}: {{{i}[1]:{element_formatter}}}\"\n",
    "                    for i in range(len(data))\n",
    "                )\n",
    "                + \"]\"\n",
    "            ).format(*zip(type_names, data))\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Don't know how to format data=`{data}` for types {type_names} with element_formatter=`{element_formatter}`\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fbf65b",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7664bacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Callable, Union, Any, Tuple\n",
    "\n",
    "import re\n",
    "import copy\n",
    "import warnings\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import sys\n",
    "from collections.abc import Sequence\n",
    "\n",
    "import torch.utils.data\n",
    "from torch import Tensor\n",
    "\n",
    "#from .data import Data\n",
    "from nequip.utils.torch_geometric.utils import makedirs\n",
    "\n",
    "IndexType = Union[slice, Tensor, np.ndarray, Sequence]\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    r\"\"\"Dataset base class for creating graph datasets.\n",
    "    See `here <https://pytorch-geometric.readthedocs.io/en/latest/notes/\n",
    "    create_dataset.html>`__ for the accompanying tutorial.\n",
    "\n",
    "    Args:\n",
    "        root (string, optional): Root directory where the dataset should be\n",
    "            saved. (optional: :obj:`None`)\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "        pre_filter (callable, optional): A function that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a boolean\n",
    "            value, indicating whether the data object should be included in the\n",
    "            final dataset. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> Union[str, List[str], Tuple]:\n",
    "        r\"\"\"The name of the files to find in the :obj:`self.raw_dir` folder in\n",
    "        order to skip the download.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> Union[str, List[str], Tuple]:\n",
    "        r\"\"\"The name of the files to find in the :obj:`self.processed_dir`\n",
    "        folder in order to skip the processing.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def download(self):\n",
    "        r\"\"\"Downloads the dataset to the :obj:`self.raw_dir` folder.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def process(self):\n",
    "        r\"\"\"Processes the dataset to the :obj:`self.processed_dir` folder.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def len(self) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get(self, idx: int) -> Data:\n",
    "        r\"\"\"Gets the data object at index :obj:`idx`.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: Optional[str] = None,\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "        pre_filter: Optional[Callable] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(root, str):\n",
    "            root = osp.expanduser(osp.normpath(root))\n",
    "\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.pre_transform = pre_transform\n",
    "        self.pre_filter = pre_filter\n",
    "        self._indices: Optional[Sequence] = None\n",
    "\n",
    "        if \"download\" in self.__class__.__dict__.keys():\n",
    "            self._download()\n",
    "\n",
    "        if \"process\" in self.__class__.__dict__.keys():\n",
    "            self._process()\n",
    "\n",
    "    def indices(self) -> Sequence:\n",
    "        return range(self.len()) if self._indices is None else self._indices\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self) -> str:\n",
    "        return osp.join(self.root, \"raw\")\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self) -> str:\n",
    "        return osp.join(self.root, \"processed\")\n",
    "\n",
    "    @property\n",
    "    def num_node_features(self) -> int:\n",
    "        r\"\"\"Returns the number of features per node in the dataset.\"\"\"\n",
    "        data = self[0]\n",
    "        if hasattr(data, \"num_node_features\"):\n",
    "            return data.num_node_features\n",
    "        raise AttributeError(\n",
    "            f\"'{data.__class__.__name__}' object has no \"\n",
    "            f\"attribute 'num_node_features'\"\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def num_features(self) -> int:\n",
    "        r\"\"\"Alias for :py:attr:`~num_node_features`.\"\"\"\n",
    "        return self.num_node_features\n",
    "\n",
    "    @property\n",
    "    def num_edge_features(self) -> int:\n",
    "        r\"\"\"Returns the number of features per edge in the dataset.\"\"\"\n",
    "        data = self[0]\n",
    "        if hasattr(data, \"num_edge_features\"):\n",
    "            return data.num_edge_features\n",
    "        raise AttributeError(\n",
    "            f\"'{data.__class__.__name__}' object has no \"\n",
    "            f\"attribute 'num_edge_features'\"\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def raw_paths(self) -> List[str]:\n",
    "        r\"\"\"The filepaths to find in order to skip the download.\"\"\"\n",
    "        files = to_list(self.raw_file_names)\n",
    "        return [osp.join(self.raw_dir, f) for f in files]\n",
    "\n",
    "    @property\n",
    "    def processed_paths(self) -> List[str]:\n",
    "        r\"\"\"The filepaths to find in the :obj:`self.processed_dir`\n",
    "        folder in order to skip the processing.\"\"\"\n",
    "        files = to_list(self.processed_file_names)\n",
    "        return [osp.join(self.processed_dir, f) for f in files]\n",
    "\n",
    "    def _download(self):\n",
    "        if files_exist(self.raw_paths):  # pragma: no cover\n",
    "            return\n",
    "\n",
    "        makedirs(self.raw_dir)\n",
    "        self.download()\n",
    "\n",
    "    def _process(self):\n",
    "        f = osp.join(self.processed_dir, \"pre_transform.pt\")\n",
    "        if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
    "            warnings.warn(\n",
    "                f\"The `pre_transform` argument differs from the one used in \"\n",
    "                f\"the pre-processed version of this dataset. If you want to \"\n",
    "                f\"make use of another pre-processing technique, make sure to \"\n",
    "                f\"sure to delete '{self.processed_dir}' first\"\n",
    "            )\n",
    "\n",
    "        f = osp.join(self.processed_dir, \"pre_filter.pt\")\n",
    "        if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
    "            warnings.warn(\n",
    "                \"The `pre_filter` argument differs from the one used in the \"\n",
    "                \"pre-processed version of this dataset. If you want to make \"\n",
    "                \"use of another pre-fitering technique, make sure to delete \"\n",
    "                \"'{self.processed_dir}' first\"\n",
    "            )\n",
    "\n",
    "        if files_exist(self.processed_paths):  # pragma: no cover\n",
    "            return\n",
    "\n",
    "        print(\"Processing dataset...\", file=sys.stderr)\n",
    "\n",
    "        makedirs(self.processed_dir)\n",
    "        self.process()\n",
    "\n",
    "        path = osp.join(self.processed_dir, \"pre_transform.pt\")\n",
    "        torch.save(_repr(self.pre_transform), path)\n",
    "        path = osp.join(self.processed_dir, \"pre_filter.pt\")\n",
    "        torch.save(_repr(self.pre_filter), path)\n",
    "\n",
    "        print(\"Done!\", file=sys.stderr)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        r\"\"\"The number of examples in the dataset.\"\"\"\n",
    "        return len(self.indices())\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: Union[int, np.integer, IndexType],\n",
    "    ) -> Union[\"Dataset\", Data]:\n",
    "        r\"\"\"In case :obj:`idx` is of type integer, will return the data object\n",
    "        at index :obj:`idx` (and transforms it in case :obj:`transform` is\n",
    "        present).\n",
    "        In case :obj:`idx` is a slicing object, *e.g.*, :obj:`[2:5]`, a list, a\n",
    "        tuple, a PyTorch :obj:`LongTensor` or a :obj:`BoolTensor`, or a numpy\n",
    "        :obj:`np.array`, will return a subset of the dataset at the specified\n",
    "        indices.\"\"\"\n",
    "        if (\n",
    "            isinstance(idx, (int, np.integer))\n",
    "            or (isinstance(idx, Tensor) and idx.dim() == 0)\n",
    "            or (isinstance(idx, np.ndarray) and np.isscalar(idx))\n",
    "        ):\n",
    "\n",
    "            data = self.get(self.indices()[idx])\n",
    "            data = data if self.transform is None else self.transform(data)\n",
    "            return data\n",
    "\n",
    "        else:\n",
    "            return self.index_select(idx)\n",
    "\n",
    "    def index_select(self, idx: IndexType) -> \"Dataset\":\n",
    "        indices = self.indices()\n",
    "\n",
    "        if isinstance(idx, slice):\n",
    "            indices = indices[idx]\n",
    "\n",
    "        elif isinstance(idx, Tensor) and idx.dtype == torch.long:\n",
    "            return self.index_select(idx.flatten().tolist())\n",
    "\n",
    "        elif isinstance(idx, Tensor) and idx.dtype == torch.bool:\n",
    "            idx = idx.flatten().nonzero(as_tuple=False)\n",
    "            return self.index_select(idx.flatten().tolist())\n",
    "\n",
    "        elif isinstance(idx, np.ndarray) and idx.dtype == np.int64:\n",
    "            return self.index_select(idx.flatten().tolist())\n",
    "\n",
    "        elif isinstance(idx, np.ndarray) and idx.dtype == np.bool:\n",
    "            idx = idx.flatten().nonzero()[0]\n",
    "            return self.index_select(idx.flatten().tolist())\n",
    "\n",
    "        elif isinstance(idx, Sequence) and not isinstance(idx, str):\n",
    "            indices = [indices[i] for i in idx]\n",
    "\n",
    "        else:\n",
    "            raise IndexError(\n",
    "                f\"Only integers, slices (':'), list, tuples, torch.tensor and \"\n",
    "                f\"np.ndarray of dtype long or bool are valid indices (got \"\n",
    "                f\"'{type(idx).__name__}')\"\n",
    "            )\n",
    "\n",
    "        dataset = copy.copy(self)\n",
    "        dataset._indices = indices\n",
    "        return dataset\n",
    "\n",
    "    def shuffle(\n",
    "        self,\n",
    "        return_perm: bool = False,\n",
    "    ) -> Union[\"Dataset\", Tuple[\"Dataset\", Tensor]]:\n",
    "        r\"\"\"Randomly shuffles the examples in the dataset.\n",
    "\n",
    "        Args:\n",
    "            return_perm (bool, optional): If set to :obj:`True`, will return\n",
    "                the random permutation used to shuffle the dataset in addition.\n",
    "                (default: :obj:`False`)\n",
    "        \"\"\"\n",
    "        perm = torch.randperm(len(self))\n",
    "        dataset = self.index_select(perm)\n",
    "        return (dataset, perm) if return_perm is True else dataset\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        arg_repr = str(len(self)) if len(self) > 1 else \"\"\n",
    "        return f\"{self.__class__.__name__}({arg_repr})\"\n",
    "\n",
    "\n",
    "def to_list(value: Any) -> Sequence:\n",
    "    if isinstance(value, Sequence) and not isinstance(value, str):\n",
    "        return value\n",
    "    else:\n",
    "        return [value]\n",
    "\n",
    "\n",
    "def files_exist(files: List[str]) -> bool:\n",
    "    # NOTE: We return `False` in case `files` is empty, leading to a\n",
    "    # re-processing of files on every instantiation.\n",
    "    return len(files) != 0 and all([osp.exists(f) for f in files])\n",
    "\n",
    "\n",
    "def _repr(obj: Any) -> str:\n",
    "    if obj is None:\n",
    "        return \"None\"\n",
    "    return re.sub(\"(<.*?)\\\\s.*(>)\", r\"\\1\\2\", obj.__repr__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "edcd68b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from collections.abc import Sequence\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "\n",
    "#from .data import Data\n",
    "#from .dataset import IndexType\n",
    "\n",
    "\n",
    "class Batch(Data):\n",
    "    r\"\"\"A plain old python object modeling a batch of graphs as one big\n",
    "    (disconnected) graph. With :class:`torch_geometric.data.Data` being the\n",
    "    base class, all its methods can also be used here.\n",
    "    In addition, single graphs can be reconstructed via the assignment vector\n",
    "    :obj:`batch`, which maps each node to its respective graph identifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch=None, ptr=None, **kwargs):\n",
    "        super(Batch, self).__init__(**kwargs)\n",
    "\n",
    "        for key, item in kwargs.items():\n",
    "            if key == \"num_nodes\":\n",
    "                self.__num_nodes__ = item\n",
    "            else:\n",
    "                self[key] = item\n",
    "\n",
    "        self.batch = batch\n",
    "        self.ptr = ptr\n",
    "        self.__data_class__ = Data\n",
    "        self.__slices__ = None\n",
    "        self.__cumsum__ = None\n",
    "        self.__cat_dims__ = None\n",
    "        self.__num_nodes_list__ = None\n",
    "        self.__num_graphs__ = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_data_list(cls, data_list, follow_batch=[], exclude_keys=[]):\n",
    "        r\"\"\"Constructs a batch object from a python list holding\n",
    "        :class:`torch_geometric.data.Data` objects.\n",
    "        The assignment vector :obj:`batch` is created on the fly.\n",
    "        Additionally, creates assignment batch vectors for each key in\n",
    "        :obj:`follow_batch`.\n",
    "        Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\n",
    "\n",
    "        keys = list(set(data_list[0].keys) - set(exclude_keys))\n",
    "        assert \"batch\" not in keys and \"ptr\" not in keys\n",
    "\n",
    "        batch = cls()\n",
    "        for key in data_list[0].__dict__.keys():\n",
    "            if key[:2] != \"__\" and key[-2:] != \"__\":\n",
    "                batch[key] = None\n",
    "\n",
    "        batch.__num_graphs__ = len(data_list)\n",
    "        batch.__data_class__ = data_list[0].__class__\n",
    "        for key in keys + [\"batch\"]:\n",
    "            batch[key] = []\n",
    "        batch[\"ptr\"] = [0]\n",
    "\n",
    "        device = None\n",
    "        slices = {key: [0] for key in keys}\n",
    "        cumsum = {key: [0] for key in keys}\n",
    "        cat_dims = {}\n",
    "        num_nodes_list = []\n",
    "        for i, data in enumerate(data_list):\n",
    "            for key in keys:\n",
    "                item = data[key]\n",
    "\n",
    "                # Increase values by `cumsum` value.\n",
    "                cum = cumsum[key][-1]\n",
    "                if isinstance(item, Tensor) and item.dtype != torch.bool:\n",
    "                    if not isinstance(cum, int) or cum != 0:\n",
    "                        item = item + cum\n",
    "                elif isinstance(item, (int, float)):\n",
    "                    item = item + cum\n",
    "\n",
    "                # Gather the size of the `cat` dimension.\n",
    "                size = 1\n",
    "                cat_dim = data.__cat_dim__(key, data[key])\n",
    "                # 0-dimensional tensors have no dimension along which to\n",
    "                # concatenate, so we set `cat_dim` to `None`.\n",
    "                if isinstance(item, Tensor) and item.dim() == 0:\n",
    "                    cat_dim = None\n",
    "                cat_dims[key] = cat_dim\n",
    "\n",
    "                # Add a batch dimension to items whose `cat_dim` is `None`:\n",
    "                if isinstance(item, Tensor) and cat_dim is None:\n",
    "                    cat_dim = 0  # Concatenate along this new batch dimension.\n",
    "                    item = item.unsqueeze(0)\n",
    "                    device = item.device\n",
    "                elif isinstance(item, Tensor):\n",
    "                    size = item.size(cat_dim)\n",
    "                    device = item.device\n",
    "\n",
    "                batch[key].append(item)  # Append item to the attribute list.\n",
    "\n",
    "                slices[key].append(size + slices[key][-1])\n",
    "                inc = data.__inc__(key, item)\n",
    "                if isinstance(inc, (tuple, list)):\n",
    "                    inc = torch.tensor(inc)\n",
    "                cumsum[key].append(inc + cumsum[key][-1])\n",
    "\n",
    "                if key in follow_batch:\n",
    "                    if isinstance(size, Tensor):\n",
    "                        for j, size in enumerate(size.tolist()):\n",
    "                            tmp = f\"{key}_{j}_batch\"\n",
    "                            batch[tmp] = [] if i == 0 else batch[tmp]\n",
    "                            batch[tmp].append(\n",
    "                                torch.full((size,), i, dtype=torch.long, device=device)\n",
    "                            )\n",
    "                    else:\n",
    "                        tmp = f\"{key}_batch\"\n",
    "                        batch[tmp] = [] if i == 0 else batch[tmp]\n",
    "                        batch[tmp].append(\n",
    "                            torch.full((size,), i, dtype=torch.long, device=device)\n",
    "                        )\n",
    "\n",
    "            if hasattr(data, \"__num_nodes__\"):\n",
    "                num_nodes_list.append(data.__num_nodes__)\n",
    "            else:\n",
    "                num_nodes_list.append(None)\n",
    "\n",
    "            num_nodes = data.num_nodes\n",
    "            if num_nodes is not None:\n",
    "                item = torch.full((num_nodes,), i, dtype=torch.long, device=device)\n",
    "                batch.batch.append(item)\n",
    "                batch.ptr.append(batch.ptr[-1] + num_nodes)\n",
    "\n",
    "        batch.batch = None if len(batch.batch) == 0 else batch.batch\n",
    "        batch.ptr = None if len(batch.ptr) == 1 else batch.ptr\n",
    "        batch.__slices__ = slices\n",
    "        batch.__cumsum__ = cumsum\n",
    "        batch.__cat_dims__ = cat_dims\n",
    "        batch.__num_nodes_list__ = num_nodes_list\n",
    "\n",
    "        ref_data = data_list[0]\n",
    "        for key in batch.keys:\n",
    "            items = batch[key]\n",
    "            if None in items:\n",
    "                raise ValueError(\n",
    "                    f\"Found a `None` in the provided data objects for batching in key `{key}`\"\n",
    "                )\n",
    "            item = items[0]\n",
    "            cat_dim = ref_data.__cat_dim__(key, item)\n",
    "            cat_dim = 0 if cat_dim is None else cat_dim\n",
    "            if isinstance(item, Tensor):\n",
    "                batch[key] = torch.cat(items, cat_dim)\n",
    "            elif isinstance(item, (int, float)):\n",
    "                batch[key] = torch.tensor(items)\n",
    "\n",
    "        # if torch_geometric.is_debug_enabled():\n",
    "        #     batch.debug()\n",
    "\n",
    "        return batch.contiguous()\n",
    "\n",
    "    def get_example(self, idx: int) -> Data:\n",
    "        r\"\"\"Reconstructs the :class:`torch_geometric.data.Data` object at index\n",
    "        :obj:`idx` from the batch object.\n",
    "        The batch object must have been created via :meth:`from_data_list` in\n",
    "        order to be able to reconstruct the initial objects.\"\"\"\n",
    "\n",
    "        if self.__slices__ is None:\n",
    "            raise RuntimeError(\n",
    "                (\n",
    "                    \"Cannot reconstruct data list from batch because the batch \"\n",
    "                    \"object was not created using `Batch.from_data_list()`.\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        data = self.__data_class__()\n",
    "        idx = self.num_graphs + idx if idx < 0 else idx\n",
    "\n",
    "        for key in self.__slices__.keys():\n",
    "            item = self[key]\n",
    "            if self.__cat_dims__[key] is None:\n",
    "                # The item was concatenated along a new batch dimension,\n",
    "                # so just index in that dimension:\n",
    "                item = item[idx]\n",
    "            else:\n",
    "                # Narrow the item based on the values in `__slices__`.\n",
    "                if isinstance(item, Tensor):\n",
    "                    dim = self.__cat_dims__[key]\n",
    "                    start = self.__slices__[key][idx]\n",
    "                    end = self.__slices__[key][idx + 1]\n",
    "                    item = item.narrow(dim, start, end - start)\n",
    "                else:\n",
    "                    start = self.__slices__[key][idx]\n",
    "                    end = self.__slices__[key][idx + 1]\n",
    "                    item = item[start:end]\n",
    "                    item = item[0] if len(item) == 1 else item\n",
    "\n",
    "            # Decrease its value by `cumsum` value:\n",
    "            cum = self.__cumsum__[key][idx]\n",
    "            if isinstance(item, Tensor):\n",
    "                if not isinstance(cum, int) or cum != 0:\n",
    "                    item = item - cum\n",
    "            elif isinstance(item, (int, float)):\n",
    "                item = item - cum\n",
    "\n",
    "            data[key] = item\n",
    "\n",
    "        if self.__num_nodes_list__[idx] is not None:\n",
    "            data.num_nodes = self.__num_nodes_list__[idx]\n",
    "\n",
    "        return data\n",
    "\n",
    "    def index_select(self, idx: IndexType) -> List[Data]:\n",
    "        if isinstance(idx, slice):\n",
    "            idx = list(range(self.num_graphs)[idx])\n",
    "\n",
    "        elif isinstance(idx, Tensor) and idx.dtype == torch.long:\n",
    "            idx = idx.flatten().tolist()\n",
    "\n",
    "        elif isinstance(idx, Tensor) and idx.dtype == torch.bool:\n",
    "            idx = idx.flatten().nonzero(as_tuple=False).flatten().tolist()\n",
    "\n",
    "        elif isinstance(idx, np.ndarray) and idx.dtype == np.int64:\n",
    "            idx = idx.flatten().tolist()\n",
    "\n",
    "        elif isinstance(idx, np.ndarray) and idx.dtype == np.bool:\n",
    "            idx = idx.flatten().nonzero()[0].flatten().tolist()\n",
    "\n",
    "        elif isinstance(idx, Sequence) and not isinstance(idx, str):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            raise IndexError(\n",
    "                f\"Only integers, slices (':'), list, tuples, torch.tensor and \"\n",
    "                f\"np.ndarray of dtype long or bool are valid indices (got \"\n",
    "                f\"'{type(idx).__name__}')\"\n",
    "            )\n",
    "\n",
    "        return [self.get_example(i) for i in idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, str):\n",
    "            return super(Batch, self).__getitem__(idx)\n",
    "        elif isinstance(idx, (int, np.integer)):\n",
    "            return self.get_example(idx)\n",
    "        else:\n",
    "            return self.index_select(idx)\n",
    "\n",
    "    def to_data_list(self) -> List[Data]:\n",
    "        r\"\"\"Reconstructs the list of :class:`torch_geometric.data.Data` objects\n",
    "        from the batch object.\n",
    "        The batch object must have been created via :meth:`from_data_list` in\n",
    "        order to be able to reconstruct the initial objects.\"\"\"\n",
    "        return [self.get_example(i) for i in range(self.num_graphs)]\n",
    "\n",
    "    @property\n",
    "    def num_graphs(self) -> int:\n",
    "        \"\"\"Returns the number of graphs in the batch.\"\"\"\n",
    "        if self.__num_graphs__ is not None:\n",
    "            return self.__num_graphs__\n",
    "        elif self.ptr is not None:\n",
    "            return self.ptr.numel() - 1\n",
    "        elif self.batch is not None:\n",
    "            return int(self.batch.max()) + 1\n",
    "        else:\n",
    "            raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d96c2c2",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97f3baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import inspect\n",
    "import itertools\n",
    "import yaml\n",
    "import hashlib\n",
    "import math\n",
    "from typing import Tuple, Dict, Any, List, Callable, Union, Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch_runstats.scatter import scatter_std, scatter_mean\n",
    "\n",
    "#from nequip.utils.torch_geometric import Batch, Dataset\n",
    "from nequip.utils.torch_geometric.utils import download_url, extract_zip\n",
    "\n",
    "#import nequip\n",
    "#from nequip.data import (\n",
    "#    AtomicData,\n",
    "#    AtomicDataDict,\n",
    "#    _NODE_FIELDS,\n",
    "#    _EDGE_FIELDS,\n",
    "#    _GRAPH_FIELDS,\n",
    "#)\n",
    "from nequip.utils.batch_ops import bincount\n",
    "from nequip.utils.regressor import solver\n",
    "from nequip.utils.savenload import atomic_write\n",
    "#from ..transforms import TypeMapper\n",
    "\n",
    "\n",
    "class AtomicDataset(Dataset):\n",
    "    \"\"\"The base class for all NequIP datasets.\"\"\"\n",
    "\n",
    "    root: str\n",
    "    dtype: torch.dtype\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        type_mapper: Optional[TypeMapper] = None,\n",
    "    ):\n",
    "        self.dtype = torch.get_default_dtype()\n",
    "        super().__init__(root=root, transform=type_mapper)\n",
    "\n",
    "    def statistics(\n",
    "        self,\n",
    "        fields: List[Union[str, Callable]],\n",
    "        modes: List[str],\n",
    "        stride: int = 1,\n",
    "        unbiased: bool = True,\n",
    "        kwargs: Optional[Dict[str, dict]] = {},\n",
    "    ) -> List[tuple]:\n",
    "        # TODO: If needed, this can eventually be implimented for general AtomicDataset \n",
    "        # by computing an online running mean and using Welford's method \n",
    "        # for a stable running standard deviation: \n",
    "        # https://jonisalonen.com/2013/deriving-welfords-method-for-computing-variance/\n",
    "        # That would be needed if we have lazy loading datasets.\n",
    "        # TODO: When lazy-loading datasets are implimented, how to deal with statistics, sampling, and subsets?\n",
    "        raise NotImplementedError(\"not implimented for general AtomicDataset yet\")\n",
    "\n",
    "    @property\n",
    "    def type_mapper(self) -> Optional[TypeMapper]:\n",
    "        # self.transform is always a TypeMapper\n",
    "        return self.transform\n",
    "\n",
    "    def _get_parameters(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get a dict of the parameters used to build this dataset.\"\"\"\n",
    "        pnames = list(inspect.signature(self.__init__).parameters)\n",
    "        IGNORE_KEYS = {\n",
    "            # the type mapper is applied after saving, not before, so doesn't matter to cache validity\n",
    "            \"type_mapper\"\n",
    "        }\n",
    "        params = {\n",
    "            k: getattr(self, k)\n",
    "            for k in pnames\n",
    "            if k not in IGNORE_KEYS and hasattr(self, k)\n",
    "        }\n",
    "        # Add other relevant metadata:\n",
    "        params[\"dtype\"] = str(self.dtype)\n",
    "        params[\"nequip_version\"] = nequip.__version__\n",
    "        return params\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self) -> str:\n",
    "        # We want the file name to change when the parameters change\n",
    "        # So, first we get all parameters:\n",
    "        params = self._get_parameters()\n",
    "        # Make some kind of string of them:\n",
    "        # we don't care about this possibly changing between python versions,\n",
    "        # since a change in python version almost certainly means a change in\n",
    "        # versions of other things too, and is a good reason to recompute\n",
    "        buffer = yaml.dump(params).encode(\"ascii\")\n",
    "        # And hash it:\n",
    "        param_hash = hashlib.sha1(buffer).hexdigest()\n",
    "        return f\"{self.root}/processed_dataset_{param_hash}\"\n",
    "\n",
    "\n",
    "class AtomicInMemoryDataset(AtomicDataset):\n",
    "    r\"\"\"Base class for all datasets that fit in memory.\n",
    "\n",
    "    Please note that, as a ``pytorch_geometric`` dataset, it must be backed by some kind of disk storage.\n",
    "    By default, the raw file will be stored at root/raw and the processed torch\n",
    "    file will be at root/process.\n",
    "\n",
    "    Subclasses must implement:\n",
    "     - ``raw_file_names``\n",
    "     - ``get_data()``\n",
    "\n",
    "    Subclasses may implement:\n",
    "     - ``download()`` or ``self.url`` or ``ClassName.URL``\n",
    "\n",
    "    Args:\n",
    "        root (str, optional): Root directory where the dataset should be saved. \n",
    "        Defaults to current working directory.\n",
    "        file_name (str, optional): file name of data source. only used in children class\n",
    "        url (str, optional): url to download data source\n",
    "        AtomicData_options (dict, optional): extra key that are not stored in data \n",
    "        but needed for AtomicData initialization\n",
    "        include_frames (list, optional): the frames to process with the constructor.\n",
    "        type_mapper (TypeMapper): the transformation to map atomic information to species index. Optional\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        file_name: Optional[str] = None,\n",
    "        url: Optional[str] = None,\n",
    "        AtomicData_options: Dict[str, Any] = {},\n",
    "        include_frames: Optional[List[int]] = None,\n",
    "        type_mapper: Optional[TypeMapper] = None,\n",
    "    ):\n",
    "        # TO DO, this may be simplified\n",
    "        # See if a subclass defines some inputs\n",
    "        self.file_name = (\n",
    "            getattr(type(self), \"FILE_NAME\", None) if file_name is None else file_name\n",
    "        )\n",
    "        self.url = getattr(type(self), \"URL\", url)\n",
    "\n",
    "        self.AtomicData_options = AtomicData_options\n",
    "        self.include_frames = include_frames\n",
    "\n",
    "        self.data = None\n",
    "\n",
    "        # !!! don't delete this block.\n",
    "        # otherwise the inherent children class\n",
    "        # will ignore the download function here\n",
    "        class_type = type(self)\n",
    "        if class_type != AtomicInMemoryDataset:\n",
    "            if \"download\" not in self.__class__.__dict__:\n",
    "                class_type.download = AtomicInMemoryDataset.download\n",
    "            if \"process\" not in self.__class__.__dict__:\n",
    "                class_type.process = AtomicInMemoryDataset.process\n",
    "\n",
    "        # Initialize the InMemoryDataset, which runs download and process\n",
    "        # See https://pytorch-geometric.readthedocs.io/en/latest/notes/create_dataset.html#creating-in-memory-datasets\n",
    "        # Then pre-process the data if disk files are not found\n",
    "        super().__init__(root=root, type_mapper=type_mapper)\n",
    "        if self.data is None:\n",
    "            self.data, include_frames = torch.load(self.processed_paths[0])\n",
    "            if not np.all(include_frames == self.include_frames):\n",
    "                raise ValueError(\n",
    "                    f\"the include_frames is changed. \"\n",
    "                    f\"please delete the processed folder and rerun {self.processed_paths[0]}\"\n",
    "                )\n",
    "\n",
    "    def len(self):\n",
    "        if self.data is None:\n",
    "            return 0\n",
    "        return self.data.num_graphs\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> List[str]:\n",
    "        return [\"data.pth\", \"params.yaml\"]\n",
    "\n",
    "    def get_data(\n",
    "        self,\n",
    "    ) -> Union[Tuple[Dict[str, Any], Dict[str, Any]], List[AtomicData]]:\n",
    "        \"\"\"Get the data --- called from ``process()``, can assume that ``raw_file_names()`` exist.\n",
    "\n",
    "        Note that parameters for graph construction such as ``pbc`` and ``r_max`` should be included here as (likely, but not necessarily, fixed) fields.\n",
    "\n",
    "        Returns:\n",
    "        A dict:\n",
    "            fields: dict\n",
    "                mapping a field name ('pos', 'cell') to a list-like sequence of tensor-like objects giving that field's value for each example.\n",
    "        Or:\n",
    "            data_list: List[AtomicData]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def download(self):\n",
    "        if (not hasattr(self, \"url\")) or (self.url is None):\n",
    "            # Don't download, assume present. \n",
    "            # Later could have FileNotFound if the files don't actually exist\n",
    "            pass\n",
    "        else:\n",
    "            download_path = download_url(self.url, self.raw_dir)\n",
    "            if download_path.endswith(\".zip\"):\n",
    "                extract_zip(download_path, self.raw_dir)\n",
    "\n",
    "    def process(self):\n",
    "        data = self.get_data()\n",
    "        if isinstance(data, list):\n",
    "\n",
    "            # It's a data list\n",
    "            data_list = data\n",
    "            if not (self.include_frames is None or data_list is None):\n",
    "                data_list = [data_list[i] for i in self.include_frames]\n",
    "            assert all(isinstance(e, AtomicData) for e in data_list)\n",
    "            assert all(AtomicDataDict.BATCH_KEY not in e for e in data_list)\n",
    "\n",
    "            fields = {}\n",
    "\n",
    "        elif isinstance(data, dict):\n",
    "            # It's fields\n",
    "            # Get our data\n",
    "            fields = data\n",
    "\n",
    "            # check keys\n",
    "            all_keys = set(fields.keys())\n",
    "            assert AtomicDataDict.BATCH_KEY not in all_keys\n",
    "            # Check bad key combinations, but don't require that this be a graph yet.\n",
    "            AtomicDataDict.validate_keys(all_keys, graph_required=False)\n",
    "\n",
    "            # check dimensionality\n",
    "            num_examples = set([len(a) for a in fields.values()])\n",
    "            if not len(num_examples) == 1:\n",
    "                shape_dict = {f: v.shape for f, v in fields.items()}\n",
    "                raise ValueError(\n",
    "                    f\"This dataset is invalid: expected all fields to have same length (same number of examples), but they had shapes {shape_dict}\"\n",
    "                )\n",
    "            num_examples = next(iter(num_examples))\n",
    "\n",
    "            include_frames = self.include_frames\n",
    "            if include_frames is None:\n",
    "                include_frames = range(num_examples)\n",
    "\n",
    "            # Make AtomicData from it:\n",
    "            if AtomicDataDict.EDGE_INDEX_KEY in all_keys:\n",
    "                # This is already a graph, just build it\n",
    "                constructor = AtomicData\n",
    "            else:\n",
    "                # do neighborlist from points\n",
    "                constructor = AtomicData.from_points\n",
    "                assert \"r_max\" in self.AtomicData_options\n",
    "                assert AtomicDataDict.POSITIONS_KEY in all_keys\n",
    "\n",
    "            data_list = [\n",
    "                constructor(\n",
    "                    **{\n",
    "                        **{f: v[i] for f, v in fields.items()},\n",
    "                        **self.AtomicData_options,\n",
    "                    }\n",
    "                )\n",
    "                for i in include_frames\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid return from `self.get_data()`\")\n",
    "\n",
    "        # Batch it for efficient saving\n",
    "        # This limits an AtomicInMemoryDataset to a maximum of LONG_MAX atoms _overall_, but that is a very big number and any dataset that large is probably not \"InMemory\" anyway\n",
    "        data = Batch.from_data_list(data_list)\n",
    "        del data_list\n",
    "        del fields\n",
    "\n",
    "        total_MBs = sum(item.numel() * item.element_size() for _, item in data) / (\n",
    "            1024 * 1024\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"Loaded data: {data}\\n    processed data size: ~{total_MBs:.2f} MB\"\n",
    "        )\n",
    "        del total_MBs\n",
    "\n",
    "        # use atomic writes to avoid race conditions between\n",
    "        # different trainings that use the same dataset\n",
    "        # since those separate trainings should all produce the same results,\n",
    "        # it doesn't matter if they overwrite each others cached'\n",
    "        # datasets. It only matters that they don't simultaneously try\n",
    "        # to write the _same_ file, corrupting it.\n",
    "        with atomic_write(self.processed_paths[0], binary=True) as f:\n",
    "            torch.save((data, self.include_frames), f)\n",
    "        with atomic_write(self.processed_paths[1], binary=False) as f:\n",
    "            yaml.dump(self._get_parameters(), f)\n",
    "\n",
    "        logging.info(\"Cached processed data to disk\")\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.data.get_example(idx)\n",
    "\n",
    "    def _selectors(\n",
    "        self,\n",
    "        stride: int = 1,\n",
    "    ):\n",
    "        if self._indices is not None:\n",
    "            graph_selector = torch.as_tensor(self._indices)[::stride]\n",
    "            # note that self._indices is _not_ necessarily in order,\n",
    "            # while self.data --- which we take our arrays from ---\n",
    "            # is always in the original order.\n",
    "            # In particular, the values of `self.data.batch`\n",
    "            # are indexes in the ORIGINAL order\n",
    "            # thus we need graph level properties to also be in the original order\n",
    "            # so that batch values index into them correctly\n",
    "            # since self.data.batch is always sorted & contiguous\n",
    "            # (because of Batch.from_data_list)\n",
    "            # we sort it:\n",
    "            graph_selector, _ = torch.sort(graph_selector)\n",
    "        else:\n",
    "            graph_selector = torch.arange(0, self.len(), stride)\n",
    "\n",
    "        node_selector = torch.as_tensor(\n",
    "            np.in1d(self.data.batch.numpy(), graph_selector.numpy())\n",
    "        )\n",
    "\n",
    "        edge_index = self.data[AtomicDataDict.EDGE_INDEX_KEY]\n",
    "        edge_selector = node_selector[edge_index[0]] & node_selector[edge_index[1]]\n",
    "\n",
    "        return (graph_selector, node_selector, edge_selector)\n",
    "\n",
    "    def statistics(\n",
    "        self,\n",
    "        fields: List[Union[str, Callable]],\n",
    "        modes: List[str],\n",
    "        stride: int = 1,\n",
    "        unbiased: bool = True,\n",
    "        kwargs: Optional[Dict[str, dict]] = {},\n",
    "    ) -> List[tuple]:\n",
    "        \"\"\"Compute the statistics of ``fields`` in the dataset.\n",
    "\n",
    "        If the values at the fields are vectors/multidimensional, they must be of fixed shape and elementwise\n",
    "        statistics will be computed.\n",
    "\n",
    "        Args:\n",
    "            fields: the names of the fields to compute statistics for.\n",
    "                Instead of a field name, a callable can also be given that reuturns a quantity \n",
    "                to compute the statisics for.\n",
    "\n",
    "                If a callable is given, it will be called with a (possibly batched) ``Data``-like object \n",
    "                and must return a sequence of points to add to the set over which the statistics \n",
    "                will be computed.\n",
    "                The callable must also return a string, one of ``\"node\"`` or ``\"graph\"``, \n",
    "                indicating whether the value it returns is a per-node or per-graph quantity.\n",
    "                PLEASE NOTE: the argument to the callable may be \"batched\", \n",
    "                and it may not be batched \"contiguously\": ``batch`` and ``edge_index`` \n",
    "                may have \"gaps\" in their values.\n",
    "\n",
    "                For example, to compute the overall statistics of the x,y, and z components \n",
    "                of a per-node vector ``force`` field:\n",
    "\n",
    "                    data.statistics([lambda data: (data.force.flatten(), \"node\")])\n",
    "\n",
    "                The above computes the statistics over a set of size 3N, where N is the total number of nodes\n",
    "                in the dataset.\n",
    "\n",
    "            modes: the statistic to compute for each field. Valid options are:\n",
    "                 - ``count``\n",
    "                 - ``rms``\n",
    "                 - ``mean_std``\n",
    "                 - ``per_atom_*``\n",
    "                 - ``per_species_*``\n",
    "\n",
    "            stride: the stride over the dataset while computing statistcs.\n",
    "\n",
    "            unbiased: whether to use unbiased for standard deviations.\n",
    "\n",
    "            kwargs: other options for individual statistics modes.\n",
    "\n",
    "        Returns:\n",
    "            List of statistics. For fields of floating dtype the statistics are the two-tuple (mean, std); for fields of integer dtype the statistics are a one-tuple (bincounts,)\n",
    "        \"\"\"\n",
    "\n",
    "        # Short circut:\n",
    "        assert len(modes) == len(fields)\n",
    "        if len(fields) == 0:\n",
    "            return []\n",
    "\n",
    "        graph_selector, node_selector, edge_selector = self._selectors(stride=stride)\n",
    "\n",
    "        num_graphs = len(graph_selector)\n",
    "        num_nodes = node_selector.sum()\n",
    "        num_edges = edge_selector.sum()\n",
    "\n",
    "        if self.transform is not None:\n",
    "            # pre-transform the data so that statistics process transformed data\n",
    "            data_transformed = self.transform(self.data.to_dict(), types_required=False)\n",
    "        else:\n",
    "            data_transformed = self.data.to_dict()\n",
    "        # pre-select arrays\n",
    "        # this ensures that all following computations use the right data\n",
    "        all_keys = set()\n",
    "        selectors = {}\n",
    "        for k in data_transformed.keys():\n",
    "            all_keys.add(k)\n",
    "            if k in _NODE_FIELDS:\n",
    "                selectors[k] = node_selector\n",
    "            elif k in _GRAPH_FIELDS:\n",
    "                selectors[k] = graph_selector\n",
    "            elif k == AtomicDataDict.EDGE_INDEX_KEY:\n",
    "                selectors[k] = (slice(None, None, None), edge_selector)\n",
    "            elif k in _EDGE_FIELDS:\n",
    "                selectors[k] = edge_selector\n",
    "        # TODO: do the batch indexes, edge_indexes, etc. after selection need to be\n",
    "        # \"compacted\" to subtract out their offsets? For now, we just punt this\n",
    "        # onto the writer of the callable field.\n",
    "        # apply selector to actual data\n",
    "        data_transformed = {\n",
    "            k: data_transformed[k][selectors[k]]\n",
    "            for k in data_transformed.keys()\n",
    "            if k in selectors\n",
    "        }\n",
    "\n",
    "        atom_types: Optional[torch.Tensor] = None\n",
    "        out: list = []\n",
    "        for ifield, field in enumerate(fields):\n",
    "            if callable(field):\n",
    "                # make a joined thing? so it includes fixed fields\n",
    "                arr, arr_is_per = field(data_transformed)\n",
    "                arr = arr.to(self.dtype)  # all statistics must be on floating\n",
    "                assert arr_is_per in (\"node\", \"graph\", \"edge\")\n",
    "            else:\n",
    "                if field not in all_keys:\n",
    "                    raise RuntimeError(\n",
    "                        f\"The field key `{field}` is not present in this dataset\"\n",
    "                    )\n",
    "                if field not in selectors:\n",
    "                    # this means field is not selected and so not available\n",
    "                    raise RuntimeError(\n",
    "                        f\"Only per-node and per-graph fields can have statistics computed; `{field}` has not been registered as either. If it is per-node or per-graph, please register it as such\"\n",
    "                    )\n",
    "                arr = data_transformed[field]\n",
    "                if field in _NODE_FIELDS:\n",
    "                    arr_is_per = \"node\"\n",
    "                elif field in _GRAPH_FIELDS:\n",
    "                    arr_is_per = \"graph\"\n",
    "                elif field in _EDGE_FIELDS:\n",
    "                    arr_is_per = \"edge\"\n",
    "                else:\n",
    "                    raise RuntimeError\n",
    "\n",
    "            # Check arr\n",
    "            if arr is None:\n",
    "                raise ValueError(\n",
    "                    f\"Cannot compute statistics over field `{field}` whose value is None!\"\n",
    "                )\n",
    "            if not isinstance(arr, torch.Tensor):\n",
    "                if np.issubdtype(arr.dtype, np.floating):\n",
    "                    arr = torch.as_tensor(arr, dtype=self.dtype)\n",
    "                else:\n",
    "                    arr = torch.as_tensor(arr)\n",
    "            if arr_is_per == \"node\":\n",
    "                arr = arr.view(num_nodes, -1)\n",
    "            elif arr_is_per == \"graph\":\n",
    "                arr = arr.view(num_graphs, -1)\n",
    "            elif arr_is_per == \"edge\":\n",
    "                arr = arr.view(num_edges, -1)\n",
    "\n",
    "            ana_mode = modes[ifield]\n",
    "            # compute statistics\n",
    "            if ana_mode == \"count\":\n",
    "                # count integers\n",
    "                uniq, counts = torch.unique(\n",
    "                    torch.flatten(arr), return_counts=True, sorted=True\n",
    "                )\n",
    "                out.append((uniq, counts))\n",
    "            elif ana_mode == \"rms\":\n",
    "                # root-mean-square\n",
    "                out.append((torch.sqrt(torch.mean(arr * arr)),))\n",
    "\n",
    "            elif ana_mode == \"mean_std\":\n",
    "                # mean and std\n",
    "                if len(arr) < 2:\n",
    "                    raise ValueError(\n",
    "                        \"Can't do per species standard deviation without at least two samples\"\n",
    "                    )\n",
    "                mean = torch.mean(arr, dim=0)\n",
    "                std = torch.std(arr, dim=0, unbiased=unbiased)\n",
    "                out.append((mean, std))\n",
    "\n",
    "            elif ana_mode == \"absmax\":\n",
    "                out.append((arr.abs().max(),))\n",
    "\n",
    "            elif ana_mode.startswith(\"per_species_\"):\n",
    "                # per-species\n",
    "                algorithm_kwargs = kwargs.pop(field + ana_mode, {})\n",
    "\n",
    "                ana_mode = ana_mode[len(\"per_species_\") :]\n",
    "\n",
    "                if atom_types is None:\n",
    "                    atom_types = data_transformed[AtomicDataDict.ATOM_TYPE_KEY]\n",
    "\n",
    "                results = self._per_species_statistics(\n",
    "                    ana_mode,\n",
    "                    arr,\n",
    "                    arr_is_per=arr_is_per,\n",
    "                    batch=data_transformed[AtomicDataDict.BATCH_KEY],\n",
    "                    atom_types=atom_types,\n",
    "                    unbiased=unbiased,\n",
    "                    algorithm_kwargs=algorithm_kwargs,\n",
    "                )\n",
    "                out.append(results)\n",
    "\n",
    "            elif ana_mode.startswith(\"per_atom_\"):\n",
    "                # per-atom\n",
    "                # only makes sense for a per-graph quantity\n",
    "                if arr_is_per != \"graph\":\n",
    "                    raise ValueError(\n",
    "                        f\"It doesn't make sense to ask for `{ana_mode}` since `{field}` is not per-graph\"\n",
    "                    )\n",
    "                ana_mode = ana_mode[len(\"per_atom_\") :]\n",
    "                results = self._per_atom_statistics(\n",
    "                    ana_mode=ana_mode,\n",
    "                    arr=arr,\n",
    "                    batch=data_transformed[AtomicDataDict.BATCH_KEY],\n",
    "                    unbiased=unbiased,\n",
    "                )\n",
    "                out.append(results)\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Cannot handle statistics mode {ana_mode}\")\n",
    "\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def _per_atom_statistics(\n",
    "        ana_mode: str,\n",
    "        arr: torch.Tensor,\n",
    "        batch: torch.Tensor,\n",
    "        unbiased: bool = True,\n",
    "    ):\n",
    "        \"\"\"Compute \"per-atom\" statistics that are normalized by the number of atoms in the system.\n",
    "\n",
    "        Only makes sense for a graph-level quantity (checked by .statistics).\n",
    "        \"\"\"\n",
    "        # using unique_consecutive handles the non-contiguous selected batch index\n",
    "        _, N = torch.unique_consecutive(batch, return_counts=True)\n",
    "        N = N.unsqueeze(-1)\n",
    "        assert N.ndim == 2\n",
    "        assert N.shape == (len(arr), 1)\n",
    "        assert arr.ndim >= 2\n",
    "        data_dim = arr.shape[1:]\n",
    "        arr = arr / N\n",
    "        assert arr.shape == (len(N),) + data_dim\n",
    "        if ana_mode == \"mean_std\":\n",
    "            if len(arr) < 2:\n",
    "                raise ValueError(\n",
    "                    \"Can't do standard deviation without at least two samples\"\n",
    "                )\n",
    "            mean = torch.mean(arr, dim=0)\n",
    "            std = torch.std(arr, unbiased=unbiased, dim=0)\n",
    "            return mean, std\n",
    "        elif ana_mode == \"rms\":\n",
    "            return (torch.sqrt(torch.mean(arr.square())),)\n",
    "        elif ana_mode == \"absmax\":\n",
    "            return (torch.max(arr.abs()),)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"{ana_mode} for per-atom analysis is not implemented\"\n",
    "            )\n",
    "\n",
    "    def _per_species_statistics(\n",
    "        self,\n",
    "        ana_mode: str,\n",
    "        arr: torch.Tensor,\n",
    "        arr_is_per: str,\n",
    "        atom_types: torch.Tensor,\n",
    "        batch: torch.Tensor,\n",
    "        unbiased: bool = True,\n",
    "        algorithm_kwargs: Optional[dict] = {},\n",
    "    ):\n",
    "        \"\"\"Compute \"per-species\" statistics.\n",
    "\n",
    "        For a graph-level quantity, models it as a linear combintation of the number of atoms \n",
    "        of different types in the graph.\n",
    "\n",
    "        For a per-node quantity, computes the expected statistic but for each type instead of over all nodes.\n",
    "        \"\"\"\n",
    "        N = bincount(atom_types.squeeze(-1), batch)\n",
    "        assert N.ndim == 2  # [batch, n_type]\n",
    "        N = N[(N > 0).any(dim=1)]  # deal with non-contiguous batch indexes\n",
    "        assert arr.ndim >= 2\n",
    "        if arr_is_per == \"graph\":\n",
    "\n",
    "            if ana_mode != \"mean_std\":\n",
    "                raise NotImplementedError(\n",
    "                    f\"{ana_mode} for per species analysis is not implemented for shape {arr.shape}\"\n",
    "                )\n",
    "\n",
    "            N = N.type(self.dtype)\n",
    "\n",
    "            return solver(N, arr, **algorithm_kwargs)\n",
    "\n",
    "        elif arr_is_per == \"node\":\n",
    "            arr = arr.type(self.dtype)\n",
    "\n",
    "            if ana_mode == \"mean_std\":\n",
    "                # There need to be at least two occurances of each atom type in the\n",
    "                # WHOLE dataset, not in any given frame:\n",
    "                if torch.any(N.sum(dim=0) < 2):\n",
    "                    raise ValueError(\n",
    "                        \"Can't do per species standard deviation without at least two samples per species\"\n",
    "                    )\n",
    "                mean = scatter_mean(arr, atom_types, dim=0)\n",
    "                assert mean.shape[1:] == arr.shape[1:]  # [N, dims] -> [type, dims]\n",
    "                assert len(mean) == N.shape[1]\n",
    "                std = scatter_std(arr, atom_types, dim=0, unbiased=unbiased)\n",
    "                assert std.shape == mean.shape\n",
    "                return mean, std\n",
    "            elif ana_mode == \"rms\":\n",
    "                square = scatter_mean(arr.square(), atom_types, dim=0)\n",
    "                assert square.shape[1:] == arr.shape[1:]  # [N, dims] -> [type, dims]\n",
    "                assert len(square) == N.shape[1]\n",
    "                dims = len(square.shape) - 1\n",
    "                for i in range(dims):\n",
    "                    square = square.mean(axis=-1)\n",
    "                return (torch.sqrt(square),)\n",
    "            else:\n",
    "                raise NotImplementedError(\n",
    "                    f\"Statistics mode {ana_mode} isn't yet implemented for per_species_\"\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def rdf(\n",
    "        self, bin_width: float, stride: int = 1\n",
    "    ) -> Dict[Tuple[int, int], Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"Compute the pairwise RDFs of the dataset.\n",
    "\n",
    "        Args:\n",
    "            bin_width: width of the histogram bin in distance units\n",
    "            stride: stride of data to include\n",
    "\n",
    "        Returns:\n",
    "            dictionary mapping `(type1, type2)` to tuples of `(hist, bin_edges)` in the style of `np.histogram`.\n",
    "        \"\"\"\n",
    "        graph_selector, node_selector, edge_selector = self._selectors(stride=stride)\n",
    "\n",
    "        data = AtomicData.to_AtomicDataDict(self.data)\n",
    "        data = AtomicDataDict.with_edge_vectors(data, with_lengths=True)\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        types = self.type_mapper(data)[AtomicDataDict.ATOM_TYPE_KEY]\n",
    "\n",
    "        edge_types = torch.index_select(\n",
    "            types, 0, data[AtomicDataDict.EDGE_INDEX_KEY].reshape(-1)\n",
    "        ).view(2, -1)\n",
    "        types_center = edge_types[0].numpy()\n",
    "        types_neigh = edge_types[1].numpy()\n",
    "\n",
    "        r_max: float = self.AtomicData_options[\"r_max\"]\n",
    "        # + 1 to always have a zero bin at the end\n",
    "        n_bins: int = int(math.ceil(r_max / bin_width)) + 1\n",
    "        # +1 since these are bin_edges including rightmost\n",
    "        bins = bin_width * np.arange(n_bins + 1)\n",
    "\n",
    "        for type1, type2 in itertools.combinations_with_replacement(\n",
    "            range(self.type_mapper.num_types), 2\n",
    "        ):\n",
    "            # Try to do as much of this as possible in-place\n",
    "            mask = types_center == type1\n",
    "            np.logical_and(mask, types_neigh == type2, out=mask)\n",
    "            np.logical_and(mask, edge_selector, out=mask)\n",
    "            mask = mask.astype(np.int32)\n",
    "            results[(type1, type2)] = np.histogram(\n",
    "                data[AtomicDataDict.EDGE_LENGTH_KEY],\n",
    "                weights=mask,\n",
    "                bins=bins,\n",
    "                density=True,\n",
    "            )\n",
    "            # RDF is symmetric\n",
    "            results[(type2, type1)] = results[(type1, type2)]\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e48fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import functools\n",
    "import itertools\n",
    "from os.path import dirname, basename, abspath\n",
    "from typing import Dict, Any, List, Union, Optional, Sequence\n",
    "\n",
    "import ase\n",
    "import ase.io\n",
    "\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "from nequip.utils.multiprocessing import num_tasks\n",
    "#from .. import AtomicData\n",
    "#from ..transforms import TypeMapper\n",
    "#from ._base_datasets import AtomicInMemoryDataset\n",
    "\n",
    "\n",
    "def _ase_dataset_reader(\n",
    "    rank: int,\n",
    "    world_size: int,\n",
    "    tmpdir: str,\n",
    "    ase_kwargs: dict,\n",
    "    atomicdata_kwargs: dict,\n",
    "    include_frames,\n",
    "    global_options: dict,\n",
    ") -> Union[str, List[AtomicData]]:\n",
    "    \"\"\"Parallel reader for all frames in file.\"\"\"\n",
    "    if world_size > 1:\n",
    "        from nequip.utils._global_options import _set_global_options\n",
    "\n",
    "        # ^ avoid import loop\n",
    "        # we only `multiprocessing` if world_size > 1\n",
    "        _set_global_options(global_options)\n",
    "    # interleave--- in theory it is better for performance for the ranks\n",
    "    # to read consecutive blocks, but the way ASE is written the whole\n",
    "    # file gets streamed through all ranks anyway, so just trust the OS\n",
    "    # to cache things sanely, which it will.\n",
    "    # ASE handles correctly the case where there are no frames in index\n",
    "    # and just gives an empty list, so that will succeed:\n",
    "    index = slice(rank, None, world_size)\n",
    "    if include_frames is None:\n",
    "        # count includes 0, 1, ..., inf\n",
    "        include_frames = itertools.count()\n",
    "\n",
    "    datas = []\n",
    "    # stream them from ase too using iread\n",
    "    for i, atoms in enumerate(ase.io.iread(**ase_kwargs, index=index, parallel=False)):\n",
    "        global_index = rank + (world_size * i)\n",
    "        datas.append(\n",
    "            (\n",
    "                global_index,\n",
    "                (\n",
    "                    AtomicData.from_ase(atoms=atoms, **atomicdata_kwargs)\n",
    "                    if global_index in include_frames\n",
    "                    # in-memory dataset will ignore this later, but needed for indexing to work out\n",
    "                    else None\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    # Save to a tempfile---\n",
    "    # there can be a _lot_ of tensors here, and rather than dealing with\n",
    "    # the complications of running out of file descriptors and setting\n",
    "    # sharing methods, since this is a one time thing, just make it simple\n",
    "    # and avoid shared memory entirely.\n",
    "    if world_size > 1:\n",
    "        path = f\"{tmpdir}/rank{rank}.pth\"\n",
    "        torch.save(datas, path)\n",
    "        return path\n",
    "    else:\n",
    "        return datas\n",
    "\n",
    "\n",
    "class ASEDataset(AtomicInMemoryDataset):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        ase_args (dict): arguments for ase.io.read\n",
    "        include_keys (list): in addition to forces and energy, the keys that needs to\n",
    "             be parsed into dataset\n",
    "             The data stored in ase.atoms.Atoms.array has the lowest priority,\n",
    "             and it will be overrided by data in ase.atoms.Atoms.info\n",
    "             and ase.atoms.Atoms.calc.results. Optional\n",
    "        key_mapping (dict): rename some of the keys to the value str. Optional\n",
    "\n",
    "    Example: Given an atomic data stored in \"H2.extxyz\" that looks like below:\n",
    "\n",
    "    ```H2.extxyz\n",
    "    2\n",
    "    Properties=species:S:1:pos:R:3 energy=-10 user_label=2.0 pbc=\"F F F\"\n",
    "     H       0.00000000       0.00000000       0.00000000\n",
    "     H       0.00000000       0.00000000       1.02000000\n",
    "    ```\n",
    "\n",
    "    The yaml input should be\n",
    "\n",
    "    ```\n",
    "    dataset: ase\n",
    "    dataset_file_name: H2.extxyz\n",
    "    ase_args:\n",
    "      format: extxyz\n",
    "    include_keys:\n",
    "      - user_label\n",
    "    key_mapping:\n",
    "      user_label: label0\n",
    "    chemical_symbols:\n",
    "      - H\n",
    "    ```\n",
    "\n",
    "    for VASP parser, the yaml input should be\n",
    "    ```\n",
    "    dataset: ase\n",
    "    dataset_file_name: OUTCAR\n",
    "    ase_args:\n",
    "      format: vasp-out\n",
    "    key_mapping:\n",
    "      free_energy: total_energy\n",
    "    chemical_symbols:\n",
    "      - H\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        ase_args: dict = {},\n",
    "        file_name: Optional[str] = None,\n",
    "        url: Optional[str] = None,\n",
    "        AtomicData_options: Dict[str, Any] = {},\n",
    "        include_frames: Optional[List[int]] = None,\n",
    "        type_mapper: TypeMapper = None,\n",
    "        key_mapping: Optional[dict] = None,\n",
    "        include_keys: Optional[List[str]] = None,\n",
    "    ):\n",
    "        self.ase_args = {}\n",
    "        self.ase_args.update(getattr(type(self), \"ASE_ARGS\", dict()))\n",
    "        self.ase_args.update(ase_args)\n",
    "        assert \"index\" not in self.ase_args\n",
    "        assert \"filename\" not in self.ase_args\n",
    "\n",
    "        self.include_keys = include_keys\n",
    "        self.key_mapping = key_mapping\n",
    "\n",
    "        super().__init__(\n",
    "            file_name=file_name,\n",
    "            url=url,\n",
    "            root=root,\n",
    "            AtomicData_options=AtomicData_options,\n",
    "            include_frames=include_frames,\n",
    "            type_mapper=type_mapper,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_atoms_list(cls, atoms: Sequence[ase.Atoms], **kwargs):\n",
    "        \"\"\"Make an ``ASEDataset`` from a list of ``ase.Atoms`` objects.\n",
    "\n",
    "        If `root` is not provided, a temporary directory will be used.\n",
    "\n",
    "        Please note that this is a convinience method that does NOT avoid a round-trip to disk; the provided ``atoms`` will be written out to a file.\n",
    "\n",
    "        Ignores ``kwargs[\"file_name\"]`` if it is provided.\n",
    "\n",
    "        Args:\n",
    "            atoms\n",
    "            **kwargs: passed through to the constructor\n",
    "        Returns:\n",
    "            The constructed ``ASEDataset``.\n",
    "        \"\"\"\n",
    "        if \"root\" not in kwargs:\n",
    "            tmpdir = tempfile.TemporaryDirectory()\n",
    "            kwargs[\"root\"] = tmpdir.name\n",
    "        else:\n",
    "            tmpdir = None\n",
    "        kwargs[\"file_name\"] = tmpdir.name + \"/atoms.xyz\"\n",
    "        atoms = list(atoms)\n",
    "        # Write them out\n",
    "        ase.io.write(kwargs[\"file_name\"], atoms, format=\"extxyz\")\n",
    "        # Read them in\n",
    "        obj = cls(**kwargs)\n",
    "        if tmpdir is not None:\n",
    "            # Make it keep a reference to the tmpdir to keep it alive\n",
    "            # When the dataset is garbage collected, the tmpdir will\n",
    "            # be too, and will (hopefully) get deleted eventually.\n",
    "            # Or at least by end of program...\n",
    "            obj._tmpdir_ref = tmpdir\n",
    "        return obj\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [basename(self.file_name)]\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self):\n",
    "        return dirname(abspath(self.file_name))\n",
    "\n",
    "    def get_data(self):\n",
    "        ase_args = {\"filename\": self.raw_dir + \"/\" + self.raw_file_names[0]}\n",
    "        ase_args.update(self.ase_args)\n",
    "\n",
    "        # skip the None arguments\n",
    "        kwargs = dict(\n",
    "            include_keys=self.include_keys,\n",
    "            key_mapping=self.key_mapping,\n",
    "        )\n",
    "        kwargs = {k: v for k, v in kwargs.items() if v is not None}\n",
    "        kwargs.update(self.AtomicData_options)\n",
    "        n_proc = num_tasks()\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            from nequip.utils._global_options import _get_latest_global_options\n",
    "\n",
    "            # ^ avoid import loop\n",
    "            reader = functools.partial(\n",
    "                _ase_dataset_reader,\n",
    "                world_size=n_proc,\n",
    "                tmpdir=tmpdir,\n",
    "                ase_kwargs=ase_args,\n",
    "                atomicdata_kwargs=kwargs,\n",
    "                include_frames=self.include_frames,\n",
    "                # get the global options of the parent to initialize the worker correctly\n",
    "                global_options=_get_latest_global_options(),\n",
    "            )\n",
    "            if n_proc > 1:\n",
    "                # things hang for some obscure OpenMP reason on some systems when using `fork` method\n",
    "                ctx = mp.get_context(\"forkserver\")\n",
    "                with ctx.Pool(processes=n_proc) as p:\n",
    "                    # map it over the `rank` argument\n",
    "                    datas = p.map(reader, list(range(n_proc)))\n",
    "                # clean up the pool before loading the data\n",
    "                datas = [torch.load(d) for d in datas]\n",
    "                datas = sum(datas, [])\n",
    "                # un-interleave the datas\n",
    "                datas = sorted(datas, key=lambda e: e[0])\n",
    "            else:\n",
    "                datas = reader(rank=0)\n",
    "                # datas here is already in order, stride 1 start 0\n",
    "                # no need to un-interleave\n",
    "        # return list of AtomicData:\n",
    "        return [e[1] for e in datas]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6974ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os.path import dirname, basename, abspath\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "\n",
    "#from .. import AtomicDataDict, _LONG_FIELDS, _NODE_FIELDS, _GRAPH_FIELDS\n",
    "#from ..transforms import TypeMapper\n",
    "#from ._base_datasets import AtomicInMemoryDataset\n",
    "\n",
    "\n",
    "class NpzDataset(AtomicInMemoryDataset):\n",
    "    \"\"\"Load data from an npz file.\n",
    "\n",
    "    To avoid loading unneeded data, keys are ignored by default unless they are in ``key_mapping``, ``include_keys``,\n",
    "    or ``npz_fixed_fields_keys``.\n",
    "\n",
    "    Args:\n",
    "        key_mapping (Dict[str, str]): mapping of npz keys to ``AtomicData`` keys. Optional\n",
    "        include_keys (list): the attributes to be processed and stored. Optional\n",
    "        npz_fixed_field_keys: the attributes that only have one instance but apply to all frames. Optional\n",
    "            Note that the mapped keys (as determined by the _values_ in ``key_mapping``) should be used in\n",
    "            ``npz_fixed_field_keys``, not the original npz keys from before mapping. If an npz key is not\n",
    "            present in ``key_mapping``, it is mapped to itself, and this point is not relevant.\n",
    "\n",
    "    Example: Given a npz file with 10 configurations, each with 14 atoms.\n",
    "\n",
    "        position: (10, 14, 3)\n",
    "        force: (10, 14, 3)\n",
    "        energy: (10,)\n",
    "        Z: (14)\n",
    "        user_label1: (10)        # per config\n",
    "        user_label2: (10, 14, 3) # per atom\n",
    "\n",
    "    The input yaml should be\n",
    "\n",
    "    ```yaml\n",
    "    dataset: npz\n",
    "    dataset_file_name: example.npz\n",
    "    include_keys:\n",
    "      - user_label1\n",
    "      - user_label2\n",
    "    npz_fixed_field_keys:\n",
    "      - cell\n",
    "      - atomic_numbers\n",
    "    key_mapping:\n",
    "      position: pos\n",
    "      force: forces\n",
    "      energy: total_energy\n",
    "      Z: atomic_numbers\n",
    "    graph_fields:\n",
    "      - user_label1\n",
    "    node_fields:\n",
    "      - user_label2\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        key_mapping: Dict[str, str] = {\n",
    "            \"positions\": AtomicDataDict.POSITIONS_KEY,\n",
    "            \"energy\": AtomicDataDict.TOTAL_ENERGY_KEY,\n",
    "            \"force\": AtomicDataDict.FORCE_KEY,\n",
    "            \"forces\": AtomicDataDict.FORCE_KEY,\n",
    "            \"Z\": AtomicDataDict.ATOMIC_NUMBERS_KEY,\n",
    "            \"atomic_number\": AtomicDataDict.ATOMIC_NUMBERS_KEY,\n",
    "        },\n",
    "        include_keys: List[str] = [],\n",
    "        npz_fixed_field_keys: List[str] = [],\n",
    "        file_name: Optional[str] = None,\n",
    "        url: Optional[str] = None,\n",
    "        AtomicData_options: Dict[str, Any] = {},\n",
    "        include_frames: Optional[List[int]] = None,\n",
    "        type_mapper: TypeMapper = None,\n",
    "    ):\n",
    "        self.key_mapping = key_mapping\n",
    "        self.npz_fixed_field_keys = npz_fixed_field_keys\n",
    "        self.include_keys = include_keys\n",
    "\n",
    "        super().__init__(\n",
    "            file_name=file_name,\n",
    "            url=url,\n",
    "            root=root,\n",
    "            AtomicData_options=AtomicData_options,\n",
    "            include_frames=include_frames,\n",
    "            type_mapper=type_mapper,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [basename(self.file_name)]\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self):\n",
    "        return dirname(abspath(self.file_name))\n",
    "\n",
    "    def get_data(self):\n",
    "\n",
    "        data = np.load(self.raw_dir + \"/\" + self.raw_file_names[0], allow_pickle=True)\n",
    "\n",
    "        # only the keys explicitly mentioned in the yaml file will be parsed\n",
    "        keys = set(list(self.key_mapping.keys()))\n",
    "        keys.update(self.npz_fixed_field_keys)\n",
    "        keys.update(self.include_keys)\n",
    "        keys = keys.intersection(set(list(data.keys())))\n",
    "\n",
    "        mapped = {self.key_mapping.get(k, k): data[k] for k in keys}\n",
    "\n",
    "        for intkey in _LONG_FIELDS:\n",
    "            if intkey in mapped:\n",
    "                mapped[intkey] = mapped[intkey].astype(np.int64)\n",
    "\n",
    "        fields = {k: v for k, v in mapped.items() if k not in self.npz_fixed_field_keys}\n",
    "        num_examples, num_atoms, n_dim = fields[AtomicDataDict.POSITIONS_KEY].shape\n",
    "        assert n_dim == 3\n",
    "\n",
    "        # now we replicate and add the fixed fields:\n",
    "        for fixed_field in self.npz_fixed_field_keys:\n",
    "            orig = mapped[fixed_field]\n",
    "            if fixed_field in _NODE_FIELDS:\n",
    "                assert orig.ndim >= 1  # [n_atom, feature_dims]\n",
    "                assert orig.shape[0] == num_atoms\n",
    "                replicated = np.expand_dims(orig, 0)\n",
    "                replicated = np.tile(\n",
    "                    replicated,\n",
    "                    (num_examples,) + (1,) * len(replicated.shape[1:]),\n",
    "                )  # [n_example, n_atom, feature_dims]\n",
    "            elif fixed_field in _GRAPH_FIELDS:\n",
    "                # orig is [feature_dims]\n",
    "                replicated = np.expand_dims(orig, 0)\n",
    "                replicated = np.tile(\n",
    "                    replicated,\n",
    "                    (num_examples,) + (1,) * len(replicated.shape[1:]),\n",
    "                )  # [n_example, feature_dims]\n",
    "            else:\n",
    "                raise KeyError(\n",
    "                    f\"npz_fixed_field_keys contains `{fixed_field}`, but it isn't registered as a node or graph field\"\n",
    "                )\n",
    "            fields[fixed_field] = replicated\n",
    "        return fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c1c3a",
   "metadata": {},
   "source": [
    "### Dataset_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cdbb75b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from importlib import import_module\n",
    "\n",
    "from nequip import data\n",
    "#from nequip.data.transforms import TypeMapper\n",
    "#from nequip.data import AtomicDataset\n",
    "from nequip.utils import instantiate, get_w_prefix\n",
    "\n",
    "\n",
    "def dataset_from_config(config, prefix: str = \"dataset\") -> AtomicDataset:\n",
    "    \"\"\"initialize database based on a config instance\n",
    "\n",
    "    It needs dataset type name (case insensitive),\n",
    "    and all the parameters needed in the constructor.\n",
    "\n",
    "    Examples see tests/data/test_dataset.py TestFromConfig\n",
    "    and tests/datasets/test_simplest.py\n",
    "\n",
    "    Args:\n",
    "\n",
    "    config (dict, nequip.utils.Config): dict/object that store all the parameters\n",
    "    prefix (str): Optional. The prefix of all dataset parameters\n",
    "\n",
    "    Return:\n",
    "\n",
    "    dataset (nequip.data.AtomicDataset)\n",
    "    \"\"\"\n",
    "\n",
    "    config_dataset = config.get(prefix, None)\n",
    "    if config_dataset is None:\n",
    "        raise KeyError(f\"Dataset with prefix `{prefix}` isn't present in this config!\")\n",
    "\n",
    "    if inspect.isclass(config_dataset):\n",
    "        # user define class\n",
    "        class_name = config_dataset\n",
    "    else:\n",
    "        try:\n",
    "            module_name = \".\".join(config_dataset.split(\".\")[:-1])\n",
    "            class_name = \".\".join(config_dataset.split(\".\")[-1:])\n",
    "            class_name = getattr(import_module(module_name), class_name)\n",
    "        except Exception:\n",
    "            # ^ TODO: don't catch all Exception\n",
    "            # default class defined in nequip.data or nequip.dataset\n",
    "            dataset_name = config_dataset.lower()\n",
    "\n",
    "            class_name = None\n",
    "            for k, v in inspect.getmembers(data, inspect.isclass):\n",
    "                if k.endswith(\"Dataset\"):\n",
    "                    if k.lower() == dataset_name:\n",
    "                        class_name = v\n",
    "                    if k[:-7].lower() == dataset_name:\n",
    "                        class_name = v\n",
    "                elif k.lower() == dataset_name:\n",
    "                    class_name = v\n",
    "\n",
    "    if class_name is None:\n",
    "        raise NameError(f\"dataset type {dataset_name} does not exists\")\n",
    "\n",
    "    # if dataset r_max is not found, use the universal r_max\n",
    "    atomicdata_options_key = \"AtomicData_options\"\n",
    "    prefixed_eff_key = f\"{prefix}_{atomicdata_options_key}\"\n",
    "    config[prefixed_eff_key] = get_w_prefix(\n",
    "        atomicdata_options_key, {}, prefix=prefix, arg_dicts=config\n",
    "    )\n",
    "    config[prefixed_eff_key][\"r_max\"] = get_w_prefix(\n",
    "        \"r_max\",\n",
    "        prefix=prefix,\n",
    "        arg_dicts=[config[prefixed_eff_key], config],\n",
    "    )\n",
    "\n",
    "    # Build a TypeMapper from the config\n",
    "    type_mapper, _ = instantiate(TypeMapper, prefix=prefix, optional_args=config)\n",
    "\n",
    "    instance, _ = instantiate(\n",
    "        class_name,\n",
    "        prefix=prefix,\n",
    "        positional_args={\"type_mapper\": type_mapper},\n",
    "        optional_args=config,\n",
    "    )\n",
    "\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1cdcba25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.TypeMapper at 0x7fae969c07c0>,\n",
       " {'type_names': None,\n",
       "  'chemical_symbol_to_type': None,\n",
       "  'type_to_chemical_symbol': None,\n",
       "  'chemical_symbols': ['H', 'C']})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instantiate(TypeMapper, prefix='dataset', optional_args=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ca60ee4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtomicData(atom_types=[64, 1], cell=[3, 3], edge_cell_shift=[1146, 3], edge_index=[2, 1146], forces=[64, 3], free_energy=[1], pbc=[3], pos=[64, 3], stress=[3, 3], total_energy=[1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config.from_file('./configs/NaBr.yaml', defaults=default_config)\n",
    "    \n",
    "\n",
    "dataset = dataset_from_config(config, prefix=\"dataset\")\n",
    "\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "edd791e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading http://quantum-machine.org/gdml/data/npz/toluene_ccsd_t.zip\n",
      "Processing dataset...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AtomicData(atom_types=[15, 1], cell=[3, 3], edge_cell_shift=[152, 3], edge_index=[2, 152], forces=[15, 3], pbc=[3], pos=[15, 3], total_energy=[1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config.from_file('./configs/example.yaml', defaults=default_config)\n",
    "    \n",
    "\n",
    "dataset = dataset_from_config(config, prefix=\"dataset\")\n",
    "\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81120a2",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ee34ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Iterator\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "from nequip.utils.torch_geometric import Batch, Data, Dataset\n",
    "\n",
    "\n",
    "class Collater(object):\n",
    "    \"\"\"Collate a list of ``AtomicData``.\n",
    "\n",
    "    Args:\n",
    "        exclude_keys: keys to ignore in the input, not copying to the output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        exclude_keys: List[str] = [],\n",
    "    ):\n",
    "        self._exclude_keys = set(exclude_keys)\n",
    "\n",
    "    @classmethod\n",
    "    def for_dataset(\n",
    "        cls,\n",
    "        dataset,\n",
    "        exclude_keys: List[str] = [],\n",
    "    ):\n",
    "        \"\"\"Construct a collater appropriate to ``dataset``.\"\"\"\n",
    "        return cls(\n",
    "            exclude_keys=exclude_keys,\n",
    "        )\n",
    "\n",
    "    def collate(self, batch: List[Data]) -> Batch:\n",
    "        \"\"\"Collate a list of data\"\"\"\n",
    "        out = Batch.from_data_list(batch, exclude_keys=self._exclude_keys)\n",
    "        return out\n",
    "\n",
    "    def __call__(self, batch: List[Data]) -> Batch:\n",
    "        \"\"\"Collate a list of data\"\"\"\n",
    "        return self.collate(batch)\n",
    "\n",
    "    @property\n",
    "    def exclude_keys(self):\n",
    "        return list(self._exclude_keys)\n",
    "\n",
    "\n",
    "class DataLoader(torch.utils.data.DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size: int = 1,\n",
    "        shuffle: bool = False,\n",
    "        exclude_keys: List[str] = [],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if \"collate_fn\" in kwargs:\n",
    "            del kwargs[\"collate_fn\"]\n",
    "\n",
    "        super(DataLoader, self).__init__(\n",
    "            dataset,\n",
    "            batch_size,\n",
    "            shuffle,\n",
    "            collate_fn=Collater.for_dataset(dataset, exclude_keys=exclude_keys),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "class PartialSampler(Sampler[int]):\n",
    "    r\"\"\"Samples elements without replacement, but divided across a number of calls to `__iter__`.\n",
    "\n",
    "    To ensure deterministic reproducibility and restartability, dataset permutations are generated\n",
    "    from a combination of the overall seed and the epoch number. As a result, the caller must\n",
    "    tell this sampler the epoch number before each time `__iter__` is called by calling\n",
    "    `my_partial_sampler.step_epoch(epoch_number_about_to_run)` each time.\n",
    "\n",
    "    This sampler decouples epochs from the dataset size and cycles through the dataset over as\n",
    "    many (partial) epochs as it may take. As a result, the _dataset_ epoch can change partway\n",
    "    through a training epoch.\n",
    "\n",
    "    Args:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "        shuffle (bool): whether to shuffle the dataset each time the _entire_ dataset is consumed\n",
    "        num_samples_per_epoch (int): number of samples to draw in each call to `__iter__`.\n",
    "            If `None`, defaults to `len(data_source)`.\n",
    "        generator (Generator): Generator used in sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    data_source: Dataset\n",
    "    num_samples_per_epoch: int\n",
    "    shuffle: bool\n",
    "    _epoch: int\n",
    "    _prev_epoch: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_source: Dataset,\n",
    "        shuffle: bool = True,\n",
    "        num_samples_per_epoch: Optional[int] = None,\n",
    "        generator=None,\n",
    "    ) -> None:\n",
    "        self.data_source = data_source\n",
    "        self.shuffle = shuffle\n",
    "        if num_samples_per_epoch is None:\n",
    "            num_samples_per_epoch = self.num_samples_total\n",
    "        self.num_samples_per_epoch = num_samples_per_epoch\n",
    "        assert self.num_samples_per_epoch <= self.num_samples_total\n",
    "        assert self.num_samples_per_epoch >= 1\n",
    "        self.generator = generator\n",
    "        self._epoch = None\n",
    "        self._prev_epoch = None\n",
    "\n",
    "    @property\n",
    "    def num_samples_total(self) -> int:\n",
    "        # dataset size might change at runtime\n",
    "        return len(self.data_source)\n",
    "\n",
    "    def step_epoch(self, epoch: int) -> None:\n",
    "        self._epoch = epoch\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        assert self._epoch is not None\n",
    "        assert (self._prev_epoch is None) or (self._epoch == self._prev_epoch + 1)\n",
    "        assert self._epoch >= 0\n",
    "\n",
    "        full_epoch_i, start_sample_i = divmod(\n",
    "            # how much data we've already consumed:\n",
    "            self._epoch * self.num_samples_per_epoch,\n",
    "            # how much data there is the dataset:\n",
    "            self.num_samples_total,\n",
    "        )\n",
    "\n",
    "        if self.shuffle:\n",
    "            temp_rng = torch.Generator()\n",
    "            # Get new randomness for each _full_ time through the dataset\n",
    "            # This is deterministic w.r.t. the combination of dataset seed and epoch number\n",
    "            # Both of which persist across restarts\n",
    "            # (initial_seed() is restored by set_state())\n",
    "            temp_rng.manual_seed(self.generator.initial_seed() + full_epoch_i)\n",
    "            full_order_this = torch.randperm(self.num_samples_total, generator=temp_rng)\n",
    "            # reseed the generator for the _next_ epoch to get the shuffled order of the\n",
    "            # _next_ dataset epoch to pad out this one for completing any partial batches\n",
    "            # at the end:\n",
    "            temp_rng.manual_seed(self.generator.initial_seed() + full_epoch_i + 1)\n",
    "            full_order_next = torch.randperm(self.num_samples_total, generator=temp_rng)\n",
    "            del temp_rng\n",
    "        else:\n",
    "            full_order_this = torch.arange(self.num_samples_total)\n",
    "            # without shuffling, the next epoch has the same sampling order as this one:\n",
    "            full_order_next = full_order_this\n",
    "\n",
    "        full_order = torch.cat((full_order_this, full_order_next), dim=0)\n",
    "        del full_order_next, full_order_this\n",
    "\n",
    "        this_segment_indexes = full_order[\n",
    "            start_sample_i : start_sample_i + self.num_samples_per_epoch\n",
    "        ]\n",
    "        # because we cycle into indexes from the next dataset epoch,\n",
    "        # we should _always_ be able to get num_samples_per_epoch\n",
    "        assert len(this_segment_indexes) == self.num_samples_per_epoch\n",
    "        yield from this_segment_indexes\n",
    "\n",
    "        self._prev_epoch = self._epoch\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples_per_epochb\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41430e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_mkl",
   "language": "python",
   "name": "torch_mkl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
