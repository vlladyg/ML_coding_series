{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b074de41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/temporary/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.0.0 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# reference example\n",
    "from nequip.data import dataset_from_config\n",
    "from nequip.utils import Config\n",
    "from nequip.utils.misc import get_default_device_name\n",
    "from nequip.utils.config import _GLOBAL_ALL_ASKED_FOR_KEYS\n",
    "\n",
    "from nequip.model import model_from_config\n",
    "\n",
    "\n",
    "default_config = dict(\n",
    "    root=\"./\",\n",
    "    tensorboard=False,\n",
    "    wandb=False,\n",
    "    model_builders=[\n",
    "        \"SimpleIrrepsConfig\",\n",
    "        \"EnergyModel\",\n",
    "        \"PerSpeciesRescale\",\n",
    "        \"StressForceOutput\",\n",
    "        \"RescaleEnergyEtc\",\n",
    "    ],\n",
    "    dataset_statistics_stride=1,\n",
    "    device=get_default_device_name(),\n",
    "    default_dtype=\"float64\",\n",
    "    model_dtype=\"float32\",\n",
    "    allow_tf32=True,\n",
    "    verbose=\"INFO\",\n",
    "    model_debug_mode=False,\n",
    "    equivariance_test=False,\n",
    "    grad_anomaly_mode=False,\n",
    "    gpu_oom_offload=False,\n",
    "    append=False,\n",
    "    warn_unused=False,\n",
    "    _jit_bailout_depth=2,  # avoid 20 iters of pain, see https://github.com/pytorch/pytorch/issues/52286\n",
    "    # Quote from eelison in PyTorch slack:\n",
    "    # https://pytorch.slack.com/archives/CDZD1FANA/p1644259272007529?thread_ts=1644064449.039479&cid=CDZD1FANA\n",
    "    # > Right now the default behavior is to specialize twice on static shapes and then on dynamic shapes.\n",
    "    # > To reduce warmup time you can do something like setFusionStrartegy({{FusionBehavior::DYNAMIC, 3}})\n",
    "    # > ... Although we would wouldn't really expect to recompile a dynamic shape fusion in a model,\n",
    "    # > provided broadcasting patterns remain fixed\n",
    "    # We default to DYNAMIC alone because the number of edges is always dynamic,\n",
    "    # even if the number of atoms is fixed:\n",
    "    _jit_fusion_strategy=[(\"DYNAMIC\", 3)],\n",
    "    # Due to what appear to be ongoing bugs with nvFuser, we default to NNC (fuser1) for now:\n",
    "    # TODO: still default to NNC on CPU regardless even if change this for GPU\n",
    "    # TODO: default for ROCm?\n",
    "    _jit_fuser=\"fuser1\",\n",
    ")\n",
    "\n",
    "# All default_config keys are valid / requested\n",
    "_GLOBAL_ALL_ASKED_FOR_KEYS.update(default_config.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d41b3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtomicData(atom_types=[15, 1], cell=[3, 3], edge_cell_shift=[152, 3], edge_index=[2, 152], forces=[15, 3], pbc=[3], pos=[15, 3], total_energy=[1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config.from_file('./configs/example.yaml', defaults=default_config)\n",
    "    \n",
    "\n",
    "dataset = dataset_from_config(config, prefix=\"dataset\")\n",
    "\n",
    "validation_dataset = None\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31281bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Torch device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "from nequip.train.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(model=None, **Config.as_dict(config))\n",
    "\n",
    "# what is this\n",
    "# to update wandb data?\n",
    "config.update(trainer.params)\n",
    "\n",
    "# = Train/test split =\n",
    "trainer.set_dataset(dataset, validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "092e0de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/temporary/Documents/GitHub/pytorch-intel-mps/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb0f5eb8250>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from vladimir import GraphModuleMixin\n",
    "from vladimir import OneHotAtomEncoding\n",
    "\n",
    "one_hot = OneHotAtomEncoding(2)\n",
    "\n",
    "from vladimir import AtomwiseLinear\n",
    "from e3nn import o3\n",
    "\n",
    "torch.manual_seed(32)\n",
    "\n",
    "\n",
    "# Linear\n",
    "al = AtomwiseLinear(irreps_in=one_hot.irreps_out, irreps_out=o3.Irreps([(32, (0, 1))]))\n",
    "\n",
    "\n",
    "# Rbe\n",
    "from vladimir import RadialBasisEdgeEncoding\n",
    "from e3nn import o3\n",
    "\n",
    "rbe = RadialBasisEdgeEncoding(basis_kwargs={'r_max': 4}, cutoff_kwargs={'r_max': 4}, irreps_in=al.irreps_out)\n",
    "\n",
    "# SH\n",
    "torch.manual_seed(32)\n",
    "\n",
    "from vladimir import SphericalHarmonicEdgeAttrs\n",
    "\n",
    "sh = SphericalHarmonicEdgeAttrs(2, irreps_in=rbe.irreps_out)\n",
    "\n",
    "\n",
    "# Convolution\n",
    "from vladimir import conv\n",
    "from vladimir import InteractionBlock, ConvNetLayer\n",
    "from e3nn import o3\n",
    "\n",
    "torch.manual_seed(32)\n",
    "\n",
    "\n",
    "avg_num_neighbors = None\n",
    "\n",
    "\n",
    "# 3 conv layers\n",
    "conv1 = ConvNetLayer(irreps_in = sh.irreps_out, \n",
    "                    feature_irreps_hidden = '32x0e+32x1e+32x2e+32x1o+32x2o',\n",
    "                    convolution = InteractionBlock,\n",
    "                    convolution_kwargs={'invariant_layers': 2, \n",
    "                                        'invariant_neurons': 64,\n",
    "                                        'avg_num_neighbors': avg_num_neighbors}\n",
    "                   )\n",
    "\n",
    "\n",
    "conv2 = ConvNetLayer(irreps_in = conv1.irreps_out, \n",
    "                    feature_irreps_hidden = '32x0e+32x1e+32x2e+32x1o+32x2o',\n",
    "                    convolution = InteractionBlock,\n",
    "                    convolution_kwargs={'invariant_layers': 2, \n",
    "                                        'invariant_neurons': 64,\n",
    "                                        'avg_num_neighbors': avg_num_neighbors}\n",
    "                   )\n",
    "\n",
    "conv3 = ConvNetLayer(irreps_in = conv2.irreps_out, \n",
    "                    feature_irreps_hidden = '32x0e+32x0o+32x1e+32x2e+32x1o+32x2o',\n",
    "                    convolution = InteractionBlock,\n",
    "                    convolution_kwargs={'invariant_layers': 2, \n",
    "                                        'invariant_neurons': 64,\n",
    "                                        'avg_num_neighbors': avg_num_neighbors}\n",
    "                   )\n",
    "\n",
    "# Last linear\n",
    "from vladimir import conv\n",
    "from vladimir import AtomwiseLinear\n",
    "from e3nn import o3\n",
    "\n",
    "torch.manual_seed(32)\n",
    "\n",
    "\n",
    "al2 = AtomwiseLinear(irreps_in=conv3.irreps_out, irreps_out=o3.Irreps([(16, (0, 1))]))\n",
    "al3 = AtomwiseLinear(irreps_in=al2.irreps_out, irreps_out=o3.Irreps([(1, (0, 1))]))\n",
    "\n",
    "\n",
    "# Shift and scale\n",
    "from vladimir import PerSpeciesScaleShift\n",
    "\n",
    "torch.manual_seed(32)\n",
    "\n",
    "\n",
    "num_types = 2\n",
    "\n",
    "scales = [-11319.556641, -11319.556641]\n",
    "shifts = [1/30.621034622192383, 1/30.621034622192383]\n",
    "\n",
    "psss = PerSpeciesScaleShift(\n",
    "    field = 'node_features',\n",
    "    type_names = ['H', 'C'],\n",
    "    num_types = num_types,\n",
    "    shifts = shifts,\n",
    "    scales = scales,\n",
    "    arguments_in_dataset_units = True,\n",
    "    scales_trainable = True,\n",
    "    shifts_trainable = True,\n",
    "    irreps_in = al3.irreps_out)\n",
    "\n",
    "# Reduce\n",
    "from vladimir import AtomwiseReduce\n",
    "\n",
    "torch.manual_seed(32)\n",
    "\n",
    "\n",
    "num_types = 2\n",
    "\n",
    "scales = [1., 1.]\n",
    "shifts = [0., 0.]\n",
    "\n",
    "ar = AtomwiseReduce(\n",
    "    field = 'shifted_node_features',\n",
    "    out_field = 'total_energy',\n",
    "    reduce = 'sum',\n",
    "    irreps_in = psss.irreps_out)\n",
    "\n",
    "\n",
    "from vladimir import SequentialGraphNetwork\n",
    "\n",
    "module_list = [one_hot, al,\n",
    "               rbe, sh,\n",
    "               conv1, conv2, conv3,\n",
    "               al2, al3,\n",
    "               psss, ar]\n",
    "\n",
    "graph_func = SequentialGraphNetwork(module_list)\n",
    "\n",
    "from vladimir import StressOutput\n",
    "\n",
    "torch.manual_seed(32)\n",
    "\n",
    "so = StressOutput(graph_func)\n",
    "\n",
    "from vladimir import RescaleOutput\n",
    "\n",
    "torch.manual_seed(32)\n",
    "\n",
    "rescale = RescaleOutput(model = so,\n",
    "        scale_keys = ['pos', 'total_energy', 'forces', 'stress', 'virial'],\n",
    "        shift_keys = ['total_energy'],\n",
    "        scale_by=[1., 1., 1., 1., 1.],\n",
    "        shift_by=[0.],\n",
    "        shift_trainable = False,\n",
    "        scale_trainable = False,\n",
    "        irreps_in = so.irreps_in)\n",
    "\n",
    "from vladimir import GraphModel\n",
    "\n",
    "torch.manual_seed(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f54e8e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nequip.model._gmm import GraphModel\n",
    "\n",
    "gm = GraphModel(rescale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e2bee02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nequip.nn._graph_model.GraphModel'>\n"
     ]
    }
   ],
   "source": [
    "print(type(gm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd0d8d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Torch device: cpu\n",
      "Number of weights: 227420\n",
      "Number of trainable weights: 227420\n",
      "! Starting training ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (3) must match the existing size (5) at non-singleton dimension 1.  Target sizes: [150, 3].  Tensor sizes: [5]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# = Train/test split =\u001b[39;00m\n\u001b[1;32m     11\u001b[0m trainer\u001b[38;5;241m.\u001b[39mset_dataset(dataset, validation_dataset)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/nequip/train/trainer.py:784\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_init_callback()\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_cond:\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_of_epoch_save()\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_callbacks:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/nequip/train/trainer.py:919\u001b[0m, in \u001b[0;36mTrainer.epoch_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset)\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mibatch, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n\u001b[0;32m--> 919\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVALIDATION\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_of_batch_log(batch_type\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_end_of_batch_callbacks:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/nequip/train/trainer.py:814\u001b[0m, in \u001b[0;36mTrainer.batch_step\u001b[0;34m(self, data, validation)\u001b[0m\n\u001b[1;32m    810\u001b[0m data_for_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39munscale(data, force_process\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    812\u001b[0m \u001b[38;5;66;03m# Run model\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;66;03m# We make a shallow copy of the input dict in case the model modifies it\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_for_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# If we're in evaluation mode (i.e. validation), then\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# data_for_loss's target prop is unnormalized, and out's has been rescaled to be in the same units\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# If we're in training, data_for_loss's target prop has been normalized, and out's hasn't been touched, so they're both in normalized units\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;66;03m# Note that either way all normalization was handled internally by GraphModel via RescaleOutput\u001b[39;00m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validation:\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# Actually do an optimization step, since we're training:\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/pytorch-intel-mps/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_mkl/lib/python3.10/site-packages/nequip/nn/_graph_model.py:112\u001b[0m, in \u001b[0;36mGraphModel.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    110\u001b[0m         new_data[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# run the model\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Documents/GitHub/pytorch-intel-mps/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/ML_coding_series/nequip/vladimir/layers/rescale.py:163\u001b[0m, in \u001b[0;36mRescaleOutput.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_keys:\n\u001b[1;32m    162\u001b[0m         v \u001b[38;5;241m=\u001b[39m data[field]\n\u001b[0;32m--> 163\u001b[0m         data[field] \u001b[38;5;241m=\u001b[39m v \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_by\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_shift:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshift_keys:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (3) must match the existing size (5) at non-singleton dimension 1.  Target sizes: [150, 3].  Tensor sizes: [5]"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "from nequip.train.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(model = gm, **Config.as_dict(config))\n",
    "\n",
    "# what is this\n",
    "# to update wandb data?\n",
    "config.update(trainer.params)\n",
    "\n",
    "# = Train/test split =\n",
    "trainer.set_dataset(dataset, validation_dataset)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b545e63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_mkl",
   "language": "python",
   "name": "torch_mkl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
